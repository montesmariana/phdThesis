<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 3 From corpora to clouds | Cloudspotting: Visual analytics for distributional semantics</title>
<meta name="author" content="Mariana Montes">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.2"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/header-attrs-2.7/header-attrs.js"></script><script src="libs/jquery-3.5.1/jquery-3.5.1.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.5.3/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.5.3/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.2.4/tabs.js"></script><script src="libs/bs3compat-0.2.4/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><link href="libs/tabwid-1.0.0/tabwid.css" rel="stylesheet">
<script src="https://cdn.jsdelivr.net/autocomplete.js/0/autocomplete.jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/mark.js@8.11.1/dist/mark.min.js"></script><!-- CSS -->
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Cloudspotting: Visual analytics for distributional semantics</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Introduction</a></li>
<li><a class="" href="acknowledgments.html">Acknowledgments</a></li>
<li class="book-part">Visualization tool</li>
<li><a class="" href="an-interface-to-the-world-of-clouds.html"><span class="header-section-number">1</span> An interface to the world of clouds</a></li>
<li><a class="" href="parameter-settings.html"><span class="header-section-number">2</span> Parameter settings</a></li>
<li><a class="active" href="from-corpora-to-clouds.html"><span class="header-section-number">3</span> From corpora to clouds</a></li>
<li><a class="" href="nephovis.html"><span class="header-section-number">4</span> NephoVis</a></li>
<li><a class="" href="hdbscan.html"><span class="header-section-number">5</span> HDBSCAN</a></li>
<li><a class="" href="annotation-schema.html"><span class="header-section-number">6</span> Annotation schema</a></li>
<li class="book-part">The language of clouds</li>
<li><a class="" href="nonsense-or-no-senses.html"><span class="header-section-number">7</span> Nonsense or no senses?</a></li>
<li><a class="" href="the-nature-of-clouds.html"><span class="header-section-number">8</span> The nature of clouds</a></li>
<li class="book-part">Appendix</li>
<li><a class="" href="definitions.html"><span class="header-section-number">A</span> Definitions</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/montesmariana/phdThesis">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="from-corpora-to-clouds" class="section level1" number="3">
<h1>
<span class="header-section-number">3</span> From corpora to clouds<a class="anchor" aria-label="anchor" href="#from-corpora-to-clouds"><i class="fas fa-link"></i></a>
</h1>
<p>The main goal of the distributional models discussed in this text is to explore semasiological structure
from textual data. The starting point is a corpus, and one of the most tangible outputs is the visual representation
as a cloud.</p>
<p>In this chapter we will describe the path needed to take to generate clouds from the raw,
seemingly indomitable ocean that is a corpus.</p>
<p>First, we will describe token-level vector space models are created.
Section <a href="from-corpora-to-clouds.html#vector-creation">3.1</a> will explain count-based models, but this is by no means the only viable path.
Other techniques, such as BERT <span class="citation">(<a href="references.html#ref-BERT" role="doc-biblioref">Devlin et al. 2019</a>)</span>, that can generate vectors for individual instances of a word,
can be used for the first stage of this workflow.
<!-- TODO explain why we don't use them --></p>
<p>Once we have token-level vectors, we need to process them. For visualization purposes,
we need to reduce the numerous dimensions of the vectors to a manageable number, such as 2.
Section <a href="from-corpora-to-clouds.html#dim-reduction">3.2</a> will explore and compare a few alternatives.</p>
<p>The same output that is put through dimensionality reduction for the visualization can also
be submitted to other forms of analysis such as clustering algorithms, whose results may
even be combined with the visualization. We will look into HDBSCAN in
<!-- cite appropriate chapter and section. --></p>
<div id="vector-creation" class="section level2" number="3.1">
<h2>
<span class="header-section-number">3.1</span> A cloud machine<a class="anchor" aria-label="anchor" href="#vector-creation"><i class="fas fa-link"></i></a>
</h2>
<p>At the core of vector space models, <em>aka</em> distributional models, we find the Distributional Hypothesis, which is most often linked to Harris’s observation that “difference of meaning correlates with difference of distribution” <span class="citation">(<a href="references.html#ref-harris_1954" role="doc-biblioref">1954, 156</a>)</span>. In other words, items that occur in similar contexts in a given corpus will be semantically similar, while those that occur in different contexts will be semantically different. Crucially, this does not imply that we can describe an individual item with their distributional properties, but that comparing the distribution of two items can tell us something about their semantic relationship <span class="citation">(<a href="references.html#ref-sahlgren_2006" role="doc-biblioref">Sahlgren 2006, 19</a>)</span>.</p>
<p>Distributional models operationalize this idea by representing words as vectors (i.e. arrays of numbers) coding frequency information. Typically, the raw frequency is transformed to some association strength measure, such as pointwise mutual information <span class="citation">(PMI, see <a href="references.html#ref-church.hanks_1989" role="doc-biblioref">Church and Hanks 1989</a>)</span>, which compares the frequency with which two words occur close to each other and the expected frequency if the words were independent. For example, Table <a href="from-corpora-to-clouds.html#tab:vec1">3.1</a> shows small vectors representing the English nouns <em>linguistic</em>, <em>lexicography</em>, <em>research</em> and <em>chocolate</em>, as well as the adjective <em>computational</em>, as series of association strengths with a set of lemmas. Empty cells indicate that the word in the row and the word in the column never co-occur in the corpus (given a certain window span).</p>
<template id="f8c9a381-729e-4a4d-b29e-a60daf3b855d"><style>
.tabwid table{
  border-collapse:collapse;
  line-height:1;
  margin-left:auto;
  margin-right:auto;
  border-width: 0;
  display: table;
  margin-top: 1.275em;
  margin-bottom: 1.275em;
  border-spacing: 0;
  border-color: transparent;
}
.tabwid_left table{
  margin-left:0;
}
.tabwid_right table{
  margin-right:0;
}
.tabwid td {
    padding: 0;
}
.tabwid a {
  text-decoration: none;
}
.tabwid thead {
    background-color: transparent;
}
.tabwid tfoot {
    background-color: transparent;
}
.tabwid table tr {
background-color: transparent;
}
</style>
<div class="tabwid">
<style>.cl-75bdc722{border-collapse:collapse;}.cl-75b51a8c{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-75b5417e{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-75b5417f{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-75b56884{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-75b56885{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-75b56886{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-75b56887{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-75b56888{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-75b56889{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-75b5688a{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}</style>
<div class="inline-table"><table class="cl-75bdc722">
<caption class>
<span id="tab:vec1">Table 3.1: </span>Example of type-level vectors.
</caption>
<thead><tr style="overflow-wrap:break-word;">
<td class="cl-75b56889"><p class="cl-75b5417e"><span class="cl-75b51a8c">target</span></p></td>
<td class="cl-75b5688a"><p class="cl-75b5417f"><span class="cl-75b51a8c">language/n</span></p></td>
<td class="cl-75b5688a"><p class="cl-75b5417f"><span class="cl-75b51a8c">word/n</span></p></td>
<td class="cl-75b5688a"><p class="cl-75b5417f"><span class="cl-75b51a8c">flemish/j</span></p></td>
<td class="cl-75b5688a"><p class="cl-75b5417f"><span class="cl-75b51a8c">english/j</span></p></td>
<td class="cl-75b5688a"><p class="cl-75b5417f"><span class="cl-75b51a8c">speak/v</span></p></td>
</tr></thead>
<tbody>
<tr style="overflow-wrap:break-word;">
<td class="cl-75b56885"><p class="cl-75b5417e"><span class="cl-75b51a8c">linguistics/n</span></p></td>
<td class="cl-75b56884"><p class="cl-75b5417f"><span class="cl-75b51a8c">4.37</span></p></td>
<td class="cl-75b56884"><p class="cl-75b5417f"><span class="cl-75b51a8c">0.99</span></p></td>
<td class="cl-75b56884"><p class="cl-75b5417f"><span class="cl-75b51a8c"></span></p></td>
<td class="cl-75b56884"><p class="cl-75b5417f"><span class="cl-75b51a8c">3.16</span></p></td>
<td class="cl-75b56884"><p class="cl-75b5417f"><span class="cl-75b51a8c">0.41</span></p></td>
</tr>
<tr style="overflow-wrap:break-word;">
<td class="cl-75b56885"><p class="cl-75b5417e"><span class="cl-75b51a8c">lexicography/n</span></p></td>
<td class="cl-75b56884"><p class="cl-75b5417f"><span class="cl-75b51a8c">3.51</span></p></td>
<td class="cl-75b56884"><p class="cl-75b5417f"><span class="cl-75b51a8c">2.18</span></p></td>
<td class="cl-75b56884"><p class="cl-75b5417f"><span class="cl-75b51a8c"></span></p></td>
<td class="cl-75b56884"><p class="cl-75b5417f"><span class="cl-75b51a8c">2.19</span></p></td>
<td class="cl-75b56884"><p class="cl-75b5417f"><span class="cl-75b51a8c">2.09</span></p></td>
</tr>
<tr style="overflow-wrap:break-word;">
<td class="cl-75b56885"><p class="cl-75b5417e"><span class="cl-75b51a8c">computational/j</span></p></td>
<td class="cl-75b56884"><p class="cl-75b5417f"><span class="cl-75b51a8c">1.60</span></p></td>
<td class="cl-75b56884"><p class="cl-75b5417f"><span class="cl-75b51a8c">0.08</span></p></td>
<td class="cl-75b56884"><p class="cl-75b5417f"><span class="cl-75b51a8c"></span></p></td>
<td class="cl-75b56884"><p class="cl-75b5417f"><span class="cl-75b51a8c">-1.00</span></p></td>
<td class="cl-75b56884"><p class="cl-75b5417f"><span class="cl-75b51a8c">-1.80</span></p></td>
</tr>
<tr style="overflow-wrap:break-word;">
<td class="cl-75b56885"><p class="cl-75b5417e"><span class="cl-75b51a8c">research/n</span></p></td>
<td class="cl-75b56884"><p class="cl-75b5417f"><span class="cl-75b51a8c">0.20</span></p></td>
<td class="cl-75b56884"><p class="cl-75b5417f"><span class="cl-75b51a8c">-0.84</span></p></td>
<td class="cl-75b56884"><p class="cl-75b5417f"><span class="cl-75b51a8c">0.04</span></p></td>
<td class="cl-75b56884"><p class="cl-75b5417f"><span class="cl-75b51a8c">-0.50</span></p></td>
<td class="cl-75b56884"><p class="cl-75b5417f"><span class="cl-75b51a8c">-0.38</span></p></td>
</tr>
<tr style="overflow-wrap:break-word;">
<td class="cl-75b56887"><p class="cl-75b5417e"><span class="cl-75b51a8c">chocolate/n</span></p></td>
<td class="cl-75b56886"><p class="cl-75b5417f"><span class="cl-75b51a8c">-1.72</span></p></td>
<td class="cl-75b56886"><p class="cl-75b5417f"><span class="cl-75b51a8c">-0.53</span></p></td>
<td class="cl-75b56886"><p class="cl-75b5417f"><span class="cl-75b51a8c">1.28</span></p></td>
<td class="cl-75b56886"><p class="cl-75b5417f"><span class="cl-75b51a8c">-0.73</span></p></td>
<td class="cl-75b56886"><p class="cl-75b5417f"><span class="cl-75b51a8c">-1.13</span></p></td>
</tr>
</tbody>
<tfoot><tr style="overflow-wrap:break-word;"><td colspan="6" class="cl-75b56888"><p class="cl-75b5417e"><span class="cl-75b51a8c">PMI values based on symmetric window of 10; frequency data from GloWbE.</span></p></td></tr></tfoot>
</table></div>
</div></template><div class="flextable-shadow-host" id="48af0fa1-3798-42d0-8ee3-6cd5d1a0650c"></div>
<script>
var dest = document.getElementById("48af0fa1-3798-42d0-8ee3-6cd5d1a0650c");
var template = document.getElementById("f8c9a381-729e-4a4d-b29e-a60daf3b855d");
var caption = template.content.querySelector("caption");
if(caption) {
  caption.style.cssText = "display:block;text-align:center;";
  var newcapt = document.createElement("p");
  newcapt.appendChild(caption)
  dest.parentNode.insertBefore(newcapt, dest.previousSibling);
}
var fantome = dest.attachShadow({mode: 'open'});
var templateContent = template.content;
fantome.appendChild(templateContent);
</script><p>Each row is a vector coding the distributional information of the lemma it represents. As we can see in this example, words with similar vectors (e.g. <em>linguistics</em> and <em>lexicography</em>) are semantically similar, while words with different vectors (e.g. <em>linguistics</em> and <em>chocolate</em>) are semantically different.</p>
<p>The vectors in Table <a href="from-corpora-to-clouds.html#tab:vec1">3.1</a> are type-level vectors: each of them aggregates over all the instances of a given word, e.g. <em>linguistics</em>, to build an overall profile. As a result, it collapses the internal variation of the lemma, i.e. its semasiological structure. In order to uncover such information, we need to build vectors for the individual instances or tokens, relying on the same principle: items occurring in similar contexts will be semantically similar. For instance, we might want to model the three (artificial) occurrences of <em>study</em> in (2) through (4), where the target item is in italics.</p>
<ol start="2" class="example" style="list-style-type: decimal">
<li>Would you like to <em>study</em> lexicography?</li>
<li>They <em>study</em> this in computational linguistics as well.</li>
<li>I eat chocolate while I <em>study</em>.</li>
</ol>
<p>Given that, at the aggregate level, a word can co-occur with thousands of different words, type-level vectors can include thousands of values. In contrast, token-level vectors can only have as many values as the individual window size comprises, which drastically reduces the chances of overlap between vectors. In fact, the three examples don’t share any item other than the target. As a solution, inspired by <span class="citation"><a href="references.html#ref-schutze_1998" role="doc-biblioref">Schütze</a> (<a href="references.html#ref-schutze_1998" role="doc-biblioref">1998</a>)</span>, we replace the context words around the token with their respective type-level vectors <span class="citation">(<a href="references.html#ref-heylen.etal_2015" role="doc-biblioref">Heylen et al. 2015</a>; <a href="references.html#ref-depascale_2019" role="doc-biblioref">De Pascale 2019</a>)</span>.</p>
<p>For example, we could represent example (2) with the vector for its context word <em>lexicography</em>, that is, the second row in Table <a href="from-corpora-to-clouds.html#tab:vec1">3.1</a>; example (3) with the sum of the vectors for <em>linguistics</em> (row 1) and <em>computational</em> (row 3); and example (4) with the vector for <em>chocolate</em> (row 5). This not only solves the sparsity issue, ensuring overlap between the vectors, but also allows us to find similarity between (2) and (3) based on the similarity between the vectors for <em>lexicography</em> and <em>linguistics</em>.</p>
<p>From applying this method we obtain numerical representations of occurrences of a word. We can compare them to each other by calculating pairwise distances, which is at the base of clustering analyses and visualization techniques based on dimensionality reduction. However, in order to obtain this result we need to make a number of decisions involved, mostly, in defining the context that will be used to represent an item <span class="citation">(Cf. <a href="references.html#ref-bolognesi_2020" role="doc-biblioref">Bolognesi 2020, 23:83</a>)</span>.</p>
</div>
<div id="dim-reduction" class="section level2" number="3.2">
<h2>
<span class="header-section-number">3.2</span> Dimensionality reduction<a class="anchor" aria-label="anchor" href="#dim-reduction"><i class="fas fa-link"></i></a>
</h2>
<!-- NOTE Dimensionality reduction can also refer to SVD and stuff like that, so we should mention it and dismiss, cite Stefano -->
<p>Dimensionality reduction refers to algorithms that try to locate different items on a low-dimensional space (e.g. 2D) preserving their distances in the high-dimensional space (e.g. 5000D) as well as possible.
The literature up to today tends to go for either multidimensional scaling (MDS) or t-stochastic neighbor embeddings (t-SNE),
which may be run on R with the function <code>metaMDS()</code> of the {vegan} package <span class="citation">(<a href="references.html#ref-R-vegan" role="doc-biblioref">Oksanen et al. 2020</a>)</span>
and <code>Rtsne()</code> of the homonymous package <span class="citation">(<a href="references.html#ref-R-Rtsne" role="doc-biblioref">Krijthe 2018</a>)</span>, respectively.</p>
<p>MDS is an ordination technique, like principal components analysis (PCA). It tries out different low-dimensional configurations and tries to maximize the correlation between the pairwise distances in the high-dimensional space and those in the low-dimensional space: items that are close together in one space should stay close together in the other, and items that are far apart in one space should stay far apart in the other.
It can be evaluated via the stress level, the complement of the correlation coefficient: if the correlation between the pairwise distances is 0.85, the stress level is 0.15.
Unlike PCA, however, the dimensions are not meaningful <em>per se</em>; two different runs of MDS may result in plots that mirror each other while representing the same thing. Nonethelesss, the R implementation rotates the plot so that the horizontal axis represents the maximum variation.
In cognitive linguistics literature both metric <span class="citation">(<a href="references.html#ref-hilpert.correiasaavedra_2017" role="doc-biblioref">Hilpert and Correia Saavedra 2017</a>; <a href="references.html#ref-hilpert.flach_2020" role="doc-biblioref">Hilpert and Flach 2020</a>; <a href="references.html#ref-koptjevskaja-tamm.sahlgren_2014" role="doc-biblioref">Koptjevskaja-Tamm and Sahlgren 2014</a>)</span>
and nonmetric MDS <span class="citation">(<a href="references.html#ref-heylen.etal_2015" role="doc-biblioref">Heylen et al. 2015</a>; <a href="references.html#ref-heylen.etal_2012" role="doc-biblioref">Heylen, Speelman, and Geeraerts 2012</a>; <a href="references.html#ref-depascale_2019" role="doc-biblioref">De Pascale 2019</a>; <a href="references.html#ref-perek_2016" role="doc-biblioref">Perek 2016</a>)</span> have been used.</p>
<p>The second technique, t-SNE <span class="citation">(<a href="references.html#ref-Rtsne2008" role="doc-biblioref">van der Maaten and Hinton 2008</a>; <a href="references.html#ref-Rtsne2014" role="doc-biblioref">van der Maaten 2014</a>)</span>, has also been incorporated in cognitive distributional semantics <span class="citation">(<a href="references.html#ref-depascale_2019" role="doc-biblioref">De Pascale 2019</a>; <a href="references.html#ref-perek_2018" role="doc-biblioref">Perek 2018</a>)</span>.
The algorithm is quite different from MDS, but for our purposes the crucial point is that it prioritizes preserving local similarity structure instead of the global structure: items that are close together in the high-dimensional space should stay close together in the low-dimensional space, but those that are far apart in the high-dimensional space may be even farther apart in low-dimensional space. This leads to nice, tight clusters but the distance between them is less interpretable than in an MDS plot.
T-SNE was the state-of-the-art visualization technique for word vectors in computational linguistics <span class="citation">(<a href="references.html#ref-smilkov.etal_2016" role="doc-biblioref">Smilkov et al. 2016</a>)</span> but is now generally being replaced by UMAP.</p>
<!-- TODO add stuff about UMAP -->
<p>In both cases we need to state the desired number of dimensions before running the algorithm –for visualization purposes, the most useful choice is 2. Three dimensions are difficult to interpret if projected on a 2D space, such as a screen <span class="citation">(<a href="references.html#ref-card.etal_1999" role="doc-biblioref">Card, Mackinlay, and Shneiderman 1999, 18</a>; <a href="references.html#ref-wielfaert.etal_2019" role="doc-biblioref">Wielfaert et al. 2019, 222</a>)</span>. In addition, t-SNE requires setting a parameter called perplexity, which basically sets how many neighbors the preserved local structure should cover.</p>

</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="parameter-settings.html"><span class="header-section-number">2</span> Parameter settings</a></div>
<div class="next"><a href="nephovis.html"><span class="header-section-number">4</span> NephoVis</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#from-corpora-to-clouds"><span class="header-section-number">3</span> From corpora to clouds</a></li>
<li><a class="nav-link" href="#vector-creation"><span class="header-section-number">3.1</span> A cloud machine</a></li>
<li><a class="nav-link" href="#dim-reduction"><span class="header-section-number">3.2</span> Dimensionality reduction</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/montesmariana/phdThesis/blob/master/viz_3.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/montesmariana/phdThesis/edit/master/viz_3.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Cloudspotting: Visual analytics for distributional semantics</strong>" was written by Mariana Montes. It was last built on 2021-05-04.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>
</html>
