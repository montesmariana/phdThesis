<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 2 Parameter settings | Cloudspotting: Visual analytics for distributional semantics</title>
<meta name="author" content="Mariana Montes">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.2"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/header-attrs-2.7/header-attrs.js"></script><script src="libs/jquery-3.5.1/jquery-3.5.1.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.5.3/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.5.3/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.2.4/tabs.js"></script><script src="libs/bs3compat-0.2.4/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><link href="libs/tabwid-1.0.0/tabwid.css" rel="stylesheet">
<script src="https://cdn.jsdelivr.net/autocomplete.js/0/autocomplete.jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/mark.js@8.11.1/dist/mark.min.js"></script><!-- CSS -->
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Cloudspotting: Visual analytics for distributional semantics</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Introduction</a></li>
<li><a class="" href="acknowledgments.html">Acknowledgments</a></li>
<li class="book-part">Visualization tool</li>
<li><a class="" href="an-interface-to-the-world-of-clouds.html"><span class="header-section-number">1</span> An interface to the world of clouds</a></li>
<li><a class="active" href="parameter-settings.html"><span class="header-section-number">2</span> Parameter settings</a></li>
<li><a class="" href="from-corpora-to-clouds.html"><span class="header-section-number">3</span> From corpora to clouds</a></li>
<li><a class="" href="nephovis.html"><span class="header-section-number">4</span> NephoVis</a></li>
<li><a class="" href="hdbscan.html"><span class="header-section-number">5</span> HDBSCAN</a></li>
<li class="book-part">The language of clouds</li>
<li><a class="" href="nonsense-or-no-senses.html"><span class="header-section-number">6</span> Nonsense or no senses?</a></li>
<li><a class="" href="the-nature-of-clouds.html"><span class="header-section-number">7</span> The nature of clouds</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/montesmariana/phdThesis">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="parameter-settings" class="section level1" number="2">
<h1>
<span class="header-section-number">2</span> Parameter settings<a class="anchor" aria-label="anchor" href="#parameter-settings"><i class="fas fa-link"></i></a>
</h1>
<p>In this chapter I will describe the various parameter settings we have explored:
which are the possible decisions, which ones we have set and which were looked at,
why. This should be preceded by an explanation of the workflow itself.</p>
<div id="first-steps" class="section level2" number="2.1">
<h2>
<span class="header-section-number">2.1</span> First steps<a class="anchor" aria-label="anchor" href="#first-steps"><i class="fas fa-link"></i></a>
</h2>
<p>Both the targets and the first and second order features are lemma/part-of-speech pairs,
such as <em>haak/verb</em> (the verb <em>haken</em> ‘to hook/crochet’),
<em>beslissing/noun</em> (the noun <em>beslissing</em> ‘decision’), <em>in/prep</em>
(the preposition <em>in</em> ‘in’).
The features (or context words) can have any part of speech except for punctuation
and have a minimum relative frequency frequency of 1 in 2 million (absolute frequency of 227)
after discarding punctuation from the token count in the full
QLVLNews corpus. There are 60533 such lemmas in the corpus.</p>
<p>(This threshold is more or less arbitrary, but we’re assuming that words with a lower frequency
won’t have a rich enough vectorial representation.)</p>
<p>In the steps between defining corpus and types and obtaining a the token-level vectors,
we have two main kinds of parameters to explore.
<strong>First-order parameters</strong> influence which context features will be selected
from the immediate environment of the target tokens,
while the <strong>second-order parameters</strong> influence the shape of the vectors
that represent such first-order features.</p>
<p>In order to visualize the tokens, we have performed dimensionality reduction, i.e.
a process by which we try to represent relative distances between items in a low-dimensional space
while preserving the distances in high-dimensional space as much as possible.
This procedure will be described in [appropriate section].</p>
</div>
<div id="first-order-selection-parameters" class="section level2" number="2.2">
<h2>
<span class="header-section-number">2.2</span> First-order selection parameters<a class="anchor" aria-label="anchor" href="#first-order-selection-parameters"><i class="fas fa-link"></i></a>
</h2>
<p>We call the immediate context of a token the <strong>first order context</strong>: therefore,
first-order parameters are those that influence which elements in the immediate environment
of the token will be included in modeling said token. This was made in two stages:
one dependent on whether syntactic information was use, and one independent of it.</p>
<p>It goes without saying that the parameter space is virtually unlimited, and decisions
had to be made regarding which particular settings would be explored. We tried to
keep the parameter settings different enough from each other to have some variation.
The decisions were based on a mix of literature <span class="citation">(<a href="references.html#ref-kiela.clark_2014" role="doc-biblioref">Kiela and Clark 2014</a>)</span>,
linguistic intuition and generalizations over the annotation of our very targets.
<!-- TODO Reference to wherever I talk about it -->
As part of the annotation task, the annotators had to select the items in the
immediate context that had helped them select the appropriate tag. In order to remove
noise from misunderstandings and idiosyncrasy, we only looked at pairs (or trios) of
annotators that had agreed with each other and with our final annotation and ranked
the context words over which they had agreed. The distance and dependency information
of these context words were used to inform some of our decisions below.</p>
<p>On the first stage, the main distinction is made by <code>BASE</code>: between bag-of-words (<code>BOW</code>) based
and dependency-based models (<code>LEMMAPATH</code> and <code>LEMMAREL</code>).
The former are further split by window size (<code>FOC-WIN</code>), part-of-speech filters (<code>FOC-POS</code>)
and whether sentence boundaries are respected (<code>BOUND</code>).</p>
<dl>
<dt>
<code>FOC-WIN</code> (first order window)</dt>
<dd>A symmetric window of 3, 5 or 10 tokens to each side of the target was used.
</dd>
<dd>Of course, virtually any other value is possible [add references!]. Windows of
5 and 10 are typical in the literature [sources?], while 3 was enough to capture most
of the context words tagged as informative by the annotators.
</dd>
<dt>
<code>FOC-POS</code> (first order part-of-speech)</dt>
<dd>A restriction was placed to only select (common) nouns, adjectives, verbs and adverbs (<code>lex</code>)
in the surroundings of the token. If no restriction is placed, the value of this parameter is <code>all</code>.
</dd>
<dd>Of course, other selections are possible. [add reference] distinguish between
<code>nav</code>, which only includes common nouns, adjectives, and verbs, and <code>nav-nap</code>, which
expand the selection to proper nouns, adverbs and prepositions.
</dd>
<dd>A more detailed research on different combinations would be material for further
research. As we will see, the <code>lex</code> filter is often redundant with the one based on
association strength.
</dd>
<dt><code>BOUNDARIES</code></dt>
<dd>Given information on the limits of sentences (e.g. in corpora annotated for
syntactic dependencies), we can exclude context words beyond the sentence of the target (<code>bound</code>)
or include them (<code>nobound</code>).
</dd>
<dd>This parameter seems to be virtually irrelevant. It was thought as a way of
leveling the comparison with the dependency-based models, which by definition don’t
include context words beyond the sentence, but they don’t seem to make a difference.
</dd>
</dl>
<p>The distinction between BOW- and dependency-based model doesn’t rely so much on
which context words are selected but on how tailored the selection is to the specific
tokens. For example, a closed-class element like a preposition may be distinctive
of particular usage patterns in which a term might occur. However, such a frequent,
multifunctional word could easily occur in the immediate raw context of the target
without actually being related to it. Unfortunately, just narrowing the window span
doesn’t solve the problem, since it would also drastically reduce the number of
context words available for the token and for any other token in the model.
In contrast, we could also have context words that are directly linked to the target
but separated by many other words in between, and enlarging the window to include
them would imply too much noise for this token and for any other token in the model.</p>
<p>A dependency-based model, instead, will only include context words in a certain
syntactic relationship to the target, regardless of the number of words in between.
The actual selection process takes two forms in our case: by path length and by
relationship. The former, which we call <code>LEMMAPATH</code>, is similar to a window size
but counts the steps in a dependency path instead of slots in a bag-of-words window.
The latter, <code>LEMMAREL</code>, matches the dependency paths to specific templates inspired
by the context words tagged as informative by the annotators.</p>
<p>To exemplify, let’s look at (1) and take <em>herhalen</em> ‘to repeat’ as the target.</p>
<ol class="example" style="list-style-type: decimal">
<li>
<em>De geschiedenis rond Remmelink herhaalt zich.</em> ‘The history around Remmelink repeats itself.’</li>
</ol>
<dl>
<dt><code>LEMMAPATH</code></dt>
<dd>This set of dependency-based models selects the features that enter a syntactic
relation with the target with a maximum number of steps.
</dd>
<dd>The possible values we have included are <code>selection2</code> and <code>selection3</code>, which filter
out context words more than two or three steps away, respectively, and <code>weight</code>, which
gives a larger weight to context words that are closer in the dependency path.
</dd>
<dd>A one-step dependency path is either the head of the target or its direct dependent.
Such features are included by both <code>selection2</code> and <code>selection3</code> and receive a weight of 1 in <code>weight</code>.
In (1) this includes the subject, <em>geschiedenis</em> ‘history,’ and the reflexive pronoun <em>zich</em>,
which depend directly on it. If the target was <em>geschiedenis</em> ‘history,’ <em>herhalen</em> ‘to repeat,’
its head, would be selected.
</dd>
<dd>A two-step dependency path is either the head of the head of the target, the dependent of its dependent,
or its sibling. Such features are included by both <code>selection2</code> and <code>selection3</code> and receive a weight of 2/3 in <code>weight</code>.
In (1) this includes the determiner <em>de</em> and the modifier <em>rond</em> ‘around’ directly depending on
a <em>geschiedenis</em> ‘history.’
</dd>
<dd>A three-step dependency path is either the head of the head of the head of the target,
the sibiling of the head of its head, the dependent of the dependent of its dependent,
or the dependent of a sibling. A typical case of the last path is the subject of a passive construction with a modal,
where the target is the verb in participium (<em>belastingen</em> ‘taxes’ in <em>de belastingen moeten geheven worden</em> ‘the taxes must be levied’).
Such features are included in <code>selection3</code> but excluded from <code>selection2</code> and receive a weight of 1/3 in <code>weight</code>.
In (1) this corresponds to <em>Remmelink</em>, the object of <em>rond</em> ‘around.’
</dd>
<dd>Features more than 3 steps away from the target are always excluded.
While some features four steps away can be interesting, such as passive subjects of a verb with two modals, they are not that frequent and may not be worth the noise included by accepting all features with so many steps between them and the target. To catch those relationships, <code>LEMMAREL</code> is a more efficient method.
There are no context words more than three steps away from the target in (1).
</dd>
<dt><code>LEMMAREL</code></dt>
<dd>This set of dependency-based models selects the features that enter in a certain
syntactic relation with the target. They are tailored to the part-of-speech of the target,
and each group expands on the selection of the group before it. The specific selections
are listed in Table <a href="parameter-settings.html#tab:lemmareltable">2.1</a>.
</dd>
</dl>
<!-- BUG Check formatting for PDF and EPUB because it is ugly in the former and nonexistent in the latter --><template id="ba99d1ff-1838-46ff-a6d7-8d7f68d67de6"><style>
.tabwid table{
  border-collapse:collapse;
  line-height:1;
  margin-left:auto;
  margin-right:auto;
  border-width: 0;
  display: table;
  margin-top: 1.275em;
  margin-bottom: 1.275em;
  border-spacing: 0;
  border-color: transparent;
}
.tabwid_left table{
  margin-left:0;
}
.tabwid_right table{
  margin-right:0;
}
.tabwid td {
    padding: 0;
}
.tabwid a {
  text-decoration: none;
}
.tabwid thead {
    background-color: transparent;
}
.tabwid tfoot {
    background-color: transparent;
}
.tabwid table tr {
background-color: transparent;
}
</style>
<div class="tabwid">
<style>.cl-8c1ec6d2{border-collapse:collapse;}.cl-8c109fb2{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-8c10c6a4{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-8c10c6a5{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-8c113b84{width:1111.3pt;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-8c113b85{width:54.2pt;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-8c113b86{width:1153.4pt;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-8c113b87{width:807.5pt;background-color:transparent;vertical-align: top;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-8c113b88{width:1111.3pt;background-color:transparent;vertical-align: top;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-8c113b89{width:54.2pt;background-color:transparent;vertical-align: top;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-8c113b8a{width:1153.4pt;background-color:transparent;vertical-align: top;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-8c113b8b{width:807.5pt;background-color:transparent;vertical-align: top;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-8c113b8c{width:1111.3pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-8c113b8d{width:54.2pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-8c113b8e{width:1153.4pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-8c1162b2{width:807.5pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}</style>
<div class="inline-table"><table class="cl-8c1ec6d2">
<caption class="Table Caption">
<span id="tab:lemmareltable">Table 2.1: </span>Dependency paths selected by different <code>LEMMAREL</code> values.
</caption>
<thead><tr style="overflow-wrap:break-word;">
<td class="cl-8c113b8d"><p class="cl-8c10c6a4"><span class="cl-8c109fb2">groups</span></p></td>
<td class="cl-8c113b8e"><p class="cl-8c10c6a4"><span class="cl-8c109fb2">nouns</span></p></td>
<td class="cl-8c1162b2"><p class="cl-8c10c6a4"><span class="cl-8c109fb2">verbs</span></p></td>
<td class="cl-8c113b8c"><p class="cl-8c10c6a4"><span class="cl-8c109fb2">adjectives</span></p></td>
</tr></thead>
<tbody>
<tr style="overflow-wrap:break-word;">
<td class="cl-8c113b85"><p class="cl-8c10c6a5"><span class="cl-8c109fb2">1</span></p></td>
<td class="cl-8c113b86"><p class="cl-8c10c6a5"><span class="cl-8c109fb2">modifiers and determiners of the target, items of which the target is modifier or determiner, and verbs of which the target is object or subject</span></p></td>
<td class="cl-8c113b87"><p class="cl-8c10c6a5"><span class="cl-8c109fb2">direct objects, active and passive subjects (with up to two modals for the active one), reflexive complement and prepositions depending directly on the target</span></p></td>
<td class="cl-8c113b84"><p class="cl-8c10c6a5"><span class="cl-8c109fb2">nouns modified by the target and direct modifiers of it (except for prepositions), subject and direct objects of the verbs of which the target is direct modifier or predicate complement, with up to one modal or auxiliary in between</span></p></td>
</tr>
<tr style="overflow-wrap:break-word;">
<td class="cl-8c113b85"><p class="cl-8c10c6a5"><span class="cl-8c109fb2">2</span></p></td>
<td class="cl-8c113b86"><p class="cl-8c10c6a5"><span class="cl-8c109fb2">conjuncts of the target (with or without conjunction), objects of the modifier of the target, and items of whose modifier the target is object</span></p></td>
<td class="cl-8c113b87"><p class="cl-8c10c6a5"><span class="cl-8c109fb2">conjuncts of the target, complementizers, nouns depending through a preposition and verbal complements or elements of which the target is a verbal complement</span></p></td>
<td class="cl-8c113b84"><p class="cl-8c10c6a5"><span class="cl-8c109fb2">object of the preposition modifying the target, conjunct of the target (with or without conjunction), prepositional object of verb modified by target (as modifier or prepositional complement)</span></p></td>
</tr>
<tr style="overflow-wrap:break-word;">
<td class="cl-8c113b89"><p class="cl-8c10c6a5"><span class="cl-8c109fb2">3</span></p></td>
<td class="cl-8c113b8a"><p class="cl-8c10c6a5"><span class="cl-8c109fb2">objects and modifiers of items of which the target is subject or modifier, subjects and modifiers of items of which the target is subject or modifier, modifiers of the modifiers of the target, and items of whose modifier the target is modifier</span></p></td>
<td class="cl-8c113b8b"><p class="cl-8c10c6a5"><span class="cl-8c109fb2"></span></p></td>
<td class="cl-8c113b88"><p class="cl-8c10c6a5"><span class="cl-8c109fb2"></span></p></td>
</tr>
</tbody>
</table></div>
</div></template><div class="flextable-shadow-host" id="3d0573ca-9362-4fff-a5ca-2952b44d6e3d"></div>
<script>
var dest = document.getElementById("3d0573ca-9362-4fff-a5ca-2952b44d6e3d");
var template = document.getElementById("ba99d1ff-1838-46ff-a6d7-8d7f68d67de6");
var caption = template.content.querySelector("caption");
if(caption) {
  caption.style.cssText = "display:block;text-align:center;";
  var newcapt = document.createElement("p");
  newcapt.appendChild(caption)
  dest.parentNode.insertBefore(newcapt, dest.previousSibling);
}
var fantome = dest.attachShadow({mode: 'open'});
var templateContent = template.content;
fantome.appendChild(templateContent);
</script><div id="ppmi-weighting" class="section level3" number="2.2.1">
<h3>
<span class="header-section-number">2.2.1</span> PPMI weighting<a class="anchor" aria-label="anchor" href="#ppmi-weighting"><i class="fas fa-link"></i></a>
</h3>
<p>The <code>PPMI</code> parameter is taken outside the set of first-order parameters because it can both filter out first-order features and reshape their vector representations. In truth, the choice of <strong>p</strong>ositive <strong>p</strong>ointwise <strong>m</strong>utual <strong>i</strong>nformation (PPMI) over other weighting mechanisms, as well as setting a threshold or not, is already a parameter setting, which in these circumstances is set to PPMI and a threshold of 0. In all cases, the PPMI was calculated based on a 4-4 window (that could also be a variable parameter).</p>
<p>This parameter can take three values. <code>selection</code> and <code>weight</code> mean that only the first-order features with a PPMI &gt; 0 with the target type are selected, and the rest discarded, while <code>no</code> does not apply the filter. The difference between <code>selection</code> and <code>weight</code> is that the former only uses the value to filter the context features, while the latter also weighs their vectors with that value.</p>
</div>
<div id="second-order-selection" class="section level3" number="2.2.2">
<h3>
<span class="header-section-number">2.2.2</span> Second-order selection<a class="anchor" aria-label="anchor" href="#second-order-selection"><i class="fas fa-link"></i></a>
</h3>
<p>The selection of second-order features influences the shape of the vectors: how the selected first-order features are represented. While the frequency transformation and the window on which such values were computed could be varied, they were set to fixed values, namely PPMI and 4-4 respectively. The parameters that were varied across, although we don’t expect drastic differences between the models, are vector length and part-of-speech.</p>
<dl>
<dt>
<code>SOC-POS</code> (second order part-of-speech)</dt>
<dd>This parameter can take two values: <code>nav</code> and <code>all</code>. In the former case, a selection of 13771 lect-neutral nouns, adjectives and verbs made by Stefano is taken as the set of possible second-order features. In the latter, all lemmas with frequency above 227 and any part-of-speech are considered.
</dd>
<dt><code>LENGTH</code></dt>
<dd>Vector length is the number of second-order features and therefore the dimensionality of the matrices on which the distance matrices are based, although the amount is not all that changes. It is applied after filtering by part-of-speech.
</dd>
<dd>We have selected two values: <code>5000</code> and <code>FOC</code>. The former includes the 5000 most frequent elements of the possible features, while the latter takes the intersection between the possible second-order-features and the first-order-features, regardless of frequency. With <code>SOC-POS:all</code>, <code>FOC</code> will include all first-order features of that model, while with <code>SOC-POS:nav</code>, only those included in Stefano’s selection.
</dd>
<dd>The actual number of dimensions resulting from <code>FOC</code> depends on the strictness of the first order filter. This information can be found on the plots that, for each staal, show how many first order context words are left after each combination of first order filters.
</dd>
</dl>
<div id="foc-as-soc" class="section level4" number="2.2.2.1">
<h4>
<span class="header-section-number">2.2.2.1</span> FOC as SOC<a class="anchor" aria-label="anchor" href="#foc-as-soc"><i class="fas fa-link"></i></a>
</h4>
<p>What does it mean to use the same first-order context words as second-order context words?</p>
<p>First, depending on the number of target tokens and the strictness of the filter, there could be a different number of context words, ranging in the hundreds or low thousands.</p>
<p>Second, the context words will be compared based on their co-occurrence with each other. The behaviour of a context word outside the context of the target will be largely ignored: of course, the association strength between two items has to do with their co-occurrence across the whole corpus, as well as their non-co-occurrence, but it will only be included in the second order vector of the first item if the second is also among the first order context words.</p>
</div>
</div>
</div>
<div id="medoids" class="section level2" number="2.3">
<h2>
<span class="header-section-number">2.3</span> Medoids<a class="anchor" aria-label="anchor" href="#medoids"><i class="fas fa-link"></i></a>
</h2>
<p>The multiple parameters return a huge number of models, and while purely quantitative methods might
be able to process and compare them, it is not feasible for a human to look at hundreds of clouds
and stay sane enough to make out anything from them. A more efficient –and easier on the human mind–
way to approach this is, instead, to look at representative models.</p>
<!-- QUESTION: Describe PAM? -->
<p>This method requires us to choose a number of medoids beforehand, which is not an easy
task. If we wanted the medoids to represent the best clustering solution, we could run
the algorithm with different values of <span class="math inline">\(k\)</span> and compare the results with measures such
as silhouette width, as suggested by <span class="citation"><a href="references.html#ref-levshina_2015" role="doc-biblioref">Levshina</a> (<a href="references.html#ref-levshina_2015" role="doc-biblioref">2015</a>)</span>. However, that is not
necessarily our goal. We want to be able to see as much variation as possible, while keeping
the number of different models manageable (i.e. below 9). It is not particularly problematic
if these models are redundant, as long as we can ensure that all the phenomena that we
are interested in are represented in them.</p>
<p>For example, given a lemma with multiple senses, it might be the case that some models
group the tokens of one sense, and others group the tokens of another: we would like to see
representatives of both kinds.
<!-- TODO add example --></p>
<p>There is no guarantee that the method with the best silhouette returns all the variation we
are interested in –our goal is, rather, to limit the number of different models we need to
examine from the total number, say 200, to a more manageable amount, like 8.
In the same terms, there is also no guarantee that when we identify something interesting
in a medoid, i.e. an island for a particular usage pattern, all the models in the cluster of
that medoid, and only those models, will share that characteristic. In order to check that,
we can look at random samples (again, of 8 or 9 models) of each of the clusters and
visually compare them to their medoids. This doesn’t need to be as thorough an examination as
that of the medoids themselves: it suffices to check if the random sample is not too different
and seems to share the characteristic of interest. [add example]</p>
<p>In general terms, for the characteristics identified in the case studies that make up this
investigation, we can be quite confident that the medoids are representative of the models in
their clusters. However, depending on the concreteness of the phenomena, the variation across
models, the clarity of the visualization and the wishful thinking that might lurk in the
researchers’ minds, it might be the case that something found or assessed in a medoid is not
shared by the models in its cluster. The comparison needed with the random sample should be
fast and honest and is strongly recommended: if the medoids are representative, you can see it
in an instant; if they are not, it just takes a bit longer to admit it. It is <em>not</em> the same as actually studying and comparing 64 different models.</p>
<!-- TODO add an example of something not working, like hoekig medoid 3? -->

</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="an-interface-to-the-world-of-clouds.html"><span class="header-section-number">1</span> An interface to the world of clouds</a></div>
<div class="next"><a href="from-corpora-to-clouds.html"><span class="header-section-number">3</span> From corpora to clouds</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#parameter-settings"><span class="header-section-number">2</span> Parameter settings</a></li>
<li><a class="nav-link" href="#first-steps"><span class="header-section-number">2.1</span> First steps</a></li>
<li>
<a class="nav-link" href="#first-order-selection-parameters"><span class="header-section-number">2.2</span> First-order selection parameters</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#ppmi-weighting"><span class="header-section-number">2.2.1</span> PPMI weighting</a></li>
<li><a class="nav-link" href="#second-order-selection"><span class="header-section-number">2.2.2</span> Second-order selection</a></li>
</ul>
</li>
<li><a class="nav-link" href="#medoids"><span class="header-section-number">2.3</span> Medoids</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/montesmariana/phdThesis/blob/master/viz_2.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/montesmariana/phdThesis/edit/master/viz_2.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Cloudspotting: Visual analytics for distributional semantics</strong>" was written by Mariana Montes. It was last built on 2021-04-30.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>
</html>
