[{"path":"index.html","id":"introduction","chapter":"Introduction","heading":"Introduction","text":"’m starting first draft ever PhD dissertation.\r\nreports scattered place, try write things already\r\nthinking Final Product. need layout (briefly discussed end February) \r\nslowly start building final text.original title (case, title project) \r\nMethodological triangulation corpus-based distributional semantics,\r\ntriangulation part lost somewhere along way.really develop visualization,\r\nbased Thomas Wielfaert’s original code,\r\nanalyze parameter settings multiple ways,\r\nchecking different combinations exploring different avenues,\r\nlead, check, study compare manual annotation 32 lemmas.’m going write .","code":""},{"path":"acknowledgments.html","id":"acknowledgments","chapter":"Acknowledgments","heading":"Acknowledgments","text":"words pages, thoughts try convey, result \r\nyears thinking, discussing, studying, learning. voice weaves together,\r\ndraws many sources encouraged growth, stood ,\r\nfed curiosity, passion enthusiasm everything makes text.support ideas, want thank supervisors Dirk Geeraerts,\r\nDirk Speelman Benedikt Szmrecsanyi. gave piece, building\r\nconfidence, encouraging keep exploring learning. main supervisor,\r\nDirk Geeraerts, deserves special acknowledgment hours deep\r\ndiscussion complex issue meaning, \r\n. appreciate patience engagement. Every time talked left\r\nfeeling excited passionate topic, confident happy.\r\nHartelijk bedankt.must also thank colleagues Linguistics Department KU Leuven,\r\ndifferent stages years lovely company \r\nsupport system. particular, like thank members \r\nNephological Semantics project, shared much excitement \r\nfrustrations common project.\r\njoined project, already brought history connection\r\n(cognitive) linguistics, corpora, statistics programming. honestly wouldn’t\r\ntoday weren’t parents, Miguel Patricia. \r\nalways supported fostered study interests, given tools \r\ngrow, face new challenges. know can, believe .","code":""},{"path":"an-interface-to-the-world-of-clouds.html","id":"an-interface-to-the-world-of-clouds","chapter":"1 An interface to the world of clouds","heading":"1 An interface to the world of clouds","text":"part (knows many chapters) mean describe\r\nvisualization tool. originally created Thomas Wielfart,\r\naround July 2019 started play around code learn Javascript\r\nD3, culminating present version.section include technical description workflow, pertains\r\ntool processing work made (Python module, \r\nPython R functions), sort manual ’s used. less\r\nredundant paper wrote Kris December (Montes Heylen Forthcoming),\r\nless like vignettes documentation (now, still documented).","code":""},{"path":"parameter-settings.html","id":"parameter-settings","chapter":"2 Parameter settings","heading":"2 Parameter settings","text":"chapter describe various parameter settings explored:\r\npossible decisions, ones set looked ,\r\n. preceded explanation workflow .","code":""},{"path":"parameter-settings.html","id":"first-steps","chapter":"2 Parameter settings","heading":"2.1 First steps","text":"targets first second order features lemma/part--speech pairs,\r\nhaak/verb (verb haken ‘hook/crochet’),\r\nbeslissing/noun (noun beslissing ‘decision’), /prep\r\n(preposition ‘’).\r\nfeatures (context words) can part speech except punctuation\r\nminimum relative frequency frequency 1 2 million (absolute frequency 227)\r\ndiscarding punctuation token count full\r\nQLVLNews corpus. 60533 lemmas corpus.(threshold less arbitrary, ’re assuming words lower frequency\r\nwon’t rich enough vectorial representation.)steps defining corpus types obtaining token-level vectors,\r\ntwo main kinds parameters explore.\r\nFirst-order parameters influence context features selected\r\nimmediate environment target tokens,\r\nsecond-order parameters influence shape vectors\r\nrepresent first-order features.order visualize tokens, performed dimensionality reduction, .e.\r\nprocess try represent relative distances items low-dimensional space\r\npreserving distances high-dimensional space much possible.\r\nprocedure described [appropriate section].","code":""},{"path":"parameter-settings.html","id":"first-order-selection-parameters","chapter":"2 Parameter settings","heading":"2.2 First-order selection parameters","text":"call immediate context token first order context: therefore,\r\nfirst-order parameters influence elements immediate environment\r\ntoken included modeling said token. made two stages:\r\none dependent whether syntactic information use, one independent .goes without saying parameter space virtually unlimited, decisions\r\nmade regarding particular settings explored. tried \r\nkeep parameter settings different enough variation.\r\ndecisions based mix literature (Kiela Clark 2014),\r\nlinguistic intuition generalizations annotation targets.\r\n\r\npart annotation task, annotators select items \r\nimmediate context helped select appropriate tag. order remove\r\nnoise misunderstandings idiosyncrasy, looked pairs (trios) \r\nannotators agreed final annotation ranked\r\ncontext words agreed. distance dependency information\r\ncontext words used inform decisions .first stage, main distinction made BASE: bag--words (BOW) based\r\ndependency-based models (LEMMAPATH LEMMAREL).\r\nformer split window size (FOC-WIN), part--speech filters (FOC-POS)\r\nwhether sentence boundaries respected (BOUND).FOC-WIN (first order window)symmetric window 3, 5 10 tokens side target used.\r\ncourse, virtually value possible [add references!]. Windows \r\n5 10 typical literature [sources?], 3 enough capture \r\ncontext words tagged informative annotators.\r\nFOC-POS (first order part--speech)restriction placed select (common) nouns, adjectives, verbs adverbs (lex)\r\nsurroundings token. restriction placed, value parameter .\r\ncourse, selections possible. [add reference] distinguish \r\nnav, includes common nouns, adjectives, verbs, nav-nap, \r\nexpand selection proper nouns, adverbs prepositions.\r\ndetailed research different combinations material \r\nresearch. see, lex filter often redundant one based \r\nassociation strength.\r\nBOUNDARIESGiven information limits sentences (e.g. corpora annotated \r\nsyntactic dependencies), can exclude context words beyond sentence target (bound)\r\ninclude (nobound).\r\nparameter seems virtually irrelevant. thought way \r\nleveling comparison dependency-based models, definition don’t\r\ninclude context words beyond sentence, don’t seem make difference.\r\ndistinction BOW- dependency-based model doesn’t rely much \r\ncontext words selected tailored selection specific\r\ntokens. example, closed-class element like preposition may distinctive\r\nparticular usage patterns term might occur. However, frequent,\r\nmultifunctional word easily occur immediate raw context target\r\nwithout actually related . Unfortunately, just narrowing window span\r\ndoesn’t solve problem, since also drastically reduce number \r\ncontext words available token token model.\r\ncontrast, also context words directly linked target\r\nseparated many words , enlarging window include\r\nimply much noise token token model.dependency-based model, instead, include context words certain\r\nsyntactic relationship target, regardless number words .\r\nactual selection process takes two forms case: path length \r\nrelationship. former, call LEMMAPATH, similar window size\r\ncounts steps dependency path instead slots bag--words window.\r\nlatter, LEMMAREL, matches dependency paths specific templates inspired\r\ncontext words tagged informative annotators.exemplify, let’s look (1) take herhalen ‘repeat’ target.De geschiedenis rond Remmelink herhaalt zich. ‘history around Remmelink repeats .’LEMMAPATHThis set dependency-based models selects features enter syntactic\r\nrelation target maximum number steps.\r\npossible values included selection2 selection3, filter\r\ncontext words two three steps away, respectively, weight, \r\ngives larger weight context words closer dependency path.\r\none-step dependency path either head target direct dependent.\r\nfeatures included selection2 selection3 receive weight 1 weight.\r\n(1) includes subject, geschiedenis ‘history,’ reflexive pronoun zich,\r\ndepend directly . target geschiedenis ‘history,’ herhalen ‘repeat,’\r\nhead, selected.\r\ntwo-step dependency path either head head target, dependent dependent,\r\nsibling. features included selection2 selection3 receive weight 2/3 weight.\r\n(1) includes determiner de modifier rond ‘around’ directly depending \r\ngeschiedenis ‘history.’\r\nthree-step dependency path either head head head target,\r\nsibiling head head, dependent dependent dependent,\r\ndependent sibling. typical case last path subject passive construction modal,\r\ntarget verb participium (belastingen ‘taxes’ de belastingen moeten geheven worden ‘taxes must levied’).\r\nfeatures included selection3 excluded selection2 receive weight 1/3 weight.\r\n(1) corresponds Remmelink, object rond ‘around.’\r\nFeatures 3 steps away target always excluded.\r\nfeatures four steps away can interesting, passive subjects verb two modals, frequent may worth noise included accepting features many steps target. catch relationships, LEMMAREL efficient method.\r\ncontext words three steps away target (1).\r\nLEMMARELThis set dependency-based models selects features enter certain\r\nsyntactic relation target. tailored part--speech target,\r\ngroup expands selection group . specific selections\r\nlisted Table 2.1.\r\n\r\nTable 2.1: Dependency paths selected different LEMMAREL values.\r\ngroupsnounsverbsadjectives1modifiers determiners target, items target modifier determiner, verbs target object subjectdirect objects, active passive subjects (two modals active one), reflexive complement prepositions depending directly targetnouns modified target direct modifiers (except prepositions), subject direct objects verbs target direct modifier predicate complement, one modal auxiliary between2conjuncts target (without conjunction), objects modifier target, items whose modifier target objectconjuncts target, complementizers, nouns depending preposition verbal complements elements target verbal complementobject preposition modifying target, conjunct target (without conjunction), prepositional object verb modified target (modifier prepositional complement)3objects modifiers items target subject modifier, subjects modifiers items target subject modifier, modifiers modifiers target, items whose modifier target modifier","code":""},{"path":"parameter-settings.html","id":"ppmi-weighting","chapter":"2 Parameter settings","heading":"2.2.1 PPMI weighting","text":"PPMI parameter taken outside set first-order parameters can filter first-order features reshape vector representations. truth, choice positive pointwise mutual information (PPMI) weighting mechanisms, well setting threshold , already parameter setting, circumstances set PPMI threshold 0. cases, PPMI calculated based 4-4 window (also variable parameter).parameter can take three values. selection weight mean first-order features PPMI > 0 target type selected, rest discarded, apply filter. difference selection weight former uses value filter context features, latter also weighs vectors value.","code":""},{"path":"parameter-settings.html","id":"second-order-selection","chapter":"2 Parameter settings","heading":"2.2.2 Second-order selection","text":"selection second-order features influences shape vectors: selected first-order features represented. frequency transformation window values computed varied, set fixed values, namely PPMI 4-4 respectively. parameters varied across, although don’t expect drastic differences models, vector length part--speech.SOC-POS (second order part--speech)parameter can take two values: nav . former case, selection 13771 lect-neutral nouns, adjectives verbs made Stefano taken set possible second-order features. latter, lemmas frequency 227 part--speech considered.\r\nLENGTHVector length number second-order features therefore dimensionality matrices distance matrices based, although amount changes. applied filtering part--speech.\r\nselected two values: 5000 FOC. former includes 5000 frequent elements possible features, latter takes intersection possible second-order-features first-order-features, regardless frequency. SOC-POS:, FOC include first-order features model, SOC-POS:nav, included Stefano’s selection.\r\nactual number dimensions resulting FOC depends strictness first order filter. information can found plots , staal, show many first order context words left combination first order filters.\r\n","code":""},{"path":"parameter-settings.html","id":"foc-as-soc","chapter":"2 Parameter settings","heading":"2.2.2.1 FOC as SOC","text":"mean use first-order context words second-order context words?First, depending number target tokens strictness filter, different number context words, ranging hundreds low thousands.Second, context words compared based co-occurrence . behaviour context word outside context target largely ignored: course, association strength two items co-occurrence across whole corpus, well non-co-occurrence, included second order vector first item second also among first order context words.","code":""},{"path":"parameter-settings.html","id":"medoids","chapter":"2 Parameter settings","heading":"2.3 Medoids","text":"multiple parameters return huge number models, purely quantitative methods might\r\nable process compare , feasible human look hundreds clouds\r\nstay sane enough make anything . efficient –easier human mind–\r\nway approach , instead, look representative models.method requires us choose number medoids beforehand, easy\r\ntask. wanted medoids represent best clustering solution, run\r\nalgorithm different values \\(k\\) compare results measures \r\nsilhouette width, suggested Levshina (2015). However, \r\nnecessarily goal. want able see much variation possible, keeping\r\nnumber different models manageable (.e. 9). particularly problematic\r\nmodels redundant, long can ensure phenomena \r\ninterested represented .example, given lemma multiple senses, might case models\r\ngroup tokens one sense, others group tokens another: like see\r\nrepresentatives kinds.\r\nguarantee method best silhouette returns variation \r\ninterested –goal , rather, limit number different models need \r\nexamine total number, say 200, manageable amount, like 8.\r\nterms, also guarantee identify something interesting\r\nmedoid, .e. island particular usage pattern, models cluster \r\nmedoid, models, share characteristic. order check ,\r\ncan look random samples (, 8 9 models) clusters \r\nvisually compare medoids. doesn’t need thorough examination \r\nmedoids : suffices check random sample different\r\nseems share characteristic interest. [add example]general terms, characteristics identified case studies make \r\ninvestigation, can quite confident medoids representative models \r\nclusters. However, depending concreteness phenomena, variation across\r\nmodels, clarity visualization wishful thinking might lurk \r\nresearchers’ minds, might case something found assessed medoid \r\nshared models cluster. comparison needed random sample \r\nfast honest strongly recommended: medoids representative, can see \r\ninstant; , just takes bit longer admit . actually studying comparing 64 different models.","code":""},{"path":"exploring-parameter-settings.html","id":"exploring-parameter-settings","chapter":"3 Exploring parameter settings","heading":"3 Exploring parameter settings","text":"chapter give brief review parameters fixed \r\ncertain values explored others along certain range.include procrustes/euclidean, addition/average, range PPMI,\r\nvalues set Kiela + Clark…\r\n","code":""},{"path":"nephovis.html","id":"nephovis","chapter":"4 NephoVis","heading":"4 NephoVis","text":"chapter learn use visualization tool explore\r\ncompare token-level vector space models.moment, tool can found \r\nGithub Page, ,\r\nGithub repository can rendered\r\nstatic website. obtains data submodule;\r\ninterested user clone repository just modify path data.code visualization written Javascript, making heavy use \r\nD3.js library, designed beautiful web-based\r\ndata-driven visualization. known steep learning curve,\r\ncan useful think terms R’s vectorized approach: links\r\nDOM elements arrays manipulates based items’ properties.main rationale framework visualization tool developed \r\nThomas Wielfaert (Wielfaert et al. 2019); code can found\r\n\r\n\r\ncurrent implementation exist without foundational setup. However,\r\nnumber available features added later.description tool follow expected workflow user, starting\r\nLevel 1, moving Level 2 diving deeper \r\nLevel 3. Finally, Section 4.4 goes Beta features\r\nrequire better development testing, well ideas might want \r\nimplement future. case, must noted July 2019 \r\n2021-03-22 user developer person, occasional,\r\nvaluable input members Nephological Semantics project. \r\nproject certainly benefit wider input suggestions.","code":""},{"path":"nephovis.html","id":"level_1","chapter":"4 NephoVis","heading":"4.1 Level 1","text":"","code":""},{"path":"nephovis.html","id":"level_2","chapter":"4 NephoVis","heading":"4.2 Level 2","text":"","code":""},{"path":"nephovis.html","id":"level_3","chapter":"4 NephoVis","heading":"4.3 Level 3","text":"","code":""},{"path":"nephovis.html","id":"wishlist","chapter":"4 NephoVis","heading":"4.4 Wishlist","text":"","code":""},{"path":"nonsense-or-no-senses.html","id":"nonsense-or-no-senses","chapter":"5 Nonsense or no senses?","heading":"5 Nonsense or no senses?","text":"part, knows many chapters, delve \r\ntheroetico-methodological insights –theoretical impact, – \r\nanalyses.describe annotation procedure (probably) , importantly,\r\ndiscuss main theoretical observations derived studies.main points, now, :cloud interpretation, see necessarily senses, rather\r\n“common/shared/similar” contexts usage.cloud interpretation, see necessarily senses, rather\r\n“common/shared/similar” contexts usage.single optimal solution every item.single optimal solution every item.second point, different parameter settings ,\r\nspecific items? (kind info pick ?)\r\nalso material first part project’s monograph\r\nsecond point, different parameter settings ,\r\nspecific items? (kind info pick ?)also material first part project’s monographCurrently (still much research ) think \r\ndifferent parameters offer different perspectives, picking different aspects\r\ncontext.\r\nperspectives won’t relevant lemmas –prepositions, dependency-informed,\r\nrelevant hoop nouns. Even relevance gradual.\r\nmight able generalize, classifying items depending parameter settings\r\nrelevant , depending look like certain kinds\r\nparameters.Say, horde heffen look quite good just setting, haten looks awful always.\r\nStill, based current way computing vectors,\r\nadds type-level vectors tokens instead averaging \r\n(thought ).\r\nstill look adjectives revise nouns verbs using medoids based \r\neuclidean distances.analyses change current conclusions? Chan chan chaaaannn…case, try look last two weeks February \r\ndiscuss DG content part 😄.","code":""},{"path":"the-nature-of-clouds.html","id":"the-nature-of-clouds","chapter":"6 The nature of clouds","heading":"6 The nature of clouds","text":"Clouds don’t necessarily show sensesbut usage patterns –senses well distinguished\r\ninsofar match usage patterns. one usage pattern multiple\r\nsenses, multiple usage patterns one sense, , importantly, different degrees\r\ndefinition usage pattern.\r\nmatter frequencywe can infrequent senses/patterns nonetheless strong enough form\r\nclear group; \r\nextremely frequent senses can also grouped separately, don’t necessarily\r\nabsorb minor senses/patterns, pattern characteristic enough.\r\n","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
