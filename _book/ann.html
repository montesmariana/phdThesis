<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 7 Annotation schema | Cloudspotting: Visual analytics for distributional semantics</title>
<meta name="author" content="Mariana Montes">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.2"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/header-attrs-2.9/header-attrs.js"></script><script src="libs/jquery-3.5.1/jquery-3.5.1.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.5.3/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.5.3/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.2.4/tabs.js"></script><script src="libs/bs3compat-0.2.4/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><link href="libs/tabwid-1.0.0/tabwid.css" rel="stylesheet">
<script src="libs/kePrint-0.0.1/kePrint.js"></script><link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<script src="https://cdn.jsdelivr.net/autocomplete.js/0/autocomplete.jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/mark.js@8.11.1/dist/mark.min.js"></script><!-- CSS --><link rel="stylesheet" href="assets/style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Cloudspotting: Visual analytics for distributional semantics</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Abstract</a></li>
<li><a class="" href="acknowledgments.html">Acknowledgments</a></li>
<li><a class="" href="introduction.html"><span class="header-section-number">1</span> Introduction</a></li>
<li class="book-part">Visualization tool</li>
<li><a class="" href="an-interface-to-the-world-of-clouds.html"><span class="header-section-number">2</span> An interface to the world of clouds</a></li>
<li><a class="" href="workflow.html"><span class="header-section-number">3</span> From corpora to clouds</a></li>
<li><a class="" href="params.html"><span class="header-section-number">4</span> Parameter settings</a></li>
<li><a class="" href="nephovis.html"><span class="header-section-number">5</span> NephoVis</a></li>
<li><a class="" href="hdbscan.html"><span class="header-section-number">6</span> HDBSCAN</a></li>
<li><a class="active" href="ann.html"><span class="header-section-number">7</span> Annotation schema</a></li>
<li class="book-part">The language of clouds</li>
<li><a class="" href="the-nature-of-clouds.html"><span class="header-section-number">8</span> The nature of clouds</a></li>
<li><a class="" href="nonsense-or-no-senses.html"><span class="header-section-number">9</span> Nonsense or no senses?</a></li>
<li><a class="" href="no-sky-fits-all.html"><span class="header-section-number">10</span> No sky fits all</a></li>
<li class="book-part">Conclusion</li>
<li><a class="" href="cloudspotters-guide.html"><span class="header-section-number">11</span> Cloudspotter’s guide</a></li>
<li><a class="" href="avenues-for-further-research.html"><span class="header-section-number">12</span> Avenues for further research</a></li>
<li><a class="" href="conclusion.html"><span class="header-section-number">13</span> Conclusion</a></li>
<li><a class="" href="references.html">References</a></li>
<li class="book-part">Appendix</li>
<li><a class="" href="definitions.html"><span class="header-section-number">A</span> Definitions</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/montesmariana/phdThesis">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="ann" class="section level1" number="7">
<h1>
<span class="header-section-number">7</span> Annotation schema<a class="anchor" aria-label="anchor" href="#ann"><i class="fas fa-link"></i></a>
</h1>
<!-- QUESTION Should we include all the definitions and translations and examples here or in an appendix? What about estimated and actual frequencies? -->
<p>For these case studies, we selected 34 Dutch lemmas to annotate and model with token level vector spaces, two of which (<em>herkennen</em> and <em>spoor</em>) were discarded.
The selection process will be presented in Section <a href="ann.html#selection">7.1</a>, with a description of the selected items and what we expected their annotation and clouds to look like. A short description of the corpus <span class="citation">(QLVLNewsCorpus, <a href="references.html#ref-depascale_2019" role="doc-biblioref">De Pascale 2019</a>)</span> will follow. The annotation procedure will be the focus of Section <a href="ann.html#annotation">7.5</a>.
Bachelor students of Linguistics at KU Leuven (later called <em>annotators</em>) were recruited and hired to manually annotate samples of the selected lemmas, and while the administrative procedure itself is not of great interest to the project, a number of practical issues will be discussed: the distribution of tokens, the assignment of tasks (in particular, the graphic interface provided) and the processing/analysis of the data.</p>
<div id="selection" class="section level2" number="7.1">
<h2>
<span class="header-section-number">7.1</span> Selection of items<a class="anchor" aria-label="anchor" href="#selection"><i class="fas fa-link"></i></a>
</h2>
<p>For this case study 34 Dutch lexical items were selected. We aimed to cover a variety of polysemy phenomena, which will be addressed in the specific sections for each part of speech: section <a href="ann.html#nouns">7.1.1</a> for nouns, section <a href="ann.html#adjs">7.1.2</a> for adjectives, and section <a href="ann.html#verbs">7.1.3</a> for verbs.
By modeling different parts of speech and different kinds of polysemy, we expected to develop more robust generalizations regarding the parameter settings that best model specific phenomena.</p>
<p>The selection procedure mixed some introspection (thinking of words that could be interesting), looking up lexical resources (going through a tentative list of dictionary entries to figure out what kind of polysemy we could expect) and corpus data (surveying a sample of concordances for evidence of the expected polysemy). While dictionaries were an essential resource to sketch the sense labels the annotators would have to choose from, we also adjusted them to a more manageable granularity. The concordances were also crucial to estimate sense distribution and adjust the granularity of the definitions. We didn’t want an overwhelmingly frequent sense to affect the annotators judgement, and very infrequent senses might be hard to model or at least to visualise in the 2D representations. Still, we did allow for some complexity and subtlety in some cases.</p>
<p>In a number of cases, the corpus survey (reading a concordance of 40-50 randomly selected instances) invalidated options that intuitively or according to the dictionary definitions would have conformed to our requirements. When judging such a discrepancy, it is important to take into account the composition of the corpus. The topics addressed in newspapers and the terms used to talk about them are certainly not representative of everyday life or the entirety of language.</p>
<p>We also had cases of adjectives that could be used in adverbial form and were not always properly tagged for part-of-speech, so we had to discard them. We made a difference between cases such as <em>hoopvol</em> ‘hopeful,’ which often occurs in predicative contexts with a verb that is not copula (e.g. <em>ik ben hoopvol gestemd</em> ‘it makes me hopeful,’ <em>hij kijkt hoopvol omhoog</em> ‘he looks up hopeful(ly)’) but still predicates over an entity, and cases such as <em>gemiddeld</em> ‘average,’ which could either predicate over an entity, as in <em>gemiddelde student</em> ‘average student,’ or a predicate, as in <em>zij eet gemiddeld 3 koekjes elke dag</em> ‘she eats in average 3 cookies per day.’ Sometimes the incorrectly tagged cases (adverbs tagged as adjectives) were infrequent enough to be dismissed, but in some other cases they were so many we had to discard the lemma as candidate. While the only direct consequence is that a certain potentially interesting lemma couldn’t be investigated, this also should be taken into account when relying on the part-of-speech tagger in other steps of the workflow.</p>
<p>The next subsections describe the selected nouns, adjectives and verbs, the QLVLNewsCorpus and the sampling method. This includes a general description of the polysemy phenomena and hypotheses, but the specific definitions, examples and translations can be found in Appendix XX.
<!-- TODO add appendix with definitions and proper references --></p>
<!-- TODO use flextable -->
<div id="nouns" class="section level3" number="7.1.1">
<h3>
<span class="header-section-number">7.1.1</span> The nouns<a class="anchor" aria-label="anchor" href="#nouns"><i class="fas fa-link"></i></a>
</h3>
<p>The 8 nouns all exhibit homonymy and polysemy in at least one of the homonyms.</p>
<p>Three nouns have one frequent, monosemous homonym and a less frequent, polysemous one: <em>hoop</em> ‘hope/bunch,’ <em>spot</em> ‘ridicule/show or spotlight’ and <em>horde</em> ‘horde/hurdle.’ The polysemy phenomena are varied: metaphor for <em>horde</em> ‘hurdle,’ metaphor/generalization for <em>hoop</em> ‘heap’ and metonymy for <em>spot</em> (‘short video/spotlight’). In this latter case, the ‘spotlight’ sense can further be used literally or metaphorically, but this distinction was not included among the definitions given to the annotators.
<!-- COMBAK Does it show up in the dictionaries? --></p>
<p>Four nouns have two polysemous homonyms: <em>schaal</em> ‘scale/dish or shell,’ <em>blik</em> ‘look/tin,’ <em>stof</em> ‘substance or fabric or topic/dust,’ <em>staal</em> ‘steal/sample.’
For <em>blik</em>, the frequent homonym (‘look’) has a concrete sense with a metonymic and a metaphoric extension, while the infrequent one can refer to a material (‘tin’), an object made of that material or its content: the distinction is quite clear but might depend on the specificity of the context and be very infrequent.
<!-- NOTE I might want to revise this. I had misintepreted the distinction, but so did the annotators, and the metonymic sense actually didn't exist... but there is a metaphoric sense, that is just super infrequent!! -->
Similarly, <em>stof</em> presents one frequent homonym with two concrete, referentially distinct senses (‘substance’ and ‘fabric’) andd an abstract one (‘topic’), and another with a subtle, context-specificity dependent difference (‘dust (in the air)’ or ‘reducing something to dust, pulverize’). <em>Schaal</em> exhibits subtle perspective shifts in one homonym (‘scale,’ e.g. “scale of Celsius” as opposed to “large scale”) and refers to different concrete objects with the second (‘shell,’ ‘dish,’ ‘scale dish’).
Finally, <em>staal</em> ‘steal’ could refer, like <em>blik</em> ‘tin,’ to either the material or an object made of it, while the ‘sample’ homonym is sensitive to construal (sample as evidence, or in general): it’s likely to present high confusion and/or a very skewed distribution in both homonyms separately.</p>
<ul>
<li>Finally, <em>spoor</em> has three homonyms, of which two polysemous: ‘footprint or trace/train(line, rail, company)/spur.’ This noun was later discarded because it proved too complicated, but the data is available for reanalysis.</li>
</ul>
<!-- The last noun, *spoor* (Table \@ref(tab:spoor)), exists to spice up the task. It presents at least three distinct homonyms (the one meaning 'spore' was not deemed frequent enough to show up in the definitions). One, in the realm of 'footprint, trace', is presented with four senses: a concrete (foot)print, a generalized application referring to general evidence of someone's (past) presence, one metaphorical and one more specific where the entity to be identified (a substance) is indeed present but in small quantities. The second homonym, relating to trains, could refer to either the vehicle itself, the railroad or the company, and the distinction will depend on the clarity of the context. Finally, the homonym relating to spurs occurs mostly in an idiom; the figurative sense is not explicit in the definition, but the example *is* the idiom, so it should be easy enough to identify. -->
</div>
<div id="adjs" class="section level3" number="7.1.2">
<h3>
<span class="header-section-number">7.1.2</span> The adjectives<a class="anchor" aria-label="anchor" href="#adjs"><i class="fas fa-link"></i></a>
</h3>
<p>The selection of adjectives includes 13 lemmas presenting different kinds of polysemy phenomena.</p>
<p>Three adjectives have a metonymic reading: <em>hoopvol</em> ‘hopeful,’ <em>geestig</em> ‘witty’ and <em>hachelijk</em> ‘dangerous/critical.’
For <em>geestig</em> and <em>hoopvol</em>, one of the senses is anthropocentric, i.e. it’s mainly or exclusively applied to people, although such distinction is not made explicit in the definitions of <em>geestig</em> but only suggested in the example. The expected frequency of the anthropocentric sense is in both cases much higher than the other one.
In <em>hachelijk</em>’s case, the difference is a matter of temporal or telic perspective, so probably harder to distinguish, and it’s probably more likely that annotators suggest the second sense as an alternative to the first one (assigning the ‘critical’ interpretation to something potentially dangerous) than the other way around.</p>
<p>Four adjectives have metaphoric readings: <em>hoekig</em> ‘angulous/clumsy,’ <em>dof</em> ‘dull,’ <em>heilzaam</em> ‘healthy/beneficial’ and <em>gekleurd</em> ‘colorful, person of color, tainted.’
<em>Heilzaam</em> has two distinctions, somewhere between metaphoric and specialization: one refers to something specifically/literally healthy, and the other one is broader and less concrete.
<em>Hoekig</em> and <em>gekleurd</em> present three sense distinctions, one of which is particularly concrete and the most frequent and another one explicitly anthropocentric. The third sense distinction has a different quality: synaesthetic for <em>hoekig</em> and very much metaphoric for <em>gekleurd</em>.
Finally, <em>dof</em> has all four kinds of senses: concrete, synaesthetic, anthropocentric and abstract.</p>
<p>Three adjectives present some other form of similarity between the readings: <em>geldig</em> ‘valid,’ <em>hemels</em> ‘heavenly’ and <em>gemeen</em> ‘shared/public/mean/serious.’
<em>Geldig</em> ‘valid’ and <em>hemels</em> ‘heavenly’ offer two options, one restricted to a specific context and one much broader. The relation between the relative frequencies of those senses are inverted: the specific sense of <em>geldig</em> is less frequent than the general one, while in <em>hemels</em> it’s the other way around.
We would expect that the specific sense would not be offered as alternative to the general sense as much as the other way around.
The case of <em>gemeen</em> is quite complex, involving a number of rather subtle distinctions. The limits between the first and the second one and between the third and the fifth are hard to establish; the fourth sense seems more clear but if the context isn’t specific enough it could be easily confused with the fifth. In addition, the senses are not always mutually exclusive, and a certain instance could very well conflate or be ambiguous between two senses.</p>
<p>Finally, we have three more complex adjectives: <em>heet</em> ‘hot’ for different entities and metaphorically, <em>grijs</em> ‘gray,’ with metaphorical and metonymical extensions and <em>goedkoop</em> ‘cheap,’ with different entities and metaphorically.</p>
<p><em>Heet</em> ‘hot’ presents, first, three very concrete senses that differ in perspective: temperatures of different kinds of things.
<!-- IDEA: cite K-tamm? -->
The second half is metaphorical, of which one synaesthetic, one anthropocentric and very specific, and one more abstract and also quite specific. Crucially, there is no exclusive sense tag for idiomatic expressions, which are quite frequent; they are expected to be tagged with the concrete senses (and maybe a comment on their figurative interpretation), but annotators might also use the <em>geen</em> tag for those cases.</p>
<p><em>Grijs</em> presents a very frequent, concrete sense, two specific metonymic extensions, one anthropocentric sense, one rather abstract and another very specific metaphor.</p>
<p><em>Goedkoop</em>, on the other hand, presents a modest set of 4 sense distinctions: a concrete, prototypical and frequent sense, two perspectival shifts and a clear metaphor.</p>
</div>
<div id="verbs" class="section level3" number="7.1.3">
<h3>
<span class="header-section-number">7.1.3</span> The verbs<a class="anchor" aria-label="anchor" href="#verbs"><i class="fas fa-link"></i></a>
</h3>
<p>For the verbs, we selected a range of combinations of syntactic and semantic variation:</p>
<p>Four verbs are always trasitive, and the sense distinction is related to the objects they can take: <em>haten</em> ‘hate,’ <em>huldigen</em> ‘honor/hold (attitudes, opinions, stances),’ <em>heffen</em> ‘raise,’ <em>herroepen</em> ‘annul (a law)/retract (statement)’)
<em>Haten</em> and <em>heffen</em> are probably more easy to distinguish, the former having an anthropocentric distinction (basically, hating people against disliking things, but with possible gray areas in between, depending on how that object is construed) and the latter presenting a rather clear and common metaphor, between physical objects and abstract entities such as taxes being levied.
<em>Huldigen</em> and <em>herroepen</em> instead have slightly more subtle differences, but the former (between honoring someone/something and holding and opinion) is probably stronger and easier to distinguish than the latter, between retracting a statement or annuling a decree (which again could be interpreted differently depending on how the entity is construed, how prototypical it is).</p>
<p>Two of the verbs can be transitive, with a distinction based on the object, or intransitive: <em>helpen</em> ‘help,’ <em>herstructureren</em> ‘restructure.’
These verbs are quite subtle and might present a lot of confusion, particularly because the intransitive uses are semantically very similar to one of the transitive cases.
For <em>herstructureren</em>, one transitive sense and the intransitive one (exemplified with a reflexive…) are more specific, regarding companies and with the connotation that the personnel is being reduced, while the other transitive sense is broader and might be selected in contexts with less specificity. It might also depend on world knowledge (whether the annotators know or can guess that a certain object -or the subject in the intransitive construction- is a company) and how prominent the implication of personnel reduction is.
For <em>helpen</em>, the distinction between the transitive uses is rather subtle (the “collaboration” sense is exclusive of animate subjects, but that’s not explicit in the definitions), so there might be some disagreement in their annotation, but if the intransitive sense is confused with the transitive ones, it should only be with the first one.</p>
<p>Three verbs can be transitive, with a distinction based on the object, or reflexive: <em>diskwalificeren</em> ‘disqualify,’ <em>herhalen</em> ‘repeat,’ <em>herinneren</em> ‘remember/remind.’ This category initially included <em>herkennen</em> ‘recognize’ but it was discarded.
For some verbs this opposition can be interpreted as a specific situation where the object and the subject coincide.
This is particularly the case with <em>diskwalificeren</em>, where the reflexive argument structure pretty much replicates the transitive senses, in a particular case where someone disqualifies themselves. The possibility to distinguish between the transitive cases, which differ in specificity (sport context against more general, mostly political context), relies instead on the clarity of the context.
<!-- For *herkennen*, the sense distinction between the three transitive uses is quite subtle, and much sharper between transitive and reflexive; -->
for <em>herhalen</em>, what could be an object in the transitive senses (but probably wouldn’t) is the subject in the reflexive, so the distinction should be very clear, while the transitive uses differ in the kind of objects that they take, with certain prototypical nouns (and the possibility of clauses for the second sense) and maybe some borderline cases.
Finally, <em>herinneren</em> shows both a clear distinction between reflexive and transitive uses and a further argument structure distinction between transitive uses, either with or without an <em>aan</em> complement (which might be absent in the restricted context).</p>
<p>Two more verbs can be transitive, intransitive or reflexive, with semantic distinctions within the transitive structure: <em>harden</em> ‘make/become hard, tolerate,’ <em>herstellen</em> ‘heal/repair.’
In the case of <em>harden</em>, the transitive can be concrete, figurative, or concrete with a different sense and in a specific construction, namely <em>(niet) te harden</em>; the intransitive structure is similar to the concrete transitive, but taking its object as subject, and the reflexive is similar to the second transitive. If senses of different argument structures were confused, the intransitive would be with the first and the reflexive with the second. The <em>(niet) te harden</em> uses should be easy to isolate, with strong agreement between annotators and high confidence.
For <em>herstellen</em>, the transitive structure presents three possible senses: one concrete, one figurative but not presented as such, and one abstract that is very subtly different from the second one. The reflexive is very close to the figurative sense and the intransitive is more specific to concrete healing (rather than repairing) and should not be confused with the others.</p>
<p>Finally, we included a verb with semantic distinctions within both the transitive and the intransitive structures: <em>haken</em>.
It was presented with two transitive senses, two intransitive and one transitive/intransitive. The transitive senses can be both concrete and literal and differ in specificity: one sense refers particularly to making somebody trip. Intransitive uses differ in literality and, while both might occur with <em>blijven</em>, only the figurative definition mentions it (apparently restricting it). No figurative options are mentioned for the transitive senses, so if they occur annotators might either tag them as transitive concrete, intransitive figurative, or <em>geen</em>. Finally, one sense that can occur as transitive or intransitve (ellided object) is that of ‘crochet’; it’s so specific that it shouldn’t be confused with others and would probably have high confidence.</p>
</div>
</div>
<div id="expectations-for-the-annotation" class="section level2" number="7.2">
<h2>
<span class="header-section-number">7.2</span> Expectations for the annotation<a class="anchor" aria-label="anchor" href="#expectations-for-the-annotation"><i class="fas fa-link"></i></a>
</h2>
<!-- IDEA Maybe move this to a presentation of what the annotation actually looked like? -->
<p>Here a first set of expectations for the annotations of the types has been summarized in six points, but discussion and revision are needed. They are formulated as predictions and followed by suggestions on how to confirm them. Some ‘technical’ terms are:</p>
<ul>
<li>
<strong>majority sense</strong>, meaning the sense tag that most of the annotators assigned to a given token.</li>
<li>
<strong>alternative sense/annotation</strong>, meaning a sense tag assigned to a given token, different from the majority sense.</li>
<li>
<strong>(be) confuse(d)</strong>, meaning there is disagreement on the annotation.
<!-- Should still find out how to talk about the asymmetry: some senses can have alternatives or be alternatives or both. -->
</li>
</ul>
<!-- NOTE: It could also be useful to find some literature into this kind of annotations. I'm identifying the anthropocentrism of some sense distinctions as particularly foregrounded, but other than some considerations in the metaphor literature I don't really have theoretical backup. On the other hand, what I've seen of the 'geen' annotations this far partially supports this intuition: some annotators would refuse to assign the *horde 1* tag to cases of *horde* + *KTM's/vrachtwagens/danceprojecten/insecten*. --><div id="for-all-types" class="section level3" number="7.2.1">
<h3>
<span class="header-section-number">7.2.1</span> For all types<a class="anchor" aria-label="anchor" href="#for-all-types"><i class="fas fa-link"></i></a>
</h3>
<ol style="list-style-type: decimal">
<li>
<strong>Very specific senses will not be confused with more general senses</strong>.
When the majority sense is a very specific one, the only alternative annotation will be <em>geen</em>.
Concretely:</li>
</ol>
<ul>
<li>
<em>haken 5</em> and <em>haken 3</em> will not be confused with each other nor with other senses.</li>
<li>
<em>heet 4</em> should not be confused with other senses.</li>
</ul>
<!-- 2. **Literal/concrete senses will be easier to agree upon than figurative senses** --><!-- If there are distinct options for literal and figurative senses, when majority sense is concrete/physical, alternative annotations will be rarely figurative. The literal option, instead, will be a more frequent alternative for the figurative majority cases. --><!-- Or: cases with more confusion in the annotation are probably not concrete, or just don't have enough context. --><ol start="2" style="list-style-type: decimal">
<li>
<strong>Metaphor will be easier to identify than metonymy/specialization</strong>
If the metaphoric sense is an option distinct from the concrete/literal one, it won’t often be confused with the literal counterpart; annotators will agree it’s figurative. For metonymy and specialisation, there will be more disagreement and less confidence.
Concretely:</li>
</ol>
<ul>
<li>
<em>blik 1.1</em> will be confused with <em>1.2</em> more than with <em>1.3</em>.</li>
<li>
<em>grijs 1</em> will be confused with <em>2</em> and <em>3</em> more than with <em>6</em>.</li>
<li>
<em>goedkoop 1</em> will be confused with <em>2</em> and <em>3</em> more than with <em>4</em>
</li>
<li>adjectives with metaphoric distinctions (<em>hoekig</em>, <em>dof</em>, <em>gekleurd</em>, <em>heilzaam</em>) will present less confusion than those with metonymic distinctions (<em>hachelijk</em>, <em>hoopvol</em>, <em>geestig</em>)</li>
</ul>
<ol start="3" style="list-style-type: decimal">
<li>
<strong>Anthropocentric senses will be more easily distinguishable</strong>.
If the definition explicitly restricts the application to people, it won’t be alternative annotation with other, non anthropo-exclusive senses. Borderline cases, due probably to unspecified context, would have low confidence.
Concretely:</li>
</ol>
<ul>
<li>There should be low confusion in <em>haten</em>, or at least low confidence in borderline cases</li>
<li>
<em>heet 5</em> should not be confused with others</li>
<li>
<em>grijs 4</em> should not be confused with other senses (except maybe 3, which is derived from it)</li>
<li>
<em>gekleurd 2</em> should not be confused with others</li>
<li>
<em>hoekig 3</em> should not be confused with others</li>
<li>
<em>dof 3</em> should not be confused with others</li>
<li>
<em>hoopvol</em> will present less confusion than <em>geestig</em> and they both will present less confusion than <em>hachelijk</em>
</li>
</ul>
</div>
<div id="only-for-nouns" class="section level3" number="7.2.2">
<h3>
<span class="header-section-number">7.2.2</span> Only for nouns<a class="anchor" aria-label="anchor" href="#only-for-nouns"><i class="fas fa-link"></i></a>
</h3>
<ol start="4" style="list-style-type: decimal">
<li>
<strong>Homonyms will not be confused with each other</strong>.
When a majority sense is from one homonym, alternative annotations will be of the same homonym or <em>geen</em>.</li>
</ol>
</div>
</div>
<div id="only-for-verbs" class="section level2" number="7.3">
<h2>
<span class="header-section-number">7.3</span> Only for verbs<a class="anchor" aria-label="anchor" href="#only-for-verbs"><i class="fas fa-link"></i></a>
</h2>
<p>The next two predictions overlap, and could explain different verb groupings (sometimes, different argument structure <em>implies</em> subject distinction, but which is more prominent?).</p>
<ol start="5" style="list-style-type: decimal">
<li>
<strong>Argument structure will be easier to identify than semantic differences.</strong>
Senses that differ in argument structure will not be confused with each other (won’t be each other’s alternatives) as much as senses with the same argument structure but different kinds of objects. The distinction will be probably easier to make with reflexive than with intransitive cases.
Concretely: in general, transitive senses will be confused with each other but not with senses of a different argument structure (unless that sense is semantically very similar to that transitive sense). Cases that might generate confusion between different argument structures are:</li>
</ol>
<ul>
<li>gral. trans. <em>haken</em> and fig. intrans. <em>haken</em> in cases of fig. trans. <em>haken</em>
</li>
<li>fig. trans. <em>harden</em> and refl. <em>harden</em>
</li>
<li>fig./abs. trans. <em>herstellen</em> and refl. <em>herstellen</em>
</li>
<li>spec. trans. <em>herstructrureren</em> and intrans. <em>herstructureren</em>
</li>
<li>
<em>helpen 1</em> and intrans. <em>helpen</em>
</li>
</ul>
<ol start="6" style="list-style-type: decimal">
<li>
<strong>Senses that require different subjects will be easier to identify than senses that require different objects or prepositional arguments</strong>
Senses with different subject restrictions won’t be each other alternatives. I’m thinking that subject restrictions are often linked to animacy, while object restriction might be more subtle in these cases.
Concretely: in general, senses of any argument structure will not be confused with each other if one takes (mostly) animate subjects and the other one (mostly) inanimate subjects.</li>
</ol>
</div>
<div id="corpus" class="section level2" number="7.4">
<h2>
<span class="header-section-number">7.4</span> The corpus and samples<a class="anchor" aria-label="anchor" href="#corpus"><i class="fas fa-link"></i></a>
</h2>
<p>The exploration of these samples of concordances also served for the calculation of the number of tokens we would have annotate. Regardless of the actual frequency of the items in the corpus, we extracted a minimum 240 tokens of each type (thinking of 6 batches of 40 tokens), and raised the amount to 280 if any of the senses had a relative frequency below 20% in the sample, to 320 if it was below 10%, and 360 if there were many senses and therefore some had a low frequency.
<!-- The table below illustrates this distribution. --></p>
<!-- SWEET: Our plan for the annotation is to distribute the sample of concordances in batches of 40 tokens and hire students to annotate one batch of each of 12 different types. Hiring 40 students, every token is annotated by 2 different students; since some of them are willing to annotate twice as much, some will be annotated by 3. -->
<p>The sample of tokens was selected almost absolutely randomly. First all the instances of each type were extracted from the corpus; then, for each type as many <em>files</em> as tokens we wanted to extract were selected, and from each file I randomly selected one token. Therefore, there are no two instances of the same lemma from the same file in the samples. There were, however, a few duplicates, due to repetition of the same fragment on different dates.</p>
<p>The corpus is a selection of the LeNC and TwNC corpora, which include newspapers articles from Flanders and the Netherlands. This selection, performed by <span class="citation"><a href="references.html#ref-depascale_2019" role="doc-biblioref">De Pascale</a> (<a href="references.html#ref-depascale_2019" role="doc-biblioref">2019</a>)</span> with an eye on a lectally balanced corpus, contains 4,614,267 types and 519,996,217 tokens (roughly 520M, 260 from Flanders and 260 from the Netherlands). The articles in the subcorpus were published between 1999 and 2004 in both quality and popular newspapers from both countries.</p>
</div>
<div id="annotation" class="section level2" number="7.5">
<h2>
<span class="header-section-number">7.5</span> Annotation procedure<a class="anchor" aria-label="anchor" href="#annotation"><i class="fas fa-link"></i></a>
</h2>
<div id="assigning-batches-to-annotators" class="section level3" number="7.5.1">
<h3>
<span class="header-section-number">7.5.1</span> Assigning batches to annotators<a class="anchor" aria-label="anchor" href="#assigning-batches-to-annotators"><i class="fas fa-link"></i></a>
</h3>
<p>In October 2019, 48 students from the General Linguistics course of the 2nd year of the Bachelor in Linguistics in KU Leuven were recruited to work as annotators. Each of them was tasked with annotating 40 tokens of each of 12 types (at least three nouns, four adjectives and four verbs, plus one of either of the categories), a total of 480 tokens, for which we expected them to work an average of 10 hours, spread over 6 weeks. Students had the option of subscribing to double the number of tokens (and hours, and pay). Both the types and the sets of tokens were assigned randomly, while keeping in mind the part-of-speech distribution. It was the intention to shuffle the samples of each lemma before splitting them into batches, but something went wrong with the code and they were ordered by source.</p>
<p>The annotation involved three compulsory tasks and one normally optional. For each of the tokens, the annotators had to:</p>
<!-- TODO add and refer appendix -->
<ol style="list-style-type: decimal">
<li>assign a sense from a predefined set of definitions, as shown in Appendix XX. If none of the senses was satisfactory, they may choose a “None of the above” option;</li>
<li>express the confidence of their decision in a Likert scale of 6 values;</li>
<li>identify the words of the context that helped them assign a sense, comprising 15 tokens to the left and to the right of the target, disregarding sentence boundaries but respecting those of the article;</li>
<li>if they couldn’t assign a sense, they had to explain why. If they did assign one, they still had the option of adding extra information or thoughts on the annotation process, but it was not compulsory.</li>
</ol>
</div>
<div id="annotation-toolinterface" class="section level3" number="7.5.2">
<h3>
<span class="header-section-number">7.5.2</span> Annotation tool/interface<a class="anchor" aria-label="anchor" href="#annotation-toolinterface"><i class="fas fa-link"></i></a>
</h3>
<p>Since entering textual information in a spreadsheet can easily lead to typos and inconsistencies and, furthermore, annotating the relevant context words (cues) is challenging in such a tool, a user-friendly visual interface was designed that transforms button-output into a json file with all the information required.</p>
<!-- COMBAK Make the interface available again in its original form??? -->
<p>The <a href="http://montesmariana.github.io/Annotation/">interface</a> had a menu of types and, for the selected type, two tabs: an overview of the concordance lines and an annotation workspace. In the annotation workspace, they could read each line individually, click on the button corresponding to the sense they wanted to assign, rate their confidence with star rating, click on the words they found useful and enter any other comments. The overview section didn’t only let them see the whole set of tokens to analyze, but the target items changed color once they had been annotated and were themselves links to the Annotation tab for their concordance lines.</p>
<p>The interface was rendered as a webpage via Github Pages, but only processed the information: the annotators had to download (and if necessary upload) their progress as a JSON file and eventually send it by mail.</p>
<p>The goals of the interface were twofold. First, it would reduce typos and inconsistencies for values that should be straightforward and present little variation: it was much faster to design the interface than it would have been to check the typos in 480 tokens times 40 annotators. Second, it would make the annotation experience simpler and even more pleasant, letting the annotators focus their energies on the lexicographical task itself rather than in technicalities.</p>
<p>This is particularly evident for the task of selecting the relevant context words. With a spreadsheet, it would be either necessary to have separate rows per context words and annotate all of them (to make sure you are not forgetting any) or make a list of items in a cell of the row of the relevant concordance. That list would need to have something truly identifying of the context word, therefore not the form or the lemma but the position (since a same item could occur more than once in the context and not always have the same relevance), and counting them reliably would be time consuming and prone to errors. Clicking on the words so that the program itself lists the position of the relevant words, also making it visible which words were selected, solves both the easiness and reliability issues.</p>
<div id="known-issues" class="section level4" number="7.5.2.1">
<h4>
<span class="header-section-number">7.5.2.1</span> Known issues<a class="anchor" aria-label="anchor" href="#known-issues"><i class="fas fa-link"></i></a>
</h4>
<p>The interface did have some issues, the consequences of which affect the output.</p>
<p>One technical issue was a bug in the code of the annotation, for which context words selected by an annotator might be replaced by other context words of the same wordform, but in a previous position. Once that bug was found, the annotators were warned, but not all of them necessarily checked their previous annotation very thoroughly. In any case, this only affects wordforms that occur more than once in the same concordance (which is not very often) and could be cleaned with some reasoning.</p>
<p>Another issue had to do with the format of the corpus, and could have been dealt with better. On the one hand, different sentences in the concordance were indiciated with a <code class="sourceCode html"><span class="kw">&lt;sentence&gt;&lt;/sentence&gt;</span></code> but had no impact in the rendering of the concordance, they were just replaced by empty spaces. They could’ve been replaced by <code class="sourceCode html"><span class="kw">&lt;p&gt;&lt;/p&gt;</span></code> tags. On the other hand, at some point of the corpus processing (before we had access to it), someone must have replaced all <em>&amp;</em> with <em>and</em>, so that HTML entities like <em>&amp;quot;</em>, which would’ve translated into a quotation mark <em>"</em>, are rendered as <em>andquot;</em>, which was extremely confusing for the annotators, especially in already complicated concordances full of them. This issue was identified too late (and in any case, if the corpus already reads it as <em>andquot;</em> instead of <em>"</em>, the confusion is for both).</p>
<!-- #### Output -->
<!-- <!-- NOTE This should be rephrased, and maybe the data should be included in the tokenclouds repository adding this as documentation. -->
<p>–&gt;</p>
<!-- From each student, we received a file in json format where both their username and annotations were recorded. After checking that all tokens were annotated in all required variables (and that all 'geen' cases had comments), the results were turned into tables and merged. After a number of attempts, we have two main tables: -->
<!-- First, a [register of tokens](C:/Users/u0118974/Box Sync/Nederlands wolken/Output/Merges/token_annotation.tsv), where each row is a token (id: **token_id**) and the variables are only: -->
<!-- - **type**: the type that token belongs to; -->
<!-- - **batch**: the batch (set of 40 tokens) that token belongs to, named with the type plus a number; -->
<!-- - **majority_sense**: the majority sense, or the tag that most of the annotators agreed on; -->
<!--     - When the tag was 'geen' ("none of the above"), I classified the comments into kinds of justification that became alternative senses: *between* (doubt between given alternatives), *not_listed* (suggestion of a different alternative from those given), *unclear* (insufficient or confusing context, unknown words) and *wrong_lemma* (issues with lemmatization, part-of-speech tagging or even spelling, so that the concordance does not really correspond to the wanted target); -->
<!--     - when there was no agreement between two annotators (or three in the case of four annotations), the majority sense becomes *no_agreement*; -->
<!-- - **majority_agree**: the proportion (0-1) of annotators that voted for the majority sense; -->
<!-- - **majority_conf**: the mean confidence (standardized by username-type combination) of the agreeing annotations; -->
<!-- - **mean_conf**: the mean confidence (standardized by username-type combination) of all the annotations of the token. -->
<!-- Second, a [register of annotations](C:/Users/u0118974/Box Sync/Nederlands wolken/Output/Merges/long_tokens.tsv), where each row is a token-annotation combination (there is no row-id, but **token_id** identifies the tokens and **annotator** identifies the annotations as *ann_1*, *ann_2*, *ann_3* or *ann_4*). For each row, the following variables are registered: -->
<!-- - **type**: the type that token belongs to; -->
<!-- - **batch**: the batch (set of 40 tokens) that token belongs to, named with the type plus a number; -->
<!-- - **username**: the name of the annotator (easier for me to keep track of, but nothing to publish); -->
<!-- - **code**: codename of the annotator, combining the number of 'student' (set of batches of 12 different types) and number of annotator (matching the **annotator** variable); -->
<!-- - **annotators**: number of annotators who tagged the given token (normally 3). Not a very important variable; -->
<!-- - **original_sense**: the sense tag applied by the annotator; either a sense represented by the name of the type and a number, or *geen* for "none of the above"; -->
<!-- - **confidence**: the confidence assigned by the annotator, with minimum 0 (1 star) and maximum 5 (6 stars); -->
<!-- - **conf_std**: standardized confidence values, grouping by **username** and **type**[^conf_std]; -->
<!-- - **comments**: the comments given by the annnotators, which are *normally* empty, unless the sense is *geen* (some annotators also commented tokens where they assigned an actual sense tag); -->
<!-- - **geen_reason**: a classification of the comments given by the annotators, grouping them in the categories described before (*between*, *other_sense*, *unclear* and *wrong_lemma*); -->
<!-- - **sense**: the modified sense annotation, replacing the *geen* annotations of **original_sense** with the categories from **geen_reason**; -->
<!-- - **annotations**: the number of different values of **sense** for that given token (so that 1 equals to total agreement); -->
<!-- - **agree_nr**: the proportion (0-1) of annotators that assigned the **sense** of a given row to the **token_id** of that row; -->
<!-- - **agree_fct**: a categorical version of **agree_nr**, where *full* represents full agreement between annotators, *none* no agreement, *half* that two out of four agreed, *minority* that the current row has a disgreeing annotation of the token and *majority* if this annotation agrees with the majority for that token. -->
<!-- - There is no column with cues, but they are stored _somewhere_ so I can retrieve them when I want to start working on them. -->
<!-- [^conf_std]:  There is a lot of variation across annotators in how they used their confidence (even within the same set of tokens). There is also a wide variation depending on type, for each username. It also makes sense to use this criterion. -->

</div>
</div>
</div>
</div>



  <div class="chapter-nav">
<div class="prev"><a href="hdbscan.html"><span class="header-section-number">6</span> HDBSCAN</a></div>
<div class="next"><a href="the-nature-of-clouds.html"><span class="header-section-number">8</span> The nature of clouds</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#ann"><span class="header-section-number">7</span> Annotation schema</a></li>
<li>
<a class="nav-link" href="#selection"><span class="header-section-number">7.1</span> Selection of items</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#nouns"><span class="header-section-number">7.1.1</span> The nouns</a></li>
<li><a class="nav-link" href="#adjs"><span class="header-section-number">7.1.2</span> The adjectives</a></li>
<li><a class="nav-link" href="#verbs"><span class="header-section-number">7.1.3</span> The verbs</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#expectations-for-the-annotation"><span class="header-section-number">7.2</span> Expectations for the annotation</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#for-all-types"><span class="header-section-number">7.2.1</span> For all types</a></li>
<li><a class="nav-link" href="#only-for-nouns"><span class="header-section-number">7.2.2</span> Only for nouns</a></li>
</ul>
</li>
<li><a class="nav-link" href="#only-for-verbs"><span class="header-section-number">7.3</span> Only for verbs</a></li>
<li><a class="nav-link" href="#corpus"><span class="header-section-number">7.4</span> The corpus and samples</a></li>
<li>
<a class="nav-link" href="#annotation"><span class="header-section-number">7.5</span> Annotation procedure</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#assigning-batches-to-annotators"><span class="header-section-number">7.5.1</span> Assigning batches to annotators</a></li>
<li><a class="nav-link" href="#annotation-toolinterface"><span class="header-section-number">7.5.2</span> Annotation tool/interface</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/montesmariana/phdThesis/blob/master/viz_6.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/montesmariana/phdThesis/edit/master/viz_6.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Cloudspotting: Visual analytics for distributional semantics</strong>" was written by Mariana Montes. It was last built on 2021-07-07.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>
</html>
