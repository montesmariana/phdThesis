<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 3 From corpora to clouds | Cloudspotting: Visual analytics for distributional semantics</title>
<meta name="author" content="Mariana Montes">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.2"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/header-attrs-2.9/header-attrs.js"></script><script src="libs/jquery-3.5.1/jquery-3.5.1.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.5.3/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.5.3/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.2.4/tabs.js"></script><script src="libs/bs3compat-0.2.4/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><link href="libs/tabwid-1.0.0/tabwid.css" rel="stylesheet">
<script src="libs/kePrint-0.0.1/kePrint.js"></script><link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<script src="https://cdn.jsdelivr.net/autocomplete.js/0/autocomplete.jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/mark.js@8.11.1/dist/mark.min.js"></script><!-- CSS --><link rel="stylesheet" href="assets/style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Cloudspotting: Visual analytics for distributional semantics</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Abstract</a></li>
<li><a class="" href="acknowledgments.html">Acknowledgments</a></li>
<li><a class="" href="introduction.html"><span class="header-section-number">1</span> Introduction</a></li>
<li class="book-part">Visualization tool</li>
<li><a class="" href="an-interface-to-the-world-of-clouds.html"><span class="header-section-number">2</span> An interface to the world of clouds</a></li>
<li><a class="active" href="workflow.html"><span class="header-section-number">3</span> From corpora to clouds</a></li>
<li><a class="" href="params.html"><span class="header-section-number">4</span> Parameter settings</a></li>
<li><a class="" href="nephovis.html"><span class="header-section-number">5</span> NephoVis</a></li>
<li><a class="" href="hdbscan.html"><span class="header-section-number">6</span> HDBSCAN</a></li>
<li><a class="" href="ann.html"><span class="header-section-number">7</span> Annotation schema</a></li>
<li class="book-part">The language of clouds</li>
<li><a class="" href="the-nature-of-clouds.html"><span class="header-section-number">8</span> The nature of clouds</a></li>
<li><a class="" href="nonsense-or-no-senses.html"><span class="header-section-number">9</span> Nonsense or no senses?</a></li>
<li><a class="" href="no-sky-fits-all.html"><span class="header-section-number">10</span> No sky fits all</a></li>
<li class="book-part">Conclusion</li>
<li><a class="" href="cloudspotters-guide.html"><span class="header-section-number">11</span> Cloudspotter’s guide</a></li>
<li><a class="" href="avenues-for-further-research.html"><span class="header-section-number">12</span> Avenues for further research</a></li>
<li><a class="" href="conclusion.html"><span class="header-section-number">13</span> Conclusion</a></li>
<li><a class="" href="references.html">References</a></li>
<li class="book-part">Appendix</li>
<li><a class="" href="definitions.html"><span class="header-section-number">A</span> Definitions</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/montesmariana/phdThesis">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="workflow" class="section level1" number="3">
<h1>
<span class="header-section-number">3</span> From corpora to clouds<a class="anchor" aria-label="anchor" href="#workflow"><i class="fas fa-link"></i></a>
</h1>
<p>The main goal of the distributional models discussed in this text is to explore semasiological structure from textual data.
The starting point is a corpus, and one of the most tangible outputs is the visual representation as a cloud.
In this chapter we will describe how to generate clouds from the raw, seemingly indomitable ocean of corpora.</p>
<p>First, we will describe how token-level vector space models are created.
Section <a href="workflow.html#vector-creation">3.1</a> will explain count-based models, but this is by no means the only viable path.
Other techniques, such as BERT <span class="citation">(<a href="references.html#ref-BERT" role="doc-biblioref">Devlin et al. 2019</a>)</span><a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;See also &lt;span class="citation"&gt;&lt;a href="references.html#ref-devries.etal_2019" role="doc-biblioref"&gt;Vries et al.&lt;/a&gt; (&lt;a href="references.html#ref-devries.etal_2019" role="doc-biblioref"&gt;2019&lt;/a&gt;)&lt;/span&gt; for a Dutch version.&lt;/p&gt;'><sup>3</sup></a>, that can generate vectors for individual instances of a word,
can be used for the first stage of this workflow.
<!-- TODO explain why we don't use them --></p>
<p>Once we have vector representations of individual instances of a lexical item, we need to process them.
For visualization purposes, we need to reduce the numerous dimensions of the vectors to a manageable number, such as 2.
Section <a href="workflow.html#dim-reduction">3.2</a> will explore and compare a few alternatives.</p>
<p>The same output that is put through dimensionality reduction for the visualization can also
be submitted to other forms of analysis such as clustering algorithms, whose results may
even be combined with the visualization. We will look at HDBSCAN in particular in Chapter <a href="hdbscan.html#hdbscan">6</a>.</p>
<div id="vector-creation" class="section level2" number="3.1">
<h2>
<span class="header-section-number">3.1</span> A cloud machine<a class="anchor" aria-label="anchor" href="#vector-creation"><i class="fas fa-link"></i></a>
</h2>
<p>At the core of vector space models, <em>aka</em> distributional models, we find the Distributional Hypothesis, which is most often linked to Harris’s observation that “difference of meaning correlates with difference of distribution” <span class="citation">(<a href="references.html#ref-harris_1954" role="doc-biblioref">1954: 156</a>)</span>, but also to <span class="citation"><a href="references.html#ref-firth_1957a" role="doc-biblioref">Firth</a> (<a href="references.html#ref-firth_1957a" role="doc-biblioref">1957</a>)</span> and Wittgenstein.
<!-- TODO read and add Wittgenstein -->
In other words, items that occur in similar contexts in a given corpus will be semantically similar, while those that occur in different contexts will be semantically different <span class="citation">(<a href="references.html#ref-jurafsky.martin_2020" role="doc-biblioref">Jurafsky &amp; Martin 2020</a>, Ch. 6; <a href="references.html#ref-lenci_2018" role="doc-biblioref">Lenci 2018</a>)</span>. Crucially, this does not imply that we can describe an individual item with their distributional properties, but that comparing the distribution of two items can tell us something about their semantic relatedness <span class="citation">(<a href="references.html#ref-sahlgren_2006" role="doc-biblioref">Sahlgren 2006: 19</a>)</span>.</p>
<p><span class="citation"><a href="references.html#ref-firth_1957a" role="doc-biblioref">Firth</a> (<a href="references.html#ref-firth_1957a" role="doc-biblioref">1957</a>)</span> inspired a whole tradition of corpus linguistics which started to look at collocations as part of the semantic description of a lemma. The Cobuild dictionary was the first to integrate collocational information into their entries. The Birmingham school, pioneered by John Sinclair, used co-occurrence frequency information to describe a lexical item (most often word forms, in English; see <span class="citation"><a href="references.html#ref-sinclair_1991" role="doc-biblioref">Sinclair</a> (<a href="references.html#ref-sinclair_1991" role="doc-biblioref">1991: 29</a>)</span> and; <span class="citation"><a href="references.html#ref-stubbs_1995" role="doc-biblioref">Stubbs</a> (<a href="references.html#ref-stubbs_1995" role="doc-biblioref">1995: 23–24</a>)</span>) by the set of those context words most attracted to them. Researchers would normally transform the raw frequencies with association measures such as mutual information
<span class="citation">(<a href="references.html#ref-church.hanks_1989" role="doc-biblioref">Church &amp; Hanks 1989</a>; <a href="references.html#ref-stubbs_1995" role="doc-biblioref">Stubbs 1995: 33</a>; <a href="references.html#ref-mcenery.etal_2010" role="doc-biblioref">McEnery, Xiao &amp; Tono 2010</a>; <a href="references.html#ref-gablasova.etal_2017" role="doc-biblioref">Gablasova, Brezina &amp; McEnery 2017</a>)</span>
or t-score
<!-- TODO add proper sources, probably sinclair, stubbs, others... @gablasova.etal_2017-->
among others (see <span class="citation"><a href="references.html#ref-gablasova.etal_2017" role="doc-biblioref">Gablasova, Brezina &amp; McEnery</a> (<a href="references.html#ref-gablasova.etal_2017" role="doc-biblioref">2017</a>)</span> for an overview),
set a threshold (around 3 for mutual information)
<!-- TODO check references: maybe sinclair and stubbs, also mcenery, gablasova... -->
and rank the context words that survive such threshold.</p>
<p>Count-based vectors basically compare two items by contrasting their list of collocates, but instead of implementing a binary distinction based on an arbitrary threshold,
<!-- TODO add citation -->
the magnitude of the association strengths will play a role. Rather than measuring the overlap between the traditional collocational profiles, it will depend on what the actual values are.</p>
<p>Distributional models operationalize<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;See &lt;span class="citation"&gt;&lt;a href="references.html#ref-stefanowitsch_2010" role="doc-biblioref"&gt;Stefanowitsch&lt;/a&gt; (&lt;a href="references.html#ref-stefanowitsch_2010" role="doc-biblioref"&gt;2010&lt;/a&gt;)&lt;/span&gt; for a discussion on definitions and operationalizations of &lt;em&gt;meaning&lt;/em&gt; and their relationship to an overarching theoretical notion of &lt;em&gt;meaning&lt;/em&gt;.
<!-- TODO add reference to the appropriate theoretical section -->&lt;/p&gt;'><sup>4</sup></a> this idea by representing words as vectors (i.e. arrays of numbers) coding frequency information. Typically, the raw frequency is transformed to some association strength measure, such as pointwise mutual information <span class="citation">(PMI, see <a href="references.html#ref-church.hanks_1989" role="doc-biblioref">Church &amp; Hanks 1989</a>)</span>, which compares the frequency with which two words occur close to each other and the expected frequency if the words were independent. For example, Table <a href="workflow.html#tab:vec1">3.1</a> shows small vectors representing the English nouns <em>linguistic</em>, <em>lexicography</em>, <em>research</em> and <em>chocolate</em>, as well as the adjective <em>computational</em>, as series of association strengths with a set of lemmas. Empty cells indicate that the word in the row and the word in the column never co-occur in the corpus (given a certain window span).</p>
<template id="88503fac-b5b0-4441-a5fd-066a02045044"><style>
.tabwid table{
  border-collapse:collapse;
  line-height:1;
  margin-left:auto;
  margin-right:auto;
  border-width: 0;
  display: table;
  margin-top: 1.275em;
  margin-bottom: 1.275em;
  border-spacing: 0;
  border-color: transparent;
}
.tabwid_left table{
  margin-left:0;
}
.tabwid_right table{
  margin-right:0;
}
.tabwid td {
    padding: 0;
}
.tabwid a {
  text-decoration: none;
}
.tabwid thead {
    background-color: transparent;
}
.tabwid tfoot {
    background-color: transparent;
}
.tabwid table tr {
background-color: transparent;
}
</style>
<div class="tabwid">
<style>.cl-43cb8ae4{border-collapse:collapse;}.cl-43b52c40{font-family:'Arial';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-43b55332{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-43b5ef0e{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-43b5ef0f{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-43b5ef10{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-43b5ef11{width:54pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}</style>
<div class="inline-table"><table class="cl-43cb8ae4">
<caption class>
<span id="tab:vec1">Table 3.1: </span>Example of type-level vectors.
</caption>
<thead><tr style="overflow-wrap:break-word;">
<td class="cl-43b5ef11"><p class="cl-43b55332"><span class="cl-43b52c40">target</span></p></td>
<td class="cl-43b5ef11"><p class="cl-43b55332"><span class="cl-43b52c40">language/n</span></p></td>
<td class="cl-43b5ef11"><p class="cl-43b55332"><span class="cl-43b52c40">word/n</span></p></td>
<td class="cl-43b5ef11"><p class="cl-43b55332"><span class="cl-43b52c40">flemish/j</span></p></td>
<td class="cl-43b5ef11"><p class="cl-43b55332"><span class="cl-43b52c40">english/j</span></p></td>
<td class="cl-43b5ef11"><p class="cl-43b55332"><span class="cl-43b52c40">eat/v</span></p></td>
<td class="cl-43b5ef11"><p class="cl-43b55332"><span class="cl-43b52c40">speak/v</span></p></td>
</tr></thead>
<tbody>
<tr style="overflow-wrap:break-word;">
<td class="cl-43b5ef0e"><p class="cl-43b55332"><span class="cl-43b52c40">linguistics/n</span></p></td>
<td class="cl-43b5ef0e"><p class="cl-43b55332"><span class="cl-43b52c40">4.37</span></p></td>
<td class="cl-43b5ef0e"><p class="cl-43b55332"><span class="cl-43b52c40">0.99</span></p></td>
<td class="cl-43b5ef0e"><p class="cl-43b55332"><span class="cl-43b52c40">-</span></p></td>
<td class="cl-43b5ef0e"><p class="cl-43b55332"><span class="cl-43b52c40">3.16</span></p></td>
<td class="cl-43b5ef0e"><p class="cl-43b55332"><span class="cl-43b52c40">-</span></p></td>
<td class="cl-43b5ef0e"><p class="cl-43b55332"><span class="cl-43b52c40">0.41</span></p></td>
</tr>
<tr style="overflow-wrap:break-word;">
<td class="cl-43b5ef0e"><p class="cl-43b55332"><span class="cl-43b52c40">lexicography/n</span></p></td>
<td class="cl-43b5ef0e"><p class="cl-43b55332"><span class="cl-43b52c40">3.51</span></p></td>
<td class="cl-43b5ef0e"><p class="cl-43b55332"><span class="cl-43b52c40">2.18</span></p></td>
<td class="cl-43b5ef0e"><p class="cl-43b55332"><span class="cl-43b52c40">-</span></p></td>
<td class="cl-43b5ef0e"><p class="cl-43b55332"><span class="cl-43b52c40">2.19</span></p></td>
<td class="cl-43b5ef0e"><p class="cl-43b55332"><span class="cl-43b52c40">-</span></p></td>
<td class="cl-43b5ef0e"><p class="cl-43b55332"><span class="cl-43b52c40">2.09</span></p></td>
</tr>
<tr style="overflow-wrap:break-word;">
<td class="cl-43b5ef0e"><p class="cl-43b55332"><span class="cl-43b52c40">computational/j</span></p></td>
<td class="cl-43b5ef0e"><p class="cl-43b55332"><span class="cl-43b52c40">1.6</span></p></td>
<td class="cl-43b5ef0e"><p class="cl-43b55332"><span class="cl-43b52c40">0.08</span></p></td>
<td class="cl-43b5ef0e"><p class="cl-43b55332"><span class="cl-43b52c40">-</span></p></td>
<td class="cl-43b5ef0e"><p class="cl-43b55332"><span class="cl-43b52c40">-1</span></p></td>
<td class="cl-43b5ef0e"><p class="cl-43b55332"><span class="cl-43b52c40">-</span></p></td>
<td class="cl-43b5ef0e"><p class="cl-43b55332"><span class="cl-43b52c40">-1.8</span></p></td>
</tr>
<tr style="overflow-wrap:break-word;">
<td class="cl-43b5ef0e"><p class="cl-43b55332"><span class="cl-43b52c40">research/n</span></p></td>
<td class="cl-43b5ef0e"><p class="cl-43b55332"><span class="cl-43b52c40">0.2</span></p></td>
<td class="cl-43b5ef0e"><p class="cl-43b55332"><span class="cl-43b52c40">-0.84</span></p></td>
<td class="cl-43b5ef0e"><p class="cl-43b55332"><span class="cl-43b52c40">0.04</span></p></td>
<td class="cl-43b5ef0e"><p class="cl-43b55332"><span class="cl-43b52c40">-0.5</span></p></td>
<td class="cl-43b5ef0e"><p class="cl-43b55332"><span class="cl-43b52c40">-0.68</span></p></td>
<td class="cl-43b5ef0e"><p class="cl-43b55332"><span class="cl-43b52c40">-0.38</span></p></td>
</tr>
<tr style="overflow-wrap:break-word;">
<td class="cl-43b5ef0f"><p class="cl-43b55332"><span class="cl-43b52c40">chocolate/n</span></p></td>
<td class="cl-43b5ef0f"><p class="cl-43b55332"><span class="cl-43b52c40">-1.72</span></p></td>
<td class="cl-43b5ef0f"><p class="cl-43b55332"><span class="cl-43b52c40">-0.53</span></p></td>
<td class="cl-43b5ef0f"><p class="cl-43b55332"><span class="cl-43b52c40">1.28</span></p></td>
<td class="cl-43b5ef0f"><p class="cl-43b55332"><span class="cl-43b52c40">-0.73</span></p></td>
<td class="cl-43b5ef0f"><p class="cl-43b55332"><span class="cl-43b52c40">3.08</span></p></td>
<td class="cl-43b5ef0f"><p class="cl-43b55332"><span class="cl-43b52c40">-1.13</span></p></td>
</tr>
</tbody>
<tfoot><tr style="overflow-wrap:break-word;"><td colspan="7" class="cl-43b5ef10"><p class="cl-43b55332"><span class="cl-43b52c40">PMI values based on symmetric window of 10; frequency data from GloWbE.</span></p></td></tr></tfoot>
</table></div>
</div></template><div class="flextable-shadow-host" id="e71d5ea0-6bac-4c61-8ac7-a640622da58f"></div>
<script>
var dest = document.getElementById("e71d5ea0-6bac-4c61-8ac7-a640622da58f");
var template = document.getElementById("88503fac-b5b0-4441-a5fd-066a02045044");
var caption = template.content.querySelector("caption");
if(caption) {
  caption.style.cssText = "display:block;text-align:center;";
  var newcapt = document.createElement("p");
  newcapt.appendChild(caption)
  dest.parentNode.insertBefore(newcapt, dest.previousSibling);
}
var fantome = dest.attachShadow({mode: 'open'});
var templateContent = template.content;
fantome.appendChild(templateContent);
</script><p>Each row is a vector coding the distributional information of the lemma it represents. By <em>lemma</em> we refer to the combination of a stem and a part of speech, e.g. <em>chocolate/n</em> covers <em>chocolate</em>, <em>chocolates</em>, <em>Chocolate</em>… How we define the unit is a decision we must make when we build a model (see Chapter <a href="params.html#params">4</a>); in computational linguistic research, this unit is often a word form, and the difference between using word forms or lemmas varies drastically depending on the language of the corpus. More importantly, we must remember that the vectors represent a distributional profile of the lemma, which is automatically extracted from a corpus, not of a meaning <span class="citation">(Cf. <a href="references.html#ref-bolognesi_2020" role="doc-biblioref">Bolognesi 2020: 82</a>)</span>.</p>
<p>Table <a href="workflow.html#tab:vec1">3.1</a> offers a brief example in which semantically similar lemmas (e.g. <em>linguistics</em> and <em>lexicography</em>) have similar vectors, while semantically different lemmas (e.g. <em>linguistics</em> and <em>chocolate</em>) have different vectors.</p>
<p>These are type-level vectors: each of them aggregates over all the instances of a given lemma, e.g. <em>linguistics</em>, to build an overall profile. As a result, it collapses the internal variation of the lemma, i.e. its semasiological structure. In order to uncover such information, we need to build vectors for the individual instances or tokens, relying on the same principle: items occurring in similar contexts will be semantically similar. For instance, we might want to model the three (artificial) occurrences of <em>study</em> in (1) through (3), where the target item is in italics.</p>
<ol class="example" style="list-style-type: decimal">
<li>Would you like to <em>study</em> lexicography?</li>
<li>They <em>study</em> this in computational linguistics as well.</li>
<li>I eat chocolate while I <em>study</em>.</li>
</ol>
<p>Given that, at the aggregate level, a word can co-occur with thousands of different words, type-level vectors can include thousands of values. In contrast, token-level vectors can only have as many values as the individual window size comprises, which drastically reduces the chances of overlap between vectors. In fact, the three examples don’t share any item other than the target. As a solution, inspired by <span class="citation"><a href="references.html#ref-schutze_1998" role="doc-biblioref">Schütze</a> (<a href="references.html#ref-schutze_1998" role="doc-biblioref">1998</a>)</span>, we replace the context words around the token with their respective type-level vectors <span class="citation">(<a href="references.html#ref-heylen.etal_2015" role="doc-biblioref">Heylen et al. 2015</a>; <a href="references.html#ref-depascale_2019" role="doc-biblioref">De Pascale 2019</a>)</span>.</p>
<p>For example, we could represent example (1) with the vector for its context word <em>lexicography</em>, that is, the second row in Table <a href="workflow.html#tab:vec1">3.1</a>; example (2) with the sum of the vectors for <em>linguistics</em> (row 1) and <em>computational</em> (row 3); and example (3) with the vector for <em>chocolate</em> (row 5). This not only solves the sparsity issue, ensuring overlap between the vectors, but also allows us to find similarity between (1) and (2) based on the similarity between the vectors for <em>lexicography</em> and <em>linguistics</em>.</p>
<p>From applying this method we obtain numerical representations of occurrences of a word. We can compare them to each other by calculating pairwise distances, which is at the base of clustering analyses (such as HDBSCAN, see Chapter <a href="hdbscan.html#hdbscan">6</a>) and visualization techniques based on dimensionality reduction (section <a href="workflow.html#dim-reduction">3.2</a>).</p>
<p>However, in order to obtain this result we need to make a number of decisions related, among other things, with how we define <em>word</em> and <em>context</em> <span class="citation">(Cf. <a href="references.html#ref-bolognesi_2020" role="doc-biblioref">Bolognesi 2020: 83</a>)</span>. These decisions will be discussed in Chapter <a href="params.html#params">4</a>.</p>
</div>
<div id="dim-reduction" class="section level2" number="3.2">
<h2>
<span class="header-section-number">3.2</span> Dimensionality reduction<a class="anchor" aria-label="anchor" href="#dim-reduction"><i class="fas fa-link"></i></a>
</h2>
<p>Dimensionality reduction algorithms try to reduce the number of dimensions of a high-dimensional entity while retaining as much information as possible. In Latent Semantic Analysys (LSA)
<!-- NOTE add authors, Sahlgren, Van De Cruys... -->
it is used to reduce type-level spaces from thousands of dimensions to a few hundred, and the resulting reduced dimensions have been found to correspond to semantic fields.
<span class="citation"><a href="references.html#ref-vandecruys_2008" role="doc-biblioref">Van De Cruys</a> (<a href="references.html#ref-vandecruys_2008" role="doc-biblioref">2008</a>)</span> tried both SVD and non-negative matrix factorization on type-level BOW-based models with whole paragraphs as windows but they did not bring any improvement.</p>
<p>It could also be used for token-level spaces, but the comparisons discussed in <span class="citation"><a href="references.html#ref-depascale_2019" role="doc-biblioref">De Pascale</a> (<a href="references.html#ref-depascale_2019" role="doc-biblioref">2019: 246</a>)</span> indicate that they don’t necessarily perform better than non reduced spaces. Besides, as we will see in Chapter <a href="params.html#params">4</a>, we can also generate spaces that are already in the few hundred dimensions by selecting the first order context words as second order features, which does not underperform in our studies. Skipping the SVD step also implies more transparent vectors, facilitating the understanding of what is going on under the hood, but, as many other paths we have not taken, the application of SVD remains a parameter to explore in the future.
<!-- NOTE Dimensionality reduction can also refer to SVD and stuff like that, so we should mention it and dismiss, cite Stefano [246]
Also check Van De Cruys' work, some LSA? Remember: Kiela+Clark do not look into this--></p>
<p>Both dimensionality reduction techniques and neural networks are suggested as ways of condensing very long, sparse vectors <span class="citation">(<a href="references.html#ref-jurafsky.martin_2020" role="doc-biblioref">Jurafsky &amp; Martin 2020</a>; <a href="references.html#ref-bolognesi_2020" role="doc-biblioref">Bolognesi 2020</a>)</span>,
but if the FOC-based selection of second order dimensions suffices, it is not really needed. Indeed, we might want to compare SVD versions and embeddings with these vectors, but for the purposes of understanding what is going on and obtaining meaningful information for corpora, this is still cool!</p>
<p>The algorithms we will discuss in this section, instead, try to locate different items on a low-dimensional space (e.g. 2D) preserving their distances in the high-dimensional space (e.g. 5000D) as well as possible.
The literature up to today tends to go for either multidimensional scaling (MDS) or t-stochastic neighbor embeddings (t-SNE);
recently, an interesting alternative called UMAP has been introduced, which we’ll discuss shortly.</p>
<p>MDS is an ordination technique, like principal components analysis (PCA). It tries out different low-dimensional configurations aiming to maximize the correlation between the pairwise distances in the high-dimensional space and those in the low-dimensional space: items that are close together in one space should stay close together in the other, and items that are far apart in one space should stay far apart in the other.
It can be evaluated via the stress level, the complement of the correlation coefficient: if the correlation between the pairwise distances is 0.85, the stress level is 0.15.
Unlike PCA, however, the dimensions are not meaningful <em>per se</em>; two different runs of MDS may result in plots that mirror each other while representing the same thing. Nonetheless, the R implementation (in particular, <code>metaMDS()</code> of the {vegan} package, see <span class="citation"><a href="references.html#ref-R-vegan" role="doc-biblioref">Oksanen et al.</a> (<a href="references.html#ref-R-vegan" role="doc-biblioref">2020</a>)</span>) rotates the plot so that the horizontal axis represents the maximum variation.
In cognitive linguistics literature both metric <span class="citation">(<a href="references.html#ref-hilpert.correiasaavedra_2017" role="doc-biblioref">Hilpert &amp; Correia Saavedra 2017</a>; <a href="references.html#ref-hilpert.flach_2020" role="doc-biblioref">Hilpert &amp; Flach 2020</a>; <a href="references.html#ref-koptjevskaja-tamm.sahlgren_2014" role="doc-biblioref">Koptjevskaja-Tamm &amp; Sahlgren 2014</a>)</span>
and nonmetric MDS <span class="citation">(<a href="references.html#ref-heylen.etal_2015" role="doc-biblioref">Heylen et al. 2015</a>; <a href="references.html#ref-heylen.etal_2012" role="doc-biblioref">Heylen, Speelman &amp; Geeraerts 2012</a>; <a href="references.html#ref-depascale_2019" role="doc-biblioref">De Pascale 2019</a>; <a href="references.html#ref-perek_2016" role="doc-biblioref">Perek 2016</a>)</span> have been used.</p>
<p>The second technique, t-SNE <span class="citation">(<a href="references.html#ref-Rtsne2008" role="doc-biblioref">van der Maaten &amp; Hinton 2008</a>; <a href="references.html#ref-Rtsne2014" role="doc-biblioref">van der Maaten 2014</a>)</span>, has also been incorporated in cognitive distributional semantics <span class="citation">(<a href="references.html#ref-depascale_2019" role="doc-biblioref">De Pascale 2019</a>; <a href="references.html#ref-perek_2018" role="doc-biblioref">Perek 2018</a>)</span>.
It is also popular in computational linguistics <span class="citation">(<a href="references.html#ref-smilkov.etal_2016" role="doc-biblioref">Smilkov et al. 2016</a>; <a href="references.html#ref-jurafsky.martin_2020" role="doc-biblioref">Jurafsky &amp; Martin 2020: 118</a>)</span>; in R, it can be implemented with the <code>Rtsne</code> function of the homonymous package <span class="citation">(<a href="references.html#ref-R-Rtsne" role="doc-biblioref">Krijthe 2018</a>)</span>.
The algorithm is quite different from MDS. For our purposes the crucial point is that it prioritizes preserving local similarity structure instead of the global structure: items that are close together in the high-dimensional space should stay close together in the low-dimensional space, but those that are far apart in the high-dimensional space may be even farther apart in low-dimensional space.</p>
<p>This leads to nice, tight clusters but the distance between them is less interpretable than in an MDS plot (where, in any case, tight clusters are a rare occurrence in these studies). In other words, we can interpret tight groups of tokens as being similar to each other, but we cannot extract meaningful information from the distance between these groups. In addition, it would seem that points that are far away in a multidimensional space might show up close together in the low dimensional space <span class="citation">(<a href="references.html#ref-oskolkov_2021" role="doc-biblioref">Oskolkov 2021</a>)</span>. Uniform Manifold Approximation and Projection <span class="citation">(<a href="references.html#ref-mcinnes.etal_2020" role="doc-biblioref">McInnes, Healy &amp; Melville 2020</a>)</span>, instead, penalizes this sort of discrepancies. It would be an interesting avenue for further research, but a brief test on the current data did not reveal such great differences between t-SNE and UMAP to warrant the replacement of the technique within the duration of this project. Other apparent advantages such as speed were not observed in the small samples under consideration (in fact, UMAP —or at least its R implementation with the {umap} package <span class="citation">(<a href="references.html#ref-R-umap" role="doc-biblioref">Konopka 2020</a>)</span>— was even slower).</p>
<p>In both cases we need to state the desired number of dimensions before running the algorithm —for visualization purposes, the most useful choice is 2. Three dimensions are difficult to interpret if projected on a 2D space, such as a screen <span class="citation">(<a href="references.html#ref-card.etal_1999" role="doc-biblioref">Card, Mackinlay &amp; Shneiderman 1999: 18</a>; <a href="references.html#ref-wielfaert.etal_2019" role="doc-biblioref">Wielfaert et al. 2019: 222</a>)</span>. In addition, t-SNE requires setting a parameter called perplexity, which basically sets how many neighbors the preserved local structure should cover.</p>

</div>
</div>

  <div class="chapter-nav">
<div class="prev"><a href="an-interface-to-the-world-of-clouds.html"><span class="header-section-number">2</span> An interface to the world of clouds</a></div>
<div class="next"><a href="params.html"><span class="header-section-number">4</span> Parameter settings</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#workflow"><span class="header-section-number">3</span> From corpora to clouds</a></li>
<li><a class="nav-link" href="#vector-creation"><span class="header-section-number">3.1</span> A cloud machine</a></li>
<li><a class="nav-link" href="#dim-reduction"><span class="header-section-number">3.2</span> Dimensionality reduction</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/montesmariana/phdThesis/blob/master/viz_3.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/montesmariana/phdThesis/edit/master/viz_3.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Cloudspotting: Visual analytics for distributional semantics</strong>" was written by Mariana Montes. It was last built on 2021-07-08.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>
</html>
