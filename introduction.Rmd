# Introduction

If meaning is found and created in use and corpora are language in use: can we find meaning in corpora? The field of usage-based semantics is large and rich, so the answer to this question is clearly positive. But there is a crucial bottleneck. Corpora offer an immense amount of usage data on which to carry analyses, even if it barely scratches the surface of the amount of language that is actually produced --- it is desirable and tempting to tap into this vast ocean to obtain the most detailed, the most reliable, the most thorough information. However, semantic annotation is time and energy consuming. As long as we cannot instruct an automatic system to disambiguate each word in a corpus --- like we do to tokenize and lemmatize, i.e. to identify what counts as a word and which is its root, or even to assign parts of speech or syntactic relations --- semantic annotation is performed by humans. Humans are slower than computers; we get tired, we get confused, we need to eat and think of things beyond semantic annotation too. We also disagree --- what is a sense? Are these two things *really* the same?

Automatic disambiguation systems do exist. Word Sense Disambiguation is an important task within Natural Language Processing (`r sc("nlp")`). The notion of *task* is of crucial importance here: `r sc("nlp")` algorithms are typically concerned with concrete applications and evaluated in terms of these applications. There exists a correct answer that the algorithm must return. This is not so directly applicable to the situation of lexicological and lexicographical research --- the study of the meanings of words and their relationships ---, especially from a Cognitive Linguistics point of view, where hard, dichotomous answers are rare. But let's suppose for a moment that we can conciliate both approaches, and what counts as *the* answer from an `r sc("nlp")` point of view is *an* answer from the lexicological perspective. Then we could use automatic disambiguation procedures to make the heavy lifting of semantic annotation of our growing body of corpus data and use their results for a partial description of language. As long as we know *which* answer the `r sc("nlp")` algorithm is resulting or, better yet, how to ask what we want to know. Maybe tuning the algorithm for outputs that from an `r sc("nlp")` point of view would be *wrong* can return as complementary answers for a richer lexicological description. Such a qualitative perspective, trying to interpret not just *whether* the computational model matches a target but also *how* or *why* it does (not), also requires appropriate analytical tools. One such tool represents the internal semantic structure of an item, derived from computational models, as a `r sc("2d")` scatterplot where instances occurring in similar context are shown together, forming clusters or *clouds*.

This dissertation concerns itself with the application of distributional methods to lexicological research and their exploration by means of visual analytics. The methodology will be tested and illustrated in a set of 32 Dutch lemmas, of which concordance lines will be extracted from a corpus of newspapers.
Distributional models, developed within the field of Computational Linguistics, will be introduced in Section \@ref(comp). In Section \@ref(cog) we will discuss their relevance in Cognitive Semantics and Section \@ref(viz) will offer an overview of the visual analytics dimension.
The study described here was conducted within a larger research project within the Quantitative Lexicology and Variational Linguistics research group (`r sc("qlvl")`) at KU Leuven. A brief history of the project and how this dissertation fits in it will be offered in Section \@ref(nephosem). Finally, Section \@ref(str) will present the structure of the dissertation.

## Distributional semantics and Computational Linguistics {#comp}

Distributional semantics is a usage-based model of meaning that underlies various computational methods for semantic representation
[@sahlgren_2008; @lenci_2018]: it is an educational program for computers that lets them pretend they understand human languages. It relies on what is called the Distributional Hypothesis, according to which lexemes with similar meanings will have similar distributions, i.e. will occur in similar contexts. The core idea is typically attributed to @harris_1954 or @firth_1957a, but how enthusiastic they would be at the sight of the current implementations is disputed: @tognini-bonelli_2001 [157] remarks that Firth would not be in favour of electronic corpora and @geeraerts_2017 offers a comprehensive comparison between Harris' position and current distributional semantics. The attribution dilemma notwithstanding, the idea that meaning can be modelled by means of distributional information is pervasive in `r sc("nlp")` and at the core of every form of Distributional Semantics. A more important question is what we mean by *meaning* or *semantics* to begin with [@sahlgren_2006; @lenci_2008], which in this research is informed by the Cognitive Linguistics framework. Beyond the particular attention to the semantic side of distributional semantics, this dissertation sets itself apart from mainstream computational approaches in three core aspects: motivation, definition of units and reliance on count-based models.

### Motivation

Computational Linguistics is typically task-oriented: it aims to solve problems such as information retrieval, question answering, sentiment analysis, machine translation, etc. For that purpose, benchmarks or gold standards are developed against which the models are tested. For example, @baroni.etal_2014 test different kinds of models against datasets tailored to evaluate semantic relatedness, synonym detection, concept categorization, selectional preferences and analogy; see @agirre.edmonds_2007a and @raganato.etal_2017 for evaluation systems for sense disambiguation. This is understandable and appropriate in a task-oriented workflow: when it comes to output, it does not really matter *how* the model reached the answer, as long as it is the answer that we seek. In contrast, investigating the structure of semantic representations, i.e. the *how* of this process, calls for a different approach [See for example @baroni.lenci_2011; @wielfaert.etal_2019]. On the one hand, we do not assume that there is one correct answer because we do not assume that there is only one question. Beyond "Are these two words similar?", we are interested in: "Are they synonyms?", "Are they co-hyponyms?", "Are they regionally specific expressions of the same concept?", and so forth. Different models may focus on different dimensions of semantic structure and thus answer different questions. For that reason, the dataset collected for this research covers a wide range of semantic phenomena, in the hope of tuning distributional models to their identification. On the other hand, we are not confident that any of those questions has a unequivocal answer either. As Chapter \@ref(dataset) will show, annotators often agree on the sense of an expression, but not always. This annotation will therefore serve as a guideline for the interpretation of the models, but not as a law to judge their accuracy.

### Units of analysis

Computational models typically work at type-level with word forms, whereas this dissertation focuses on token-level models with lemmas as units.
Type-level modelling represents a lexical unit, such as *word*, as the aggregated distributional behaviour of all its occurrences, e.g. "*word* tends to be preceded by *the*". Patterns can be found by accumulating and classifying contextual information from thousands if not millions of events. The profile of a type can subsequently be compared to the profiles of other types, e.g. "*sentence* also tends to be preceded by *the*, while *walking* does not". Such a representation conflates the variation within the range of application of that item as part of one overall tendency, and is therefore not suited to study polysemy. Even if the context does contain disambiguating cues, such as "Can we have a *word*?" compared to "That *word* is not in the dictionary", the type-level representation will cover both. Some computational approaches to modelling polysemy try to find the patterns in the type-level representations, e.g. @koptjevskaja-tamm.sahlgren_2014. The work presented here, instead, relies on token-level modelling, which represents individual instances, e.g. comparing the two occurrences of *word* in the examples above. This approach does originate in computational linguistics [@schutze_1998] but is far less popular than type-level approaches, which are considered the default in most introductory descriptions of distributional models [@lenci_2018; @turney.pantel_2010; @bolognesi_2020].

Next to the distinction between modelling types or tokens, a crucial difference between this approach and most of computational linguistics is the definition of a lemma as the unit of analysis instead of the word form. Relying on word forms avoids layers of preprocessing that already incorporate a certain interpretation in terms of what counts as a word, which different forms go together and how they are classified grammatically. @sinclair_1991 also argues along these lines for the usage of word forms as lexical units in corpus linguistics. And, admittedly, different word forms of a given lemma might exhibit diverging distributional and semantic profiles. However, understanding lemmas as a combination of stems and grammatical category, the (mis)match between word forms and lemmas --- and therefore between either of them and meanings --- depends on the language we are describing and the words themselves. From a lexicological and lexicographical perspective, centring the lemma is common practice and what will be followed in this dissertation. This is not to say that the workflow depends on this decision, in the same way that it does not depend on Dutch being the language of the corpus. The methodology presented in these pages could be applied with word forms at the centre, but the degree to which the conclusions reached here would be applicable is an empirical question.


### Context-counting and context-predicting

Currently, the most popular approach for distributional semantics relies on neural networks, which will not be explored in this dissertation. The methodology favoured in this project relies instead on count-based or context-counting models: the values of the vectors, i.e. numerical representations of lexical units, are (relatively) directly derived from frequency counts. In contrast, the approach initiated by @mikolov.etal_2013 and which has taken over `r sc("nlp")`, i.e. word embeddings, is a context-predicting architecture. Neural networks are trained to predict empty slots in a fragment of text: given a fixed window with a target item in the middle, `r sc("cbow")` models are given the surrounding context in order to predict the target item, whereas skip-gram models try to predict the context based on the item in the middle. The training consists on a long sequence of trial and error: there is a right answer, i.e. the actual corpus, the algorithm starts by guessing and receives feedback, and iteratively it adapts its guessing strategy to minimise the error. The strategy consists of weights in the hidden layer of neural network; these weights are then used to represent the target item. In other words, while context-counting models define the distributional profile of a word along the lines of "it tends to co-occur with *chocolate* and *cookies* but not with *mycorrhyza* or *algorithm*", context-predicting models say, more or less, "this is how I feel/what my brain does when I see that word". The latter is, in a sense, more in line with the core of meaning as an introspective experience that defies definitions and restrictions, although computational models are far from actually *understanding* language. Exploring to what degree these models approximate humans' assessments lies in the purview of other research programmes involving psycholinguistic experiments. Studies have been carried out to compare the performance of context-counting and context-predicting models --- in terms, of course, of their accuracy with regards to popular benchmarks. @baroni.etal_2014 found that word2vec outperformed context-counting models, much to their disappointment. In contrast, @levy.etal_2015 fine-tuned context-counting models based on the hyperparameters from word embedding and found that performance differences where local or even insignificant.

When our purpose is to understand what of meaning, if anything, can be found in text data, the interpretation of context-counting models is much more transparent. We can trace the composition of the vectors to concrete frequencies and instances. As we will see in the second part of the dissertation, these supposedly more transparent models are already quite opaque, especially with the added transformation from type-level to token-level models. That said, most of the workflow described here can also be combined with context-predicting models.

The almost two decades since @mikolov.etal_2013 have seen a rapid and enthusiastic growth in the field of word embeddings and `r sc("nlp")`, with new models continually surpassing the previous ones. One of these is `r sc("bert")` [@BERT], which, in spite of its indubitable relevance to the approach proposed here, will not be explored. Bidirectional Encoder Representations from Transformers (`r sc("bert")`) is a machine-learning technique that can represent individual instances and sentences: unlike other context-predicting models, it can be used for token-level representations. But like other context-predicting models, its output is somewhat less interpretable than context-counting models. It has been tested on the typical task-based benchmarks and it is so time- and resources-consuming that `r sc("nlp")` researchers will typically use pre-trained embeddings and fine-tune them for specific tasks rather than generate them from scratch. In principle, combining a model of the `r sc("bert")` family with the workflow described here is not impossible: as long as occurrences are represented with vectors from which we can derive pairwise distances, the rest of the analysis stays the same. Except that we don't know which elements of the context informed the models' decision. And they use word forms. Tokenized in a different way. A brief test of `r sc("bert")`je [@devries.etal_2019], the Dutch counterpart of `r sc("bert")`, on a section of the dataset used for this project revealed that (i) for some lemmas `r sc("bert")`je's answer might be closer to the human perspective, (ii) for other lemmas a deeper investigation is in order and (iii) other lemmas cannot be modelled at all because of the discrepancy in the tokenization procedure^[The comparison was applied to a few lemmas, including *hoop* 'hope/heap', *dof* 'dull' and *heilzaam* 'healthy/beneficial'. In the first case, which was particularly challenging for the context-counting models, `r sc("bert")`je outperformed them; in the second, some context-counting models outperformed `r sc("bert")`je; and the third was never identified as one unit by `r sc("bert")`je's tokenizer.]. In other words, even if combining the methodologies is possible, the actual implementation requires some planning, specific decisions and tailoring the procedure to extract as much as we can from the backstage operations in context-predicting models.

## Distributional Semantics and Cognitive Semantics {#cog}

As a computational approach, distributional semantics is not intrinsically linked to any particular linguistic theory. Its usage-based essence makes it a natural fit for approaches that describe the *parole* over or next to the *langue* [in terms of @desaussure_1971], such as Cognitive Linguistics. In the introduction to *The Oxford Handbook of Cognitive Linguistics*, it is described as

> an approach to the analysis of natural language that originated in the late seventies and early eighties in the work of George Lakoff, Ron Langacker, and Len Talmy, and that focuses on language as an instrument for organizing, processing, and conveying information. [@geeraerts.cuyckens_2007a 3]

It stands in contrast to frameworks that uphold a strict separation of semantics and pragmatics, of structure and usage, of word knowledge and world knowledge [@geeraerts_2010a]. As the introduction and composition of the *Handbook* shows, as well as other compilations along these lines [Such as @rudzka-ostyn_1988; @kristiansen.etal_2006; @ibarretxe-antunano.valenzuela_2016], the diverse field of Cognitive Linguistics is guided by a number of principles derived from this central notion of language as categorization. Among these principles, three in particular constitute the theoretical cornerstones of this study: (i) an emphasis on meaning, (ii) the notion of fuzzy and prototypical categories and (iii) a usage-based approach.

### Everything is semantics

Understanding language as categorization and its function in the organization and communication of knowledge necessarily places the focus on meaning [@geeraerts.cuyckens_2007; @geeraerts_2016a]. From a Cognitive Linguistics perspective, all linguistic structures are considered inherently meaningful: not just lexical items but also syntactic patterns [@langacker_2008a; @lemmens_2015]. Moreover, meaning in Cognitive Linguistics goes beyond traditional semantics --- i.e. distinguishing linguistic from nonlinguistic features --- and includes encyclopaedic knowledge and pragmatics [@glynn_2010; @geeraerts_1997]. While it is crucially a cognitive phenomenon involving conceptualization, it takes place in the mind of physical, embodied beings who perceive, understand, and interact with their world: meaning is embodied and neither limited to nor separated from reference [@rohrer_2007].

The centrality of semantics in Cognitive Linguistics has lead to a strong body of work on meaning and on how traditional notions fit in with cognitive principles.
For example, the line of work initiated in the '80s with @lakoff.johnson_2003 and further developed along different lines by Raymond Gibbs Jr., Gerard Steen, Zoltán Kövecses, Elena Semino and many others [See for example @gibbs.steen_1999; @gibbs_2008; @semino_2008; @kovecses_2015] builds on understanding a traditional linguistic concept, i.e. metaphor, with the tools of Cognitive Linguistics. In these terms, metaphor refers to ways of thinking, understanding, conceptualizing, that manifest in linguistic behaviour but also permeate other areas of everyday life.

Along these lines, relationships between senses are understood as cognitive mechanisms that need not be restricted to linguistic behaviour nor to extralinguistic reference. Semantic categories such us metaphor, metonymy, specialization, homonymy and prototypicality are crucial tools to make sense of the variety of relationships between what we understand as senses. They are not exclusive of Cognitive Linguistics, but a framework that understands meaning as a property of any linguistic structure and as covering linguistic and extralinguistic features allows us to look for meaning in distributional models without expecting them to exhaust semantic description.

Cognitive Linguistics also incorporates the combination of a semasiological and onomasiological perspective, while previous frameworks have defined either one or the other as the only possibility [@geeraerts_2010a]. A semasiological perspective, which is predominant in the research described here, starts from a form or expression and investigates its range of meanings or applications, e.g. the study of polysemy. An onomasiological perspective, on the other hand, starts from a concept and describes the forms that are used to express it, e.g. synonymy. This dissertation takes a semasiological perspective, but token-level distributional models can be used from both perspectives, as can be shown in the work of @depascale_2019.

### Prototypicality

One of the most important notions in the Cognitive Linguistics understanding of categorization is that of prototypicality and salience [@rosch_1978]. Categories cannot always be described in terms of necessary and sufficient conditions; instead, they may be characterised by clusters of co-occurring properties that do not apply to all members to the same degree. They may even have fuzzy boundaries, an unclear range of application. As a property of categorization, this is a property of language, which Cognitive Linguistics embraces, incorporating a quantitative dimension to the study of meaning [@geeraerts_2010a]. At this point, a quantitative perspective does not immediately require statistical methods, but refers to a shift in the understanding of what counts as meaning description. The notion of prototypicality makes it interesting, if not inevitable, to look at the uneven distribution and importance of the different features or members of a category, as is done in @geeraerts.etal_1994 and @geeraerts_1997:

> the essence of prototype theory lies in the fact that it highlights the importance of flexibility (absence of clear demarcational boundaries) and salience (differences of structural weight) in the semantic structure of linguistic categories. [@geeraerts_2006e 74]

Given the set of meanings that a form can express, i.e. the intensional level, some of them are more salient than others, e.g. my current lifestyle means that the 'device to control the cursor on a screen' is a more salient meaning of *mouse* than 'small rodent'; but, crucially, this might not be the case in other contexts. Given the range of application of a form or a meaning, i.e. the extensional level, some may be more typical members than others. For instance, an apple is a typical member of the category of fruits, while a mango is less typical; the example of the apple and mango is a typical member of the category of 'examples of prototypicality', while the *mouse* example is not. These situations represent intensional and extensional nonequality, respectively: some senses or members of a category are better representatives of the category than others. Both dimensions may overlap, e.g. a typical fruit concentrates most of the features that characterise fruits, and a typical feature of fruits is present in most of the instances of fruits. These are two of the characteristics of prototypicality, and are complemented by intensional and extensional non discreteness, i.e. the lack of a single set of necessary and sufficient conditions and fuzzy boundaries of the categories. As could be expected, even prototypicality is a prototypical category, as these four features need not co-exist. The relative salience of the two senses of *mouse* does not mean that we might find an unknown entity and be in doubt whether it is a mouse; meanwhile, discussions such as whether a tomato is a fruit, whether hot dogs are sandwiches and whether water is wet might easily ensue. @geeraerts_2006e [Ch. 4] offers a typology of salience phenomena as application of prototype theory beyond the semasiological structure. For example, if from the semasiological perspective we are interested in describing how frequent (or salient) apples are as referents for the word *fruit*, from the onomasiological perspective we are interested in how frequent (or salient) the word *fruit* is when talking about an apple.

The notion of (semasiological) prototypicality will be relevant for the interpretation of the modelling in Chapter \@ref(semantic-interpretation). Until then, it also permeates the understanding of meaning that underlies this research. On the one hand, fuzzy boundaries and degrees of membership invite us to rethink the usefulness of reified senses: ambiguous examples and overlapping features are to be expected. Instead, a bottom-up procedure would rather capture configurations of features [@glynn_2014c]; assigning discrete senses to corpus data imposes a categorical structure that we know to be inappropriate. On the other hand, distributional models, as a quantitative approach that measures similarity between entities, is particularly adequate to such a non-discrete representation.

In this dissertation I will continue to talk about senses and I will extract discrete patterns from the non-discrete representations in terms of clusters, but the goal is to be able to manipulate and talk about abstract entities, without implying that they have any ontological reality beyond the explanatory purposes. When it comes to senses, they are not considered a gold standard, an unique solution to the semasiological description of a lexical item; instead, they are guides and operationalization of certain research questions. The clusters, on the other hand, will be generated by an algorithm that is forced to produce discrete groups but does assign its elements different degrees of membership (See Section \@ref(hdbscan)). Finally, the overall approach describes tendencies, preferences, probabilities: at no level are the categories and typologies offered in this dissertation discrete and uniform. I have tried, but language resists.

### A usage-based approach

Cognitive Linguistics presents itself as a usage-based approach and, as such, it is entirely compatible with a bottom-up, empirical, quantitative methodology such as distributional semantics. Quantitative cognitive semantics is now an established trend, as shown by the contributions gathered in @gries.stefanowitsch_2006, @glynn.fischer_2010 and @glynn.robinson_2014, among others. However, not all of Cognitive Linguistics --- and especially Cognitive Semantics --- relies on empirical methods: introspection was still the main source of information in much of the foundational sources [See for example the discussion illustrated in @geeraerts_1999]. In practice, both introspection and empirical methods are required in scientific research, albeit applied to different stages or aspects of the investigation [@geeraerts_2010].
Interpretation is needed in order to formulate hypotheses that will guide the data collection and analysis and to interpret the results: the data does not speak for itself. The empirical steps, in contrast, facilitate reproducibility and falsifiability: by describing the concrete corpus, the method of collection and the quantitative methods applied to it, the study can be replicated by different researchers and the results compared.
At the same time, large-scale quantitative methods such as distributional semantics delegate time consuming or computationally expensive tasks, such as reading and comparing thousands of attestations of a word, to an automatic system that can perform it faster and more systematically than humans, leaving the researcher to dedicate their energies in the tasks that humans are best at: interpretation and creativity. That is precisely the long-term goal of this research: to offer an empirical, quantitative workflow that transforms huge amounts of data, finds relevant patterns and provides them to the linguist for interpretation and the formulation of hypotheses.

It must be noted that empirical research in semantics is not limited to corpus-based approaches, but also covers experimental and referential methods. As @geeraerts_2015b [242-243] argues, each of them captures a different aspect of meaning, namely textual patterns, on-line processing or referential properties. Meaning, especially from the maximalist perspective taken in Cognitive Linguistics, is too complex to be fully described by any one of these methods in isolation [See also @arppe.etal_2010; @stefanowitsch_2010]. As such, we do not have such high expectations from distributional semantics --- part of the question is *what* do these models say.
Concretely, we do not expect distributional models to provide information on how we *think*, but on how a community speaks and categorises: "'language as cognition' encompasses shared and socially distributed knowledge and not just individual ideas and experiences" [@geeraerts_2016a 533]. It is the pool of shared practices and knowledge and corpora offer and distributional semantics tries to model.

Moreover, despite the large corpora, the advanced quantitative techniques and the sophisticated visualization tools on which this dissertation is built, this study has its limits. It is restricted to a specific corpus, and as such to a specific varieties of a specific language, to a specific genre and period, to written text; it is restricted to a limited set of lexical items that were investigated; it is restricted to the precise samples collected, the precise questions asked, the precise techniques used to answer them. Most importantly, I will be as thorough as possible in stating the conditions in which the research was carried out and the choices made along the way. As a result, these limits are not just warnings as to the range of applicability of the results and conclusions, but also and more importantly sources of possibilities, inspiration for similar studies facilitated by the empirical nature of the investigation. 

## Visual analytics {#viz}

Distributional models return mathematical representations of lexical items --- or, in the case of token-level models, their attestations. These mathematical representations are arrays of numbers that, in the best case scenario, we can interpret as co-occurrence information, as an unsorted list of collocations. We need an additional step to transform these individual representations into similarities, which operationalize the Distributional Hypothesis mentioned above. However, even then, the output is a matrix with as many rows and columns as items we are comparing; depending on the magnitude of our sample and the subtlety of its structure, scanning it visually can be taxing, if not entirely in vain. So, that is not what we do.

For word sense disambiguation, evaluation would normally involve a clustering algorithm, a benchmark and a measure of accuracy. The clustering algorithm would take the vectors or the similarity matrix and return clusters: groups of similar items that are different from each other. The measure of accuracy would report on the agreement between the clustering solution and the benchmark: the closer they are, the better the model. However, these measures say nothing about the qualitative differences between models, i.e. whether they misclassified the same items or how they differ from the benchmark. Even if we take the gold standard as an actual ground truth and the only correct solution --- which is not the case in this study --- this is not an ideal situation.

It is responding to these concerns that a visualization tool for the exploration of token-level models was envisaged [@wielfaert.etal_2019]. The tool developed by Wielfaert in the context of the Nephological Semantics projects takes the output from a dimensionality reduction algorithm, i.e. a procedure that tries to map distances based on multiple dimensions on a `r sc("2d")` or `r sc("3d")` space, and surrounds its visual representation with interactive features. These additional features, tailored for the exploration of distributional models, set the tool apart from a static scatterplot, or even from a default interactive plot.

In @card.etal_1999 [6]'s words, "'The purpose of visualization is insight, not pictures'. The main goals of this insight are *discovery*, *decision making* and *explanation*" . Indeed, the kind of qualitative exploration achieved through this tool would have been extremely hard without it, if not impossible.
In the first place, the tool sets up a workflow that goes from the exploration of the similarity *between* models and the role of parameter settings through the qualitative comparison of selections models to the detailed exploration of individual models. It is built to facilitate a fluid exploration and interconnection between levels of analysis. The tool offers simultaneous, interconnected access to the actual output of a model (as coordinates on a `r sc("2d")` plane), the variation of parameter settings, semantic annotation, metadata of the corpora and frequency data on the context words. The interaction of these different aspects of distributional models in a practical visual interface makes patterns and insights accessible that would not have been found any other way. 

Because of this, the visualization tool is a key component of this dissertation. It is in these scatterplots that we find the clouds: clusters of similar tokens that come together in denser areas of the (reduced) semantic space. In an actual case study involving the methodological workflow presented here, a lot of the technicalities go into generating the clouds, but a lot of the *analysis* involves looking at them and finding shapes: cloudspotting.

## Nephological Semantics {#nephosem}

The research presented in this dissertation is part of a larger project within the `r sc("qlvl")` research unit, the `r sc("bof")` C1-project (3H150305) "Nephological Semantics: using token clouds for meaning detection in variationist linguistics", with Dr. Prof. Dirk Geeraerts as Principal Investigator. Both the Python module for the creation of the models, written by Tao Chen, and the visualization tools for their analysis, designed by Thomas Wielfaert and me, are products of this project. Moreover, this dissertation would not be what it is without the integration of the case studies, questions and insights discussed here with other branches of the project, and without the feedback loop on ideas, tests and thoughts on the different techniques.

The main objective of the project is to develop --- and understand --- appropriate methods for the retrieval of semantic information from corpus data, addressing concerns that stem from a longer tradition of usage-based lexical research. @geeraerts.etal_1994 and @geeraerts.etal_1999 embark in comprehensive, detailed lexicological analyses of the lexical fields of clothing and football terms in Dutch. Their approach is referential: in @geeraerts.etal_1994, for instance, they collect pictures and descriptions of garments from Dutch and Flemish magazines and describe each clothing item in terms of a variety of features, such as the length of the sleeve. Based on the relationship between the (configurations of) features and the items used to name the objects, they developed a model of lexical variation that takes into account prototypicality and salience in terms of semasiological, onomasiological and contextual variation. However, the manual and detailed identification of features at a large enough scale is painstaking and time consuming, if at all feasible. In contrast, machine-readable linguistic material is available, more or less accessible and, given the right resources, processable. It will not provide the same kind of information as a referential approach, but it is more easily scalable to large amounts of data.

In the context of this project, token-level models for semasiological research are introduced by @heylen.etal_2015. Another work-package, culminating in @depascale_2019's PhD dissertation, applies the technique to lexical lectometric research, i.e. measuring distances between language varieties based on their naming choices for different concepts. The visualization tool, as mentioned before, is first described in @wielfaert.etal_2019. Between their work, this dissertation and further case studies taking place in the last year, the project is covering the application of token-level vector space models on semasiological, onomasiological and lectometric studies in varieties of Dutch and Mandarin, at both synchronic and diachronic level.

## Structure of the dissertation {#str}

As a product of the Nephological Semantics project, this dissertation aims to contribute to both the development and understanding of distributional models for lexical semasiological research.
It brings together the theoretical perspective on semantics from Cognitive Linguistics with computational methods and visual analytics in the hope to pave the way for future research along the same lines.
With that in mind, the three chapters of the first part of this dissertation, *The cloudspotter's toolkit*, will focus on the technical or methodological side of the project.
Chapter \@ref(workflow) will describe the procedure to create clouds and the parameter settings explored, taking care to be thorough and specific about the technical decisions that resulted in the final models.
Chapter \@ref(nephovis) will showcase the visualization tool designed by Thomas Wielfaert and me as well as a ShinyApp extension that provides additional functionalities.
Finally, Chapter \@ref(dataset) will illustrate the dataset on which the models were tested: the selection of lemmas and the questions they try to address, the collection of data and the annotation procedure.

The notion behind token-level models, i.e. that we can represent meaning differences in terms of distributional differences, and in particular the image of a scatterplot that translates these intuitions into an interpretable picture, *sounds good*.
Alas, the reality is not as bright as we could have wished for, and the skies of distributional semantics have all but a stable weather. Hopefully, this dissertation can offer a guide for researchers who would dare tread these waters. The three chapters in *The cloudspotter's handbook*, therefore, will discuss the results of the analyses, with an emphasis on the naive misconceptions that such a researcher might understandably bring with them. First, Chapter \@ref(shapes) debunks the idea of a perfect cloud emerging from the ocean of the corpus. Clouds come in many different shapes, caused by different phenomena of distributional behaviour, and thus this chapter offers a classification of what we might encounter. Chapter \@ref(semantic-interpretation) follows with a linguistic perspective on the variation of these shapes and discusses what we can or we cannot find in these models. Finally, Chapter \@ref(no-optimal) shows how no set of parameter settings offers the best solution across the board --- not even close. Instead, the same parameter settings will return different shapes for different lemmas, and they have to be tailored *to the specific lemma* to capture the relevant semantic structure.

An enthusiastic and hopeful aspiring cloudspotter might feel discouraged by the variability --- bordering on unpredictability --- of these clouds. I wouldn't blame them. However, in spite of the diversity of shapes, of semantic phenomena and of parameter settings to explore, the methodology can offer interesting insights. They are partial insights, but insights nonetheless, and once we know what to expect from clouds, we can focus on acquiring them. In that perspective, the third and final part of this dissertation will close with a general practical guide, a summary of suggestions for further research and an overall conclusion.
