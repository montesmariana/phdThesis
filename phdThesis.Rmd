--- 
title: "Cloudspotting: Visual analytics for distributional semantics"
author: "Mariana Montes"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography: [assets/bib/PhDCitations.bib, assets/bib/packages.bib]
biblio-style: apa
csl: assets/bib/unified-style-sheet-for-linguistics.csl
link-citations: yes
description: "This is my PhD dissertation, still a draft, always perfectable."
---

```{r, code=readLines("src/init.R"), include = FALSE}
```

```{r, include = FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
```


# Abstract {-}

The present study is part of the Nephological Semantics research project at QLVL,
which aims to develop tools for large-scale corpus-based semantic analyses.
A core aspect of the project involves representing semantic structure with vector space models (VSMs),
a computational tool that currently requires a deeper understanding of its inner workings
and how its results relate to cognitive theories of meaning. 

Count-based VSMs represent words[^word] as vectors of co-occurrence frequencies in a multidimensional space
[@turney.pantel_2010; @lenci_2018]. Basically, a word is represented by
its association strength to other words.
They can be generated at both type- and token-level [@heylen.etal_2012; @heylen.etal_2015; @depascale_2019].
At type level, two words are represented as more similar if they are attracted to the same
contextual features (e.g. other words) and repelled by the same contextual features. This should
allow us to identify semantic fields and other relationships between words, but collapses the full
range of contexts of each word into one representation.
At the token level, instead, we look at individual *occurrences*, and define them as more similar if
the words in their contexts are attracted to and repelled by the same contextual features.
This way we should be able to map the internal variation of the behavior of individual words,
i.e. their semasiological structure.

Within the larger Nephological Semantics project, this work package is dedicated
to the understanding of token-level vector space models as a tool
for the study of polysemy. Concretely, we explore a number of parameter settings for the models,
i.e. ways of defining the context used to represent each tokens, and their impact on the
resulting representation, by means of visual analytics.
We used manual annotation of sense tags as a heuristic, but without
considering them a golden standard. Instead, we aim to map parameter settings to various
semantic phenomena coded in the annotations, such as
meaning granularity (e.g. distinguishing homonyms and senses within the homonyms).
The vector space models, which take the form of large matrices,
can be reduced to two dimensions via different methods,
such as t-SNE [@Rtsne2008; @Rtsne2014].
These coordinates can then be mapped onto a scatterplot, resulting in a variety of
shapes, which we call *clouds*.

The workflow was applied to a set of 32 Dutch nouns, verbs and adjectives exhibiting
a range of semantic phenomena. For each of them, 240-320 concordance lines were extracted,
annotated and modeled. The combination of parameter settings, some of which included syntactic
information, resulted in 200-212 different models. The models were clustered with Partition
Around Medoids [@kaufman.rousseeuw_1990; @R-cluster] so that a manageable, representative set could be explored
in more depth, in particular visualizing their t-SNE representations.

Preliminary results suggest that the shape of the clouds depends on the
distinctiveness of the collocational patterns, which may or may not match the sense
annotations. Noisier models can smooth over these sharp distinctions, while more refined
models emphasize them. More importantly, there is no set of parameters that works
across the board.

[^word]: The term *word* is used very loosely here to encompass different possible definitions.

<!--chapter:end:index.Rmd-->

# Acknowledgments {-}

The words in this pages, the thoughts they try to convey, are the result of
years of thinking, discussing, studying, learning. My voice weaves them together,
but it draws from so many sources that have encouraged my growth, stood by me,
fed my curiosity, passion and enthusiasm for everything that makes up this text.

For their support and their ideas, I want to thank my supervisors Dirk Geeraerts,
Dirk Speelman and Benedikt Szmrecsanyi. Each of them gave their own piece, building
my confidence, encouraging me to keep exploring and learning. My main supervisor,
Dirk Geeraerts, deserves a special acknowledgment for all the hours in deep
discussion on the complex issue that is *meaning*, and what all this is about after
all. I appreciate his patience and his engagement. Every time we talked I left
feeling more excited and passionate about the topic, more confident and happy.
Hartelijk bedankt.

<!-- NOTE Where should I include the jury? -->

I must also thank my colleagues from the Linguistics Department at KU Leuven,
which at the different stages of my years here have been a lovely company and
support system. In particular, I would like to thank the members of the
Nephological Semantics project, with whom I shared so much of the excitement and
frustrations of our common project.
<!-- TODO Specify names? -->

When I joined the project, I already brought with me my own history and connection
to (cognitive) linguistics, corpora, statistics and programming. I honestly wouldn't
be where I am today if it weren't for my parents, Miguel and Patricia. They have
always supported and fostered my study and my interests, given me the tools to
grow, to face new challenges. I know I can, because they believe it too.

<!-- TODO Add Alex, Paula, mis hermanos. UNC? -->

<!-- TODO Taihou <3 -->

<!--chapter:end:gracias.Rmd-->

# Introduction

This dissertation concerns itself with the application of distributional methods,
developed within the field of Computational Linguistics (Section \@ref(comp)),
to lexicological research, in particular the theoretical framework of Cognitive Semantics (Section \@ref(cog)).
In addition, the study makes heavy use of visual analytics (Section \@ref(viz)).

It is part of a larger project, Nephological Semantics, an even a longer research programme within QLVL (Section \@ref(nephosem)).

## Distributional semantics and Computational Linguistics {#comp}

Distributional semantics is a popular technique in Computational Linguistics, where it originated.
[Brief mention of references, but I will elaborate what it's about in the first technical chapter.]
It relies on the Distributional Hypothesis, which states a correlation between the distributional properties
of words and their meaning (or rather, between differences in distribution and differences in meaning).

There are some differences between the use of this technique in Computational Linguistics and what we'll show
in this thesis.

First, Computational Linguistics is typically task-oriented, and tests its models by comparing their
results to benchmarks, or gold standards [reference to some of them]. In contrast, we will
take manual annotation as a guideline, but not as a ground truth, and we are more interested in
learning what the models can tell us about the behaviour of words and in *how* models differ than
in their accuracy. [Admittedly, this is in part because there is no best model.]

Second, their distributional models mostly work at type-level and with word forms as units,
whereas we will look into token-level models with lemmas as units.
Of course, there is work at token-level in computational linguistics (as I will describe in the first chapter)
but it is not as popular as the type level. A more recent exception is BERT and family.

Third, since the advent of prediction-based models [Mikolov et al 2013...], word embeddings have become
increasingly popular; a number of papers have compared the performance of these and count-based models,
with different results, but in practice, neural networks are the norm in NLP. For these studies, we
looked into count-based model as a more transparent method, i.e. one in which we can trace the similarities
between tokens to the words that co-occur with them and their type-level similarity.
As the rest of the dissertation will show, the models are not necessarily as transparent as we thought they would be,
but that intuition was still the reasoning behind the preference for count-based models.

[Can I equate Computational Linguistics and NLP?]

## Distributional semantics and Cognitive Semantics {#cog}

[For the computational audience?]

Cognitive Linguistics is a theoretical framework characterized by (among other principles) an emphasis on meaning
(everything can be meaningful), the notion of fuzzy and prototypical categories and an usage-based approach.
[Bunch of references!]

These three cornerstones inform our study of distributional models in this dissertation.

First, given that the Distributional Hypothesis that underlies the methodology suggests a correlation between
distributional differences and semantic differences, what *kind* of semantic differences are at play?
Can we model different semantic dimensions with different distributional properties?
What kind of semantic phenomena (specialization, generalization, metaphor, metonymy... or even animacy, concreteness) can be modelled?
[This from a semasiological point of view; refer to onomasiological and lectometric studies too :) ]

Second, the notion of fuzzy and prototypical categories underlies a sceptical perspective towards the existence of senses
as discrete categories [more references] and encourages us to pursue a mechanism that can represent semasiological structure
in a non concrete way. To a certain degree, we need discrete entities to talk about them, we need to classify things,
and both the sense annotation and the use of clustering algorithms respond to such needs. But we don't take them as a norm; instead,
we embrace the presence of noise, of degrees of membership, of partial overlap between solutions and general tendencies.
The full dissertation should be read in this light.

Third, the usage-based approach is easily mapped into this bottom-up, empirical, quantitative methodology,
within an already established trend in Cognitive Linguistics [references!]. With a few exceptions (we'll see),
all the examples will be (randomly?^[I like this idea, because it's (1) more honest and (2) fast, but might not return the best illustrations]) extracted from the samples taken for the case studies. The arguments exposed in this
dissertation are backed up by the data used (which are available in The Cloud) and the specific analyses performed,
which also means that their generalization power is limited to that data and analyses.
Given that the results don't *look* like they would be specific to this corpus (not in terms of the concrete descriptions of the lemmas,
but the methodological and theoretical generalizations), it is likely that they apply to other forms as well, but of course,
it's an empirical question :)

## Visual analytics {#viz}

Brief introduction of the motivation and rationale of the visualization tools (which will get their own chapter.)

<!-- In order to explore the non discrete organization of attestations in the semantic space, -->
<!-- we can combine dimensionality reduction techniques with a graphic representation to visualize them -->
<!-- as points on a 2-dimensional space. While spatial distances represent distributional similarities, -->
<!-- colour-coding can represent the mapping of manually annotated senses, lects and clustering solutions. -->
<!-- More importantly, the visual tools developed within our research team and used for this project can also let us explore the -->
<!-- plots interactively, checking the actual contexts, finding the relative distances of points across different models, -->
<!-- and retrieving the context words responsible for the organization. -->

## Nephological Semantics {#nephosem}

Brief history and description of the project, how it brings the three points above together,
and what this PhD shares with previous PhD's/publications within the project or what holes it fills.
For this I still have to reread the project description, Stefano's subsection on his PhD and your Chapter 1 (any other suggestions?).

## Structure of the dissertation {#str}

The rest of the dissertation will consist of three main parts.
The first part will focus on the methodological procedure and choices taken in this study:
the workflow to create vector space models (including an introduction to distributional semantics),
the visualization tool and clustering algorithms, and the selection of a dataset, its annotation, parameter settings, etc. (Not necessarily in that order.)
The second part will focus on the most important findings from the study [i.e. three main points].
The third (and less substantial?) part will round up the dissertation with a general practical guide (tips, tricks and warnings),
suggestions for further research and a conclusion. [Whether this amounts to one or two chapters, I'll still have to see.]
<!-- TODO add project number: BOF C1-project (3H150305) “Nephological Semantics: using token clouds for meaning detection in variationist linguistics”; history in Stefano's thesis, pp. 7-8 -->


<!--chapter:end:introduction.Rmd-->

# (PART) Visualization tool {-}

# An interface to the world of clouds

In this and the following chapters, we will go through the steps needed to obtain "clouds" from corpora and how to explore them in the visualization tool developed within the Nephological Semantics project. It was originally created by [Thomas Wielfart](https://github.com/tokenclouds/tokenclouds.github.io),
using [D3](https://d3js.org), a popular Javascript library for data-driven visualization, and then further developed by me, culminating in the [present version](https://github.com/qlvl/NephoVis) [@montes.qlvl_2021].

This section will include the technical description of the workflow, as it pertains
to the tool itself and to the processing work made before (the Python module, other
Python and R functions), and a sort of manual of how it's used.

<!-- TODO short introduction of the outline of this PART -->

Visual analytics aims to integrate statistical data analysis with techniques from information visualization so that human analysts can recognize, interpret and reason about the statistical patterns that the data analysis reveals [@card.etal_1999]. Importantly, a visual analytics approach offers a manipulable, interactive visualization that, unlike static diagrams, enables the exploration of a space of parameter values and modeling outputs.

The following chapters interlace some technical explanations of the methodology with
description of the actual steps taken in this research. Chapter \@ref(workflow) offers an introduction
to distributional models and everything that a researcher needs to know to go from a corpus
to the material needed for the visualization. This abstract description is followed by
an explanation of the specific parameters explored in our case studies in Chapter \@ref(params).
This information should be enough to understand the usefulness of the visualization,
thoroughly described in \@ref(nephovis), which builds on @montes.heylen_Submitted.
Chapter \@ref(hdbscan) expands the analytical possibilities combining the output of the workflow
with HDBSCAN and visualizing it on a Shiny App.
Finally, Chapter \@ref(ann) gives an overview of the annotation procedure. While the step itself
was taken before any modeling was performed, it is only necessary for visual evaluation
and more relevant to the theoretical insights in the second part of this thesis.

<!--chapter:end:viz_1.Rmd-->

# From corpora to clouds {#workflow}

The main goal of the distributional models discussed in this text is to explore semasiological structure from textual data.
The starting point is a corpus, and one of the most tangible outputs is the visual representation as a cloud.
In this chapter we will describe how to generate clouds from the raw, seemingly indomitable ocean of corpora.

First, we will describe how token-level vector space models are created.
Section \@ref(vector-creation) will explain count-based models, but this is by no means the only viable path.
Other techniques, such as BERT [@BERT]^[See also @devries.etal_2019 for a Dutch version.], that can generate vectors for individual instances of a word,
can be used for the first stage of this workflow.
<!-- TODO explain why we don't use them -->

Once we have vector representations of individual instances of a lexical item, we need to process them.
For visualization purposes, we need to reduce the numerous dimensions of the vectors to a manageable number, such as 2.
Section \@ref(dim-reduction) will explore and compare a few alternatives.

The same output that is put through dimensionality reduction for the visualization can also
be submitted to other forms of analysis such as clustering algorithms, whose results may
even be combined with the visualization. We will look at HDBSCAN in particular in Chapter \@ref(hdbscan).

## A cloud machine {#vector-creation}

At the core of vector space models, *aka* distributional models, we find the Distributional Hypothesis, which is most often linked to Harris's observation that "difference of meaning correlates with difference of distribution" [-@harris_1954 156], but also to @firth_1957a and Wittgenstein.
<!-- TODO read and add Wittgenstein -->
In other words, items that occur in similar contexts in a given corpus will be semantically similar, while those that occur in different contexts will be semantically different [@jurafsky.martin_2020, Ch. 6; @lenci_2018]. Crucially, this does not imply that we can describe an individual item with their distributional properties, but that comparing the distribution of two items can tell us something about their semantic relatedness [@sahlgren_2006 19].

@firth_1957a inspired a whole tradition of corpus linguistics which started to look at collocations as part of the semantic description of a lemma. The Cobuild dictionary was the first to integrate collocational information into their entries. The Birmingham school, pioneered by John Sinclair, used co-occurrence frequency information to describe a lexical item (most often word forms, in English; see @sinclair_1991 [29] and; @stubbs_1995 [23-24]) by the set of those context words most attracted to them. Researchers would normally transform the raw frequencies with association measures such as mutual information
[@church.hanks_1989; @stubbs_1995 33; @mcenery.etal_2010; @gablasova.etal_2017]
or t-score
<!-- TODO add proper sources, probably sinclair, stubbs, others... @gablasova.etal_2017-->
among others (see @gablasova.etal_2017 for an overview),
set a threshold (around 3 for mutual information)
<!-- TODO check references: maybe sinclair and stubbs, also mcenery, gablasova... -->
and rank the context words that survive such threshold.

Count-based vectors basically compare two items by contrasting their list of collocates, but instead of implementing a binary distinction based on an arbitrary threshold,
<!-- TODO add citation -->
the magnitude of the association strengths will play a role. Rather than measuring the overlap between the traditional collocational profiles, it will depend on what the actual values are.

Distributional models operationalize[^oper] this idea by representing words as vectors (i.e. arrays of numbers) coding frequency information. Typically, the raw frequency is transformed to some association strength measure, such as pointwise mutual information [PMI, see @church.hanks_1989], which compares the frequency with which two words occur close to each other and the expected frequency if the words were independent. For example, Table \@ref(tab:vec1) shows small vectors representing the English nouns *linguistic*, *lexicography*, *research* and *chocolate*, as well as the adjective *computational*, as series of association strengths with a set of lemmas. Empty cells indicate that the word in the row and the word in the column never co-occur in the corpus (given a certain window span).

[^oper]: See @stefanowitsch_2010 for a discussion on definitions and operationalizations of *meaning* and their relationship to an overarching theoretical notion of *meaning*.
<!-- TODO add reference to the appropriate theoretical section -->

```{r, vec1, echo=FALSE, tab.cap="Example of type-level vectors."}
vex <- read_csv("assets/vector-examples/vectorexample.csv", col_types = cols()) %>% 
  mutate_if(is.numeric, round, 2) %>% 
  mutate_if(is.numeric, na2zero)
vex %>% 
  # select(-`eat/v`) %>%
  flextable() %>% 
  add_footer_lines("PMI values based on symmetric window of 10; frequency data from GloWbE.")
```

Each row is a vector coding the distributional information of the lemma it represents. By *lemma* we refer to the combination of a stem and a part of speech, e.g. *chocolate/n* covers *chocolate*, *chocolates*,  *Chocolate*... How we define the unit is a decision we must make when we build a model (see Chapter \@ref(params)); in computational linguistic research, this unit is often a word form, and the difference between using word forms or lemmas varies drastically depending on the language of the corpus. More importantly, we must remember that the vectors represent a distributional profile of the lemma, which is automatically extracted from a corpus, not of a meaning [Cf. @bolognesi_2020 82].

Table \@ref(tab:vec1) offers a brief example in which semantically similar lemmas (e.g. *linguistics* and *lexicography*) have similar vectors, while semantically different lemmas (e.g. *linguistics* and *chocolate*) have different vectors.

These are type-level vectors: each of them aggregates over all the instances of a given lemma, e.g. *linguistics*, to build an overall profile. As a result, it collapses the internal variation of the lemma, i.e. its semasiological structure. In order to uncover such information, we need to build vectors for the individual instances or tokens, relying on the same principle: items occurring in similar contexts will be semantically similar. For instance, we might want to model the three (artificial) occurrences of *study* in (@ex1) through (@ex3), where the target item is in italics.

(@ex1) Would you like to *study* lexicography?
(@ex2) They *study* this in computational linguistics as well.
(@ex3) I eat chocolate while I *study*.

Given that, at the aggregate level, a word can co-occur with thousands of different words, type-level vectors can include thousands of values. In contrast, token-level vectors can only have as many values as the individual window size comprises, which drastically reduces the chances of overlap between vectors. In fact, the three examples don't share any item other than the target. As a solution, inspired by @schutze_1998, we replace the context words around the token with their respective type-level vectors [@heylen.etal_2015; @depascale_2019].

For example, we could represent example (@ex1) with the vector for its context word *lexicography*, that is, the second row in Table \@ref(tab:vec1); example (@ex2) with the sum of the vectors for *linguistics* (row 1) and *computational* (row 3); and example (@ex3) with the vector for *chocolate* (row 5). This not only solves the sparsity issue, ensuring overlap between the vectors, but also allows us to find similarity between (@ex1) and (@ex2) based on the similarity between the vectors for *lexicography* and *linguistics*.

From applying this method we obtain numerical representations of occurrences of a word. We can compare them to each other by calculating pairwise distances, which is at the base of clustering analyses (such as HDBSCAN, see Chapter \@ref(hdbscan)) and visualization techniques based on dimensionality reduction (section \@ref(dim-reduction)).

However, in order to obtain this result we need to make a number of decisions related, among other things, with how we define *word* and *context* [Cf. @bolognesi_2020 83]. These decisions will be discussed in Chapter \@ref(params).

## Dimensionality reduction {#dim-reduction}

Dimensionality reduction algorithms try to reduce the number of dimensions of a high-dimensional entity while retaining as much information as possible. In Latent Semantic Analysys (LSA)
<!-- NOTE add authors, Sahlgren, Van De Cruys... -->
it is used to reduce type-level spaces from thousands of dimensions to a few hundred, and the resulting reduced dimensions have been found to correspond to semantic fields.
@vandecruys_2008 tried both SVD and non-negative matrix factorization on type-level BOW-based models with whole paragraphs as windows but they did not bring any improvement.

It could also be used for token-level spaces, but the comparisons discussed in @depascale_2019 [246] indicate that they don't necessarily perform better than non reduced spaces. Besides, as we will see in Chapter \@ref(params), we can also generate spaces that are already in the few hundred dimensions by selecting the first order context words as second order features, which does not underperform in our studies. Skipping the SVD step also implies more transparent vectors, facilitating the understanding of what is going on under the hood, but, as many other paths we have not taken, the application of SVD remains a parameter to explore in the future.
<!-- NOTE Dimensionality reduction can also refer to SVD and stuff like that, so we should mention it and dismiss, cite Stefano [246]
Also check Van De Cruys' work, some LSA? Remember: Kiela+Clark do not look into this-->

Both dimensionality reduction techniques and neural networks are suggested as ways of condensing very long, sparse vectors [@jurafsky.martin_2020; @bolognesi_2020],
but if the FOC-based selection of second order dimensions suffices, it is not really needed. Indeed, we might want to compare SVD versions and embeddings with these vectors, but for the purposes of understanding what is going on and obtaining meaningful information for corpora, this is still cool!

The algorithms we will discuss in this section, instead, try to locate different items on a low-dimensional space (e.g. 2D) preserving their distances in the high-dimensional space (e.g. 5000D) as well as possible.
The literature up to today tends to go for either multidimensional scaling (MDS) or t-stochastic neighbor embeddings (t-SNE);
recently, an interesting alternative called UMAP has been introduced, which we'll discuss shortly.

MDS is an ordination technique, like principal components analysis (PCA). It tries out different low-dimensional configurations aiming to maximize the correlation between the pairwise distances in the high-dimensional space and those in the low-dimensional space: items that are close together in one space should stay close together in the other, and items that are far apart in one space should stay far apart in the other.
It can be evaluated via the stress level, the complement of the correlation coefficient: if the correlation between the pairwise distances is 0.85, the stress level is 0.15.
Unlike PCA, however, the dimensions are not meaningful *per se*; two different runs of MDS may result in plots that mirror each other while representing the same thing. Nonetheless, the R implementation (in particular, `metaMDS()` of the {vegan} package, see @R-vegan) rotates the plot so that the horizontal axis represents the maximum variation.
In cognitive linguistics literature both metric [@hilpert.correiasaavedra_2017; @hilpert.flach_2020; @koptjevskaja-tamm.sahlgren_2014]
and nonmetric MDS [@heylen.etal_2015; @heylen.etal_2012; @depascale_2019; @perek_2016] have been used.

The second technique, t-SNE [@Rtsne2008; @Rtsne2014], has also been incorporated in cognitive distributional semantics [@depascale_2019; @perek_2018].
It is also popular in computational linguistics [@smilkov.etal_2016; @jurafsky.martin_2020 118]; in R, it can be implemented with the `Rtsne` function of the homonymous package [@R-Rtsne].
The algorithm is quite different from MDS. For our purposes the crucial point is that it prioritizes preserving local similarity structure instead of the global structure: items that are close together in the high-dimensional space should stay close together in the low-dimensional space, but those that are far apart in the high-dimensional space may be even farther apart in low-dimensional space.

This leads to nice, tight clusters but the distance between them is less interpretable than in an MDS plot (where, in any case, tight clusters are a rare occurrence in these studies). In other words, we can interpret tight groups of tokens as being similar to each other, but we cannot extract meaningful information from the distance between these groups. In addition, it would seem that points that are far away in a multidimensional space might show up close together in the low dimensional space [@oskolkov_2021]. Uniform Manifold Approximation and Projection [@mcinnes.etal_2020], instead, penalizes this sort of discrepancies. It would be an interesting avenue for further research, but a brief test on the current data did not reveal such great differences between t-SNE and UMAP to warrant the replacement of the technique within the duration of this project. Other apparent advantages such as speed were not observed in the small samples under consideration (in fact, UMAP ---or at least its R implementation with the {umap} package [@R-umap]--- was even slower).

In both cases we need to state the desired number of dimensions before running the algorithm ---for visualization purposes, the most useful choice is 2. Three dimensions are difficult to interpret if projected on a 2D space, such as a screen [@card.etal_1999 18; @wielfaert.etal_2019 222]. In addition, t-SNE requires setting a parameter called perplexity, which basically sets how many neighbors the preserved local structure should cover.

<!--chapter:end:viz_3.Rmd-->

# Parameter settings {#params}

In this chapter I will describe the various parameter settings we have explored:
which are the possible decisions, which ones we have set and which were looked at,
why. This should be preceded by an explanation of the workflow itself.


## Fixed decisions

There are a number of parameters that were set fixed for the studies in this project, but which are by no means the only possibility. In some cases we rely on previous work to set a fixed solution, while in other cases it was rather a matter of practicality: time and resources are limited.

- Corpus: QLVLNews corpus ---language: Dutch, register: newspapers, mode: written, period: 1999-2004

- Unit definition: stem + part of speech (it could have been wordform, just stem, wordform and part of speech, or even include dependency information) See @kiela.clark_2014 [25] for results on English, favoring stemming but not requiring pos tags.

- Cosine distances between vectors See @jurafsky.martin_2020 105, but @kiela.clark_2014 suggest correlation instead; euclidean distances between models; log transformation of ranks... 

- PPMI values [@kiela.clark_2014; @depascale_2019] --- @vandecruys_2008 distinguishes between local and global PPMI; we use global PPMI

- Window size for second order: 4 (used by depascale I think,
did not show very different results from win 10)

## First steps

Both the targets and the first and second order features are lemma/part-of-speech pairs,
such as *haak/verb* (the verb *haken* 'to hook/crochet'),
*beslissing/noun* (the noun *beslissing* 'decision'), *in/prep*
(the preposition *in* 'in').
The features (or context words) can have any part of speech except for punctuation
and have a minimum relative frequency frequency of 1 in 2 million (absolute frequency of 227)
after discarding punctuation from the token count in the full
QLVLNews corpus. There are 60533 such lemmas in the corpus.

(This threshold is more or less arbitrary, but we're assuming that words with a lower frequency
won't have a rich enough vectorial representation.)

In the steps between defining corpus and types and obtaining a the token-level vectors,
we have two main kinds of parameters to explore.
**First-order parameters** influence which context features will be selected
from the immediate environment of the target tokens,
while the **second-order parameters** influence the shape of the vectors
that represent such first-order features.

In order to visualize the tokens, we have performed dimensionality reduction, i.e.
a process by which we try to represent relative distances between items in a low-dimensional space
while preserving the distances in high-dimensional space as much as possible.
This procedure was described in Section \@ref(dim-reduction).

## First-order selection parameters

We call the immediate context of a token the **first order context**: therefore,
first-order parameters are those that influence which elements in the immediate environment
of the token will be included in modeling said token. This was made in two stages:
one dependent on whether syntactic information was use, and one independent of it.

It goes without saying that the parameter space is virtually unlimited, and decisions
had to be made regarding which particular settings would be explored. We tried to
keep the parameter settings different enough from each other to have some variation.
The decisions were based on a mix of literature [@kiela.clark_2014],
linguistic intuition and generalizations over the annotation of our very targets.
<!-- TODO Reference to wherever I talk about it -->
As part of the annotation task, the annotators had to select the items in the
immediate context that had helped them select the appropriate tag. In order to remove
noise from misunderstandings and idiosyncrasy, we only looked at pairs (or trios) of
annotators that had agreed with each other and with our final annotation and ranked
the context words over which they had agreed. The distance and dependency information
of these context words were used to inform some of our decisions below.

On the first stage, the main distinction is made by `BASE`: between bag-of-words (`BOW`) based
and dependency-based models (`LEMMAPATH` and `LEMMAREL`).
The former are further split by window size (`FOC-WIN`), part-of-speech filters (`FOC-POS`)
and whether sentence boundaries are respected (`BOUND`).

`FOC-WIN` (first order window)
:    A symmetric window of 3, 5 or 10 tokens to each side of the target was used.
:    Of course, virtually any other value is possible [add references!]. Windows of
5 and 10 are typical in the literature [sources?], while 3 was enough to capture most
of the context words tagged as informative by the annotators.

`FOC-POS` (first order part-of-speech)
:    A restriction was placed to only select (common) nouns, adjectives, verbs and adverbs (`lex`)
in the surroundings of the token. If no restriction is placed, the value of this parameter is `all`.
:    Of course, other selections are possible. [add reference] distinguish between
`nav`, which only includes common nouns, adjectives, and verbs, and `nav-nap`, which
expand the selection to proper nouns, adverbs and prepositions.
:    A more detailed research on different combinations would be material for further
research. As we will see, the `lex` filter is often redundant with the one based on
association strength.

`BOUNDARIES`
:    Given information on the limits of sentences (e.g. in corpora annotated for
syntactic dependencies), we can exclude context words beyond the sentence of the target (`bound`)
or include them (`nobound`).
:    This parameter seems to be virtually irrelevant. It was thought as a way of
leveling the comparison with the dependency-based models, which by definition don't
include context words beyond the sentence, but they don't seem to make a difference.

The distinction between BOW- and dependency-based model doesn't rely so much on
which context words are selected but on how tailored the selection is to the specific
tokens. For example, a closed-class element like a preposition may be distinctive
of particular usage patterns in which a term might occur. However, such a frequent,
multifunctional word could easily occur in the immediate raw context of the target
without actually being related to it. Unfortunately, just narrowing the window span
doesn't solve the problem, since it would also drastically reduce the number of 
context words available for the token and for any other token in the model.
In contrast, we could also have context words that are directly linked to the target
but separated by many other words in between, and enlarging the window to include
them would imply too much noise for this token and for any other token in the model.

A dependency-based model, instead, will only include context words in a certain 
syntactic relationship to the target, regardless of the number of words in between.
The actual selection process takes two forms in our case: by path length and by
relationship. The former, which we call `LEMMAPATH`, is similar to a window size
but counts the steps in a dependency path instead of slots in a bag-of-words window.
The latter, `LEMMAREL`, matches the dependency paths to specific templates inspired
by the context words tagged as informative by the annotators.

To exemplify, let's look at (@expath) and take *herhalen* 'to repeat' as the target.

(@expath) *De geschiedenis rond Remmelink herhaalt zich.* 'The history around Remmelink repeats itself.'

This is different from say @vandecruys_2008 where the combination of dependency relation and item make up the feature (I think). This requires a (carefully planned) type-level matrix with dependency features, which we didn't get to generate, but it might be interesting for future research, if it indeed is better at distinguishing between nouns with different degrees of concreteness (although apparently this is also achieved by a BOW model with global PPMI, which is what we use...).


`LEMMAPATH`
:    This set of dependency-based models selects the features that enter a syntactic
relation with the target with a maximum number of steps.
:    The possible values we have included are `selection2` and `selection3`, which filter
out context words more than two or three steps away, respectively, and `weight`, which
gives a larger weight to context words that are closer in the dependency path.
:    A one-step dependency path is either the head of the target or its direct dependent.
Such features are included by both `selection2` and `selection3` and receive a weight of 1 in `weight`.
In (@expath) this includes the subject, *geschiedenis* 'history', and the reflexive pronoun *zich*,
which depend directly on it. If the target was *geschiedenis* 'history', *herhalen* 'to repeat',
its head, would be selected.
:    A two-step dependency path is either the head of the head of the target, the dependent of its dependent,
or its sibling. Such features are included by both `selection2` and `selection3` and receive a weight of 2/3 in `weight`.
In (@expath) this includes the determiner *de* and the modifier *rond* 'around' directly depending on
a *geschiedenis* 'history'.
:    A three-step dependency path is either the head of the head of the head of the target,
the sibiling of the head of its head, the dependent of the dependent of its dependent,
or the dependent of a sibling. A typical case of the last path is the subject of a passive construction with a modal,
where the target is the verb in participium (*belastingen* 'taxes' in *de belastingen moeten geheven worden* 'the taxes must be levied').
Such features are included in `selection3` but excluded from `selection2` and receive a weight of 1/3 in `weight`.
In (@expath) this corresponds to *Remmelink*, the object of *rond* 'around'.
:    Features more than 3 steps away from the target are always excluded.
While some features four steps away can be interesting, such as passive subjects of a verb with two modals, they are not that frequent and may not be worth the noise included by accepting all features with so many steps between them and the target. To catch those relationships, `LEMMAREL` is a more efficient method.
There are no context words more than three steps away from the target in (@expath).

See @kiela.clark_2014 24-25 for a discussion of type-level results with a similar approach compared to a small BOW window. I don't think it's really comparable though.

`LEMMAREL`
:    This set of dependency-based models selects the features that enter in a certain
syntactic relation with the target. They are tailored to the part-of-speech of the target,
and each group expands on the selection of the group before it. The specific selections
are listed in Table \@ref(tab:lemmareltable).

<!-- BUG Check formatting for PDF and EPUB because it is ugly in the former and nonexistent in the latter -->

```{r, "lemmareltable", anchor = "Table"}
# Looks really bad in pdf, but at least computes!
lrel <- jsonlite::read_json("assets/lemmarel.json")
tibble(
  groups = c("1", "2", "3"),
  nouns = map_chr(c("1", "2", "3"), ~lrel$nouns[[.]]),
  verbs = c(map_chr(c("1", "2"), ~lrel$verbs[[.]]), ""),
  adjectives = c(map_chr(c("1", "2"), ~lrel$adjectives[[.]]), "")
) %>% 
  flextable() %>% 
  valign(valign = "top") %>% 
  autofit() %>% 
  set_caption(caption = "Dependency paths selected by different `LEMMAREL` values.")
```


### PPMI weighting

The `PPMI` parameter is taken outside the set of first-order parameters because it can both filter out first-order features and reshape their vector representations. In truth, the choice of **p**ositive **p**ointwise **m**utual **i**nformation (PPMI) over other weighting mechanisms, as well as setting a threshold or not, is already a parameter setting, which in these circumstances is set to PPMI and a threshold of 0. In all cases, the PPMI was calculated based on a 4-4 window (that could also be a variable parameter).

This parameter can take three values. `selection` and `weight` mean that only the first-order features with a PPMI > 0 with the target type are selected, and the rest discarded, while `no` does not apply the filter. The difference between `selection` and `weight` is that the former only uses the value to filter the context features, while the latter also weighs their vectors with that value.

### Second-order selection

The selection of second-order features influences the shape of the vectors: how the selected first-order features are represented. While the frequency transformation and the window on which such values were computed could be varied, they were set to fixed values, namely PPMI and 4-4 respectively. The parameters that were varied across, although we don't expect drastic differences between the models, are vector length and part-of-speech.

`SOC-POS` (second order part-of-speech)
:    This parameter can take two values: `nav` and `all`. In the former case, a selection of 13771 lect-neutral nouns, adjectives and verbs made by Stefano is taken as the set of possible second-order features. In the latter, all lemmas with frequency above 227 and any part-of-speech are considered.

`LENGTH`
:    Vector length is the number of second-order features and therefore the dimensionality of the matrices on which the distance matrices are based, although the amount is not all that changes. It is applied after filtering by part-of-speech.
:    We have selected two values: `5000` and `FOC`. The former includes the 5000 most frequent elements of the possible features, while the latter takes the intersection between the possible second-order-features and the first-order-features, regardless of frequency. With `SOC-POS:all`, `FOC` will include all first-order features of that model, while with `SOC-POS:nav`, only those included in Stefano's selection.
:    The actual number of dimensions resulting from `FOC` depends on the strictness of the first order filter. This information can be found on the plots that, for each staal, show how many first order context words are left after each combination of first order filters.

@kiela.clark_2014 24 say it is not worth it to make it longer than 50k; jurafsky.martin_2020 did mention something of the kind too
Does stefano say anything?

#### FOC as SOC

What does it mean to use the same first-order context words as second-order context words?

First, depending on the number of target tokens and the strictness of the filter, there could be a different number of context words, ranging in the hundreds or low thousands.

Second, the context words will be compared based on their co-occurrence with each other. The behaviour of a context word outside the context of the target will be largely ignored: of course, the association strength between two items has to do with their co-occurrence across the whole corpus, as well as their non-co-occurrence, but it will only be included in the second order vector of the first item if the second is also among the first order context words.



## Medoids

The multiple parameters return a huge number of models, and while purely quantitative methods might
be able to process and compare them, it is not feasible for a human to look at hundreds of clouds
and stay sane enough to make out anything from them. A more efficient --and easier on the human mind--
way to approach this is, instead, to look at representative models.

<!-- QUESTION: Describe PAM? -->

This method requires us to choose a number of medoids beforehand, which is not an easy
task. If we wanted the medoids to represent the best clustering solution, we could run
the algorithm with different values of $k$ and compare the results with measures such
as silhouette width, as suggested by @levshina_2015. However, that is not
necessarily our goal. We want to be able to see as much variation as possible, while keeping
the number of different models manageable (i.e. below 9). It is not particularly problematic
if these models are redundant, as long as we can ensure that all the phenomena that we
are interested in are represented in them.

For example, given a lemma with multiple senses, it might be the case that some models
group the tokens of one sense, and others group the tokens of another: we would like to see
representatives of both kinds.
<!-- TODO add example -->

There is no guarantee that the method with the best silhouette returns all the variation we
are interested in --our goal is, rather, to limit the number of different models we need to
examine from the total number, say 200, to a more manageable amount, like 8.
In the same terms, there is also no guarantee that when we identify something interesting
in a medoid, i.e. an island for a particular usage pattern, all the models in the cluster of
that medoid, and only those models, will share that characteristic. In order to check that,
we can look at random samples (again, of 8 or 9 models) of each of the clusters and
visually compare them to their medoids. This doesn't need to be as thorough an examination as
that of the medoids themselves: it suffices to check if the random sample is not too different
and seems to share the characteristic of interest. [add example]

In general terms, for the characteristics identified in the case studies that make up this
investigation, we can be quite confident that the medoids are representative of the models in
their clusters. However, depending on the concreteness of the phenomena, the variation across
models, the clarity of the visualization and the wishful thinking that might lurk in the
researchers' minds, it might be the case that something found or assessed in a medoid is not
shared by the models in its cluster. The comparison needed with the random sample should be
fast and honest and is strongly recommended: if the medoids are representative, you can see it
in an instant; if they are not, it just takes a bit longer to admit it. It is *not* the same as actually studying and comparing 64 different models.

<!-- TODO add an example of something not working, like hoekig medoid 3? -->

<!--chapter:end:viz_2.Rmd-->

# NephoVis {#nephovis}

In this chapter we will learn how to use the visualization tool to explore
and compare token-level vector space models.

As of this moment, the tool can be found at a
[Github Page](https://qlvl.github.io/NephoVis), that is,
a [Github repository](https://github.com/qlvl/NephoVis) that can be rendered
as a static website [@montes.qlvl_2021].
It obtains its data from a [submodule](https://github.com/qlvl/tokenclouds);
an interested user could clone the repository and just modify the path to the data.

<!-- TODO Decide on a data format: API?? -->

The code for the visualization is written in Javascript, making heavy use of the
[D3.js](https://d3.js) library, which was designed for beautiful web-based
**d**ata-**d**riven visualization. While it is known of its steep learning curve,
it can be useful to think of it in terms of R's vectorized approach: it links
DOM elements to arrays and manipulates them based on the items' properties.

The main rationale and framework for this visualization tool was developed by
Thomas Wielfaert [@wielfaert.etal_2019]; that code can be found
[here](https://github.com/tokenclouds/tokenclouds.github.io/LeTok/).

The current implementation would not exist without this foundational setup. However,
a number of the available features were added later.

The description of the tool will not immediately follow the expected workflow of an user.
Instead, we will start with the lowest level, [Level 3](#level_3),
which represents individual token-level clouds,
zoom out into [Level 2](#level_2), which shows multiple token-level clouds simultaneously,
which will make the most abstract level, [Level 1](#level_1), more clear.
Afterwards (Section \@ref(nephowf)) we will briefly simulate the path an user would take from Level 1 to Level 3.
This perspective was also taken in @montes.heylen_Submitted.
Finally, Section \@ref(wishlist) goes into the Beta features
that require better development and testing, as well as ideas that we might want to
implement in the future. In any case, it must be noted that from July 2019 to
`r Sys.Date()` the user and developer have been the same person, with occasional,
valuable input from other members of the Nephological Semantics project. The
project could certainly benefit from a wider input of suggestions.

<!-- NOTE following Shneiderman‘s Visual Information Seeking Mantra: “Overview first, zoom and filter, then details-on-demand” [-@shneiderman_1996 97], the tool organizes the full range of available information along three levels. -->

<!-- NOTE we might actually want to dedicate a section to the shared features of levels 2 and 3 -->


## Level 3 {#level_3}

Level 3 of the visualization tool shows a zoomable scatterplot in which each glyph represents a token, i.e. an instance of the target lexical item. The name of the model, coding the parameter settings as described in
<!-- FIXME add crossreference -->
, is indicated on the top. It is possible to map colors and shapes to categorical variables (such as sense labels) and sizes to numerical variables (such as number of available context words) and to select tokens with a given value by clicking on the corresponding legend key.

<!-- COMBAK add example -->

## Level 2 {#level_2}

Level 2 of the visualization is *not* a scatterplot matrix, although it looks like one and the code was inspired by Mike Bostock's example. Instead, it is just an array of small plots next to each other and wrap of easier readability.

Each of them represents a different model and the same basic features from Level 3 are available: color, shape and size coding, selection by clicking and brushing, and finding the context by hovering over the tokens.

<!-- COMBAK add example -->

Because they are model-dependent, highlighted context and searching tokens by context word are meaningless in this level, where multiple models are being shown simultaneously. The key contribution of this level, next to the superficial visual comparison of the shape of each plot, is the ability to select one or more tokens in a plot and highlighting them in the rest of the plots as well. Thanks to this functionality, the user can compare the relative position of a group of tokens in a model against that in a different model.

## Level 1 {#level_1}

Level 1 shows one zoomable scatterplot, similar to Level 3, but with each glyph representing one model, instead of one token. As a reminder of the difference, the default shape in Level 1 is a wye (“Y”), while that in the other levels is a circle. The data represented by this scatterplot is not the distance between tokens anymore, but that between models, as described at the beginning of Section 3. This scatterplot aims to represent the similarity between models and allows the user to select the models to inspect according to different criteria. Categorical variables (e.g. whether sentence boundaries are used) can be mapped to colors and shapes, as shown in Figure 5, and numerical variables (e.g. number of tokens in the model) can be mapped to size. A selection of buttons on the left panel, as well as the legends for color and shape, can be used to filter models with a certain parameter setting. Otherwise, models can be selected by clicking on the glyphs that represent them.

## The full story {#nephowf}

The increasing granularity from Level 1 to Level 3 and the manner of access to different functionalities respect the mantra “Overview first, zoom and filter, then details-on-demand” [@shneiderman_1996 97].
The individual plots in Levels 1 and 3 are literally zoomable; and in all cases it is possible to select items (either models, in Level 1, or tokens, in the other two), for more detailed inspection. Finally, a number of features show details on demand, such as the names of the models in Level 1 and the context of the tokens in the other two levels.

In practice, the user will start with Level 1, the scatterplot of models, and can look for structure in the distribution of the parameters on the plot. For example, color coding may reveal that models with nouns, adjectives, verbs and adverbs as first-order context words are very different from those without strong filters for part-of-speech, while the use of sentence boundaries makes little difference. Depending on whether the user wants to compare models similar or different to each other, or which parameters they would like to keep fixed, they will use individual selection or the buttons to choose models for Level 2. In our case, we click on “Select medoids”, which selects the 8 models returned by a partitioning algorithm, which offers a wide range of variation in a manageable number of plots.

In Level 2 the user can already compare the shapes that the models take in their respective plots, the distribution of categories like sense labels, and the number of lost tokens. In addition, the “distance matrix” button offers a heatmap of the pairwise distances between the selected models. In the case of heffen, the restrictive collocational patterns it presents lead to crisp clusters in the visualization and consistent organization across models. However, models with less clearly defined structure may prove harder to understand. In both cases, the brushing and linking functionality highlights whether tokens that are grouped in one model are also grouped in a different model. From here, the user might switch back and forth between Level 2 and Level 3 for a more detailed inspection of the models.

### Examining context words

While it is possible to look at the individual context of each token by hovering over them, it loses track of the larger patterns we want to understand. That is the purpose of the frequency tables in levels 2 and 3.

In any given model, tokens might be close together because they share a context word, and/or because their context words are (based on the second-order modelling) similar to each other. First-order parameters are, by definition, directly responsible for the selection of context words that will be used to model each token. Therefore, when inspecting a model, we might want to know which context word(s) pull certain tokens together, or why tokens that we expect to be together are far apart instead. In other words, if each model offers a different perspective on the distributional behavior of a token, we want to understand what informs said perspective.

In Level 3, individual tokens and groups of them may be selected in different ways. Given such a selection, clicking on “Frequency table” will open a table with one row per context word, a column indicating in how many of the selected tokens it occurs, and more columns with pre-computed information (e.g. PMI values).

The following five columns include pre-computed frequency information, such as the raw co-occurrence frequency and PMI value between the context word and the target based on windows of 10 and 4, and raw frequency in the corpus.
These values can be interesting if we would like to strengthen or weaken filters for a smarter selection of context words. This particular model uses dependency-based information as well as a PMI threshold of 0 to select context words.

In Level 2, while comparing different models, the frequency table takes a different form. There is still one context word per row, but the number of tokens with which it co-occurs will depend on the model.
The columns in this table are all computed by the visualization based on the lists of context words per token per model. Next to the column with the name of the context word, the default table shows one column called "total" and one per model, headed by the corresponding number. The columns for each model match the second column in their Level 3 frequency table: they indicate with how many of the selected tokens the context word co-occurs. The "total" column, in contrast, reveals the union of this selection: with how many of the selected tokens the context word co-occurs in at least one model.

The default table counts how many of the selected tokens co-occur with each of the context words, but it does not use information from other tokens outside the selection, i.e. the cue validity or association strength of the context words for the selected group. For that purpose, a dropdown button in the top left corner of the frequency table offers a small range of transformations, such as odds ratio, Fisher Exact, cue validity, etc. One such option shows the absolute frequencies within and outside the selection, where the green columns count the number of selected tokens that co-occur with each context word, and the white columns count the number of tokens outside of the selection co-occurring with those context words.

## Wishlist {#wishlist}

<!-- TODO add things for future development? -->

<!--chapter:end:viz_4.Rmd-->

# HDBSCAN {#hdbscan}

The visual exploration is extremely useful for a thorough, qualitative description
of the vector space models. However, such an application can also become an obstacle
to a truly systematic, scientific description. I would avoid talking about objectivity:
neither of us, individually, can truly be objective, and we should instead strive for a
humble admission of our own partiality and a fruitful combination of all our partialities.

When describing a cloud ---in particular these clouds that refuse to show clear images,
perfect sense disambiguation, distinct clusters---, how can we ensure that what we
see will be found by other researchers? How can we make our observations, if not
inherently valid, at least reproducible?
This is, after all, the goal we strive for when we embark on quantitative methods.

One tool that can help us systematize our observations, such as the tightness or at least
existEnce of distinct islands on a plot, is
Hierarchical Density-Based Spatial Clustering of Applications (HDBSCAN) [@campello.etal_2013].
This algorithm basically tries to distinguish dense areas separated by less dense areas[^hdbscan]
and allows for noisy data. In other words, unlike traditional hierarchical clustering,
it will not try to cluster all of the points in the dataset, but instead may discard those
that are too far from everything else. Moreover, in comparison to its non hierarchical counterpart,
DBSCAN,
<!-- TODO add citation -->
it requires only one parameter to be set *a priori*, namely $minPts$.

[^hdbscan]: For a friendly description of how the algorithm works, the reader is directed to @mcinnes.etal_2016; for an even friendlier explanation, find the authors on YouTube.

The $minPts$ parameter indicates the minimum size of a dense group of points to be considered
a cluster. An isolated dense group of points for which $n < minPts$ will be considered noise.
In the case studies described here we have fixed $minPts$ to 8, which seems a reasonable
size for the smallest clusters, but it would be interested to look more systematically into
the effect of lowering this threshold. Rising the threshold, on the other hand, would
increase the proportion of points that are considered noise, which is already very high.

Like other clustering algorithms and the dimensionality reduction techniques,
HDBSCAN can take a token-context matrix as input or a distance matrix. We have used
the transformed distance matrix, that is, the same input fed into the t-SNE algorithm,
for the `hdbscan()` function of the {dbscan} R package [@R-dbscan]. The output includes,
among other things, the cluster assignment, with noise points assigned to a cluster 0,
and epsilon values, which can be used as an estimate of density.

HDBSCAN estimates the density of [the area in which we find] a point $a$ by calculating its
core distance $core_{k}(a)$, which is the distance to its $k$ nearest neighbor, $k$ being $minPts - 1$.
Then it recalculates the distance matrix by defining a new distance measure, called
*mutual reachability*, which is defined as the maximum between the distance between
the items $d(a, b)$ and each of their core distances.

$$d_{mreach-k}(a,b) = max(core_{k}(a), core_{k}(b), d(a,b))$$

<!-- TODO Check if that stands like that in the paper and cite appropriate page number -->

Once the algorithm obtains these distances, it uses a single linkage method to create
hierarchical clusters, then using again $minPts$ and other calculations to merge them
into the final selection. The `eps` (epsilon) values returned by `hdbscan()` indicate
the height, in the single linkeage tree, at which each point was joined to a cluster,
and can thus be used as a proxy for its "density".

This is intuitively more clear if we map the clustering solution to colors and the
`eps` value to transparency in a t-SNE plot with perplexity 30. For the most part,
the results converge, which has two main upsides. In the first place, we have independent
confirmation of the structure found by t-SNE, as a different algorithm processing
the same input returns compatible output. Second, insofar the HDBSCAN output matches
our visual assessment, it can systematize it and render it reproducible.

This wonders notwithstanding, the compatibility between the HDBSCAN output and visual examination is not guaranteed. We might find interesting tokens that are discarded as noise, or structure within
a single HDBSCAN cluster. However, it must be noted that this match (or lack thereof)
has been assessed only between `dbscan::hdbscan()` with $minPts = 8$ and
`Rtsne::Rtsne()` with $perplexity = 30$. It would be an interesting avenue for further research to experiment with other combinations, and of course UMAP output.


<!--chapter:end:viz_5.Rmd-->

# Annotation schema {#ann}

<!-- QUESTION Should we include all the definitions and translations and examples here or in an appendix? What about estimated and actual frequencies? -->
For these case studies, we selected 34 Dutch lemmas to annotate and model with token level vector spaces, two of which (*herkennen* and *spoor*) were discarded.
The selection process will be presented in Section \@ref(selection), with a description of the selected items and what we expected their annotation and clouds to look like. A short description of the corpus [QLVLNewsCorpus, @depascale_2019] will follow. The annotation procedure will be the focus of Section \@ref(annotation).
Bachelor students of Linguistics at KU Leuven (later called *annotators*) were recruited and hired to manually annotate samples of the selected lemmas, and while the administrative procedure itself is not of great interest to the project, a number of practical issues will be discussed: the distribution of tokens, the assignment of tasks (in particular, the graphic interface provided) and the processing/analysis of the data.

## Selection of items {#selection}

For this case study 34 Dutch lexical items were selected. We aimed to cover a variety of polysemy phenomena, which will be addressed in the specific sections for each part of speech: section \@ref(nouns) for nouns, section \@ref(adjs) for adjectives, and section \@ref(verbs) for verbs.
By modeling different parts of speech and different kinds of polysemy, we expected to develop more robust generalizations regarding the parameter settings that best model specific phenomena.

The selection procedure mixed some introspection (thinking of words that could be interesting), looking up lexical resources (going through a tentative list of dictionary entries to figure out what kind of polysemy we could expect) and corpus data (surveying a sample of concordances for evidence of the expected polysemy). While dictionaries were an essential resource to sketch the sense labels the annotators would have to choose from, we also adjusted them to a more manageable granularity. The concordances were also crucial to estimate sense distribution and adjust the granularity of the definitions. We didn't want an overwhelmingly frequent sense to affect the annotators judgement, and very infrequent senses might be hard to model or at least to visualise in the 2D representations. Still, we did allow for some complexity and subtlety in some cases.

In a number of cases, the corpus survey (reading a concordance of 40-50 randomly selected instances) invalidated options that intuitively or according to the dictionary definitions would have conformed to our requirements. When judging such a discrepancy, it is important to take into account the composition of the corpus. The topics addressed in newspapers and the terms used to talk about them are certainly not representative of everyday life or the entirety of language.

We also had cases of adjectives that could be used in adverbial form and were not always properly tagged for part-of-speech, so we had to discard them. We made a difference between cases such as _hoopvol_ 'hopeful', which often occurs in predicative contexts with a verb that is not copula (e.g. _ik ben hoopvol gestemd_ 'it makes me hopeful', _hij kijkt hoopvol omhoog_ 'he looks up hopeful(ly)') but still predicates over an entity, and cases such as _gemiddeld_ 'average', which could either predicate over an entity, as in _gemiddelde student_ 'average student', or a predicate, as in _zij eet gemiddeld 3 koekjes elke dag_ 'she eats in average 3 cookies per day'. Sometimes the incorrectly tagged cases (adverbs tagged as adjectives) were infrequent enough to be dismissed, but in some other cases they were so many we had to discard the lemma as candidate. While the only direct consequence is that a certain potentially interesting lemma couldn't be investigated, this also should be taken into account when relying on the part-of-speech tagger in other steps of the workflow.

The next subsections describe the selected nouns, adjectives and verbs, the QLVLNewsCorpus and the sampling method. This includes a general description of the polysemy phenomena and hypotheses, but the specific definitions, examples and translations can be found in Appendix XX.
<!-- TODO add appendix with definitions and proper references -->

<!-- TODO use flextable -->
```{r, include=FALSE}
# filename <- "assets/vector-examples/definitions.tsv"
# definitions <- read_tsv(filename)

show_defs <- function(lemmas) {
  tibble(lemma = lemmas, text = rep("Coming soon", length(lemmas))) %>% flextable() %>% set_caption(caption = "Some caption.")
}
# show_defs <- function(lemmas) {
#   lemma_caption <- str_c("'", sort(lemmas), "'", collapse = ", ")
#   d1 <- definitions %>% 
#     filter(lemma %in% lemmas) %>% 
#     select(code, definition, Dutch, English, freqs) %>% 
#     separate(code, into=c('lemma', 'sense')) %>% 
#     arrange(lemma, sense)
#   kable(select(d1, -lemma),
#         caption = paste("Definitions and examples offered for", lemma_caption)) %>% 
#     kable_styling() %>% 
#     pack_rows(index = deframe(count(d1, lemma))) %>% 
#     column_spec(3, italic = T)
# }
```

### The nouns {#nouns}

The 8 nouns all exhibit homonymy and polysemy in at least one of the homonyms.

Three nouns have one frequent, monosemous homonym and a less frequent, polysemous one: *hoop* 'hope/bunch', *spot* 'ridicule/show or spotlight' and *horde* 'horde/hurdle'. The polysemy phenomena are varied: metaphor for *horde* 'hurdle', metaphor/generalization for *hoop* 'heap' and metonymy for *spot* ('short video/spotlight'). In this latter case, the 'spotlight' sense can further be used literally or metaphorically, but this distinction was not included among the definitions given to the annotators.
<!-- COMBAK Does it show up in the dictionaries? -->

Four nouns have two polysemous homonyms: *schaal* 'scale/dish or shell', *blik* 'look/tin', *stof* 'substance or fabric or topic/dust', *staal* 'steal/sample'.
For *blik*, the frequent homonym ('look') has a concrete sense with a metonymic and a metaphoric extension, while the infrequent one can refer to a material ('tin'), an object made of that material or its content: the distinction is quite clear but might depend on the specificity of the context and be very infrequent.
<!-- NOTE I might want to revise this. I had misintepreted the distinction, but so did the annotators, and the metonymic sense actually didn't exist... but there is a metaphoric sense, that is just super infrequent!! -->
Similarly, *stof* presents one frequent homonym with two concrete, referentially distinct senses ('substance' and 'fabric') andd an abstract one ('topic'), and another with a subtle, context-specificity dependent difference ('dust (in the air)' or 'reducing something to dust, pulverize'). *Schaal* exhibits subtle perspective shifts in one homonym ('scale', e.g. "scale of Celsius" as opposed to "large scale") and refers to different concrete objects with the second ('shell', 'dish', 'scale dish').
Finally, *staal* 'steal' could refer, like *blik* 'tin', to either the material or an object made of it, while the 'sample' homonym is sensitive to construal (sample as evidence, or in general): it's likely to present high confusion and/or a very skewed distribution in both homonyms separately.

- Finally, *spoor* has three homonyms, of which two polysemous: 'footprint or trace/train(line, rail, company)/spur'. This noun was later discarded because it proved too complicated, but the data is available for reanalysis.

<!-- The last noun, *spoor* (Table \@ref(tab:spoor)), exists to spice up the task. It presents at least three distinct homonyms (the one meaning 'spore' was not deemed frequent enough to show up in the definitions). One, in the realm of 'footprint, trace', is presented with four senses: a concrete (foot)print, a generalized application referring to general evidence of someone's (past) presence, one metaphorical and one more specific where the entity to be identified (a substance) is indeed present but in small quantities. The second homonym, relating to trains, could refer to either the vehicle itself, the railroad or the company, and the distinction will depend on the clarity of the context. Finally, the homonym relating to spurs occurs mostly in an idiom; the figurative sense is not explicit in the definition, but the example *is* the idiom, so it should be easy enough to identify. -->

### The adjectives {#adjs}

The selection of adjectives includes 13 lemmas presenting different kinds of polysemy phenomena.

Three adjectives have a metonymic reading: *hoopvol* 'hopeful', *geestig* 'witty' and *hachelijk* 'dangerous/critical'.
For *geestig* and *hoopvol*, one of the senses is anthropocentric, i.e. it's mainly or exclusively applied to people, although such distinction is not made explicit in the definitions of *geestig* but only suggested in the example. The expected frequency of the anthropocentric sense is in both cases much higher than the other one.
In *hachelijk*'s case, the difference is a matter of temporal or telic perspective, so probably harder to distinguish, and it's probably more likely that annotators suggest the second sense as an alternative to the first one (assigning the 'critical' interpretation to something potentially dangerous) than the other way around.

Four adjectives have metaphoric readings: *hoekig* 'angulous/clumsy', *dof* 'dull', *heilzaam* 'healthy/beneficial' and *gekleurd* 'colorful, person of color, tainted'.
*Heilzaam* has two distinctions, somewhere between metaphoric and specialization: one refers to something specifically/literally healthy, and the other one is broader and less concrete.
*Hoekig* and *gekleurd* present three sense distinctions, one of which is particularly concrete and the most frequent and another one explicitly anthropocentric. The third sense distinction has a different quality: synaesthetic for *hoekig* and very much metaphoric for *gekleurd*.
Finally, *dof* has all four kinds of senses: concrete, synaesthetic, anthropocentric and abstract.

Three adjectives present some other form of similarity between the readings: *geldig* 'valid', *hemels* 'heavenly' and *gemeen* 'shared/public/mean/serious'.
*Geldig* 'valid' and *hemels* 'heavenly' offer two options, one restricted to a specific context and one much broader. The relation between the relative frequencies of those senses are inverted: the specific sense of *geldig* is less frequent than the general one, while in *hemels* it's the other way around.
We would expect that the specific sense would not be offered as alternative to the general sense as much as the other way around.
The case of *gemeen* is quite complex, involving a number of rather subtle distinctions. The limits between the first and the second one and between the third and the fifth are hard to establish; the fourth sense seems more clear but if the context isn't specific enough it could be easily confused with the fifth. In addition, the senses are not always mutually exclusive, and a certain instance could very well conflate or be ambiguous between two senses.

Finally, we have three more complex adjectives: *heet* 'hot' for different entities and metaphorically, *grijs* 'gray', with metaphorical and metonymical extensions and *goedkoop* 'cheap', with different entities and metaphorically.

*Heet* 'hot' presents, first, three very concrete senses that differ in perspective: temperatures of different kinds of things.
<!-- IDEA: cite K-tamm? -->
The second half is metaphorical, of which one synaesthetic, one anthropocentric and very specific, and one more abstract and also quite specific. Crucially, there is no exclusive sense tag for idiomatic expressions, which are quite frequent; they are expected to be tagged with the concrete senses (and maybe a comment on their figurative interpretation), but annotators might also use the *geen* tag for those cases.

*Grijs* presents a very frequent, concrete sense, two specific metonymic extensions, one anthropocentric sense, one rather abstract and another very specific metaphor.

*Goedkoop*, on the other hand, presents a modest set of 4 sense distinctions: a concrete, prototypical and frequent sense, two perspectival shifts and a clear metaphor.

### The verbs {#verbs}

For the verbs, we selected a range of combinations of syntactic and semantic variation:

Four verbs are always trasitive, and the sense distinction is related to the objects they can take: *haten* 'hate', *huldigen* 'honor/hold (attitudes, opinions, stances)', *heffen* 'raise', *herroepen* 'annul (a law)/retract (statement)')
*Haten* and *heffen* are probably more easy to distinguish, the former having an anthropocentric distinction (basically, hating people against disliking things, but with possible gray areas in between, depending on how that object is construed) and the latter presenting a rather clear and common metaphor, between physical objects and abstract entities such as taxes being levied.
*Huldigen* and *herroepen* instead have slightly more subtle differences, but the former (between honoring someone/something and holding and opinion) is probably stronger and easier to distinguish than the latter, between retracting a statement or annuling a decree (which again could be interpreted differently depending on how the entity is construed, how prototypical it is).

Two of the verbs can be transitive, with a distinction based on the object, or intransitive: *helpen* 'help', *herstructureren* 'restructure'.
These verbs are quite subtle and might present a lot of confusion, particularly because the intransitive uses are semantically very similar to one of the transitive cases.
For *herstructureren*, one transitive sense and the intransitive one (exemplified with a reflexive...) are more specific, regarding companies and with the connotation that the personnel is being reduced, while the other transitive sense is broader and might be selected in contexts with less specificity. It might also depend on world knowledge (whether the annotators know or can guess that a certain object -or the subject in the intransitive construction- is a company) and how prominent the implication of personnel reduction is.
For *helpen*, the distinction between the transitive uses is rather subtle (the "collaboration" sense is exclusive of animate subjects, but that's not explicit in the definitions), so there might be some disagreement in their annotation, but if the intransitive sense is confused with the transitive ones, it should only be with the first one.

Three verbs can be transitive, with a distinction based on the object, or reflexive: *diskwalificeren* 'disqualify', *herhalen* 'repeat', *herinneren* 'remember/remind'. This category initially included *herkennen* 'recognize' but it was discarded.
For some verbs this opposition can be interpreted as a specific situation where the object and the subject coincide.
This is particularly the case with *diskwalificeren*, where the reflexive argument structure pretty much replicates the transitive senses, in a particular case where someone disqualifies themselves. The possibility to distinguish between the transitive cases, which differ in specificity (sport context against more general, mostly political context), relies instead on the clarity of the context.
<!-- For *herkennen*, the sense distinction between the three transitive uses is quite subtle, and much sharper between transitive and reflexive; -->
for *herhalen*, what could be an object in the transitive senses (but probably wouldn't) is the subject in the reflexive, so the distinction should be very clear, while the transitive uses differ in the kind of objects that they take, with certain prototypical nouns (and the possibility of clauses for the second sense) and maybe some borderline cases.
Finally, *herinneren* shows both a clear distinction between reflexive and transitive uses and a further argument structure distinction between transitive uses, either with or without an *aan* complement (which might be absent in the restricted context).

Two more verbs can be transitive, intransitive or reflexive, with semantic distinctions within the transitive structure: *harden* 'make/become hard, tolerate', *herstellen* 'heal/repair'.
In the case of *harden*, the transitive can be concrete, figurative, or concrete with a different sense and in a specific construction, namely *(niet) te harden*; the intransitive structure is similar to the concrete transitive, but taking its object as subject, and the reflexive is similar to the second transitive. If senses of different argument structures were confused, the intransitive would be with the first and the reflexive with the second. The *(niet) te harden* uses should be easy to isolate, with strong agreement between annotators and high confidence.
For *herstellen*, the transitive structure presents three possible senses: one concrete, one figurative but not presented as such, and one abstract that is very subtly different from the second one. The reflexive is very close to the figurative sense and the intransitive is more specific to concrete healing (rather than repairing) and should not be confused with the others.

Finally, we included a verb with semantic distinctions within both the transitive and the intransitive structures: _haken_.
It was presented with two transitive senses, two intransitive and one transitive/intransitive. The transitive senses can be both concrete and literal and differ in specificity: one sense refers particularly to making somebody trip. Intransitive uses differ in literality and, while both might occur with *blijven*, only the figurative definition mentions it (apparently restricting it). No figurative options are mentioned for the transitive senses, so if they occur annotators might either tag them as transitive concrete, intransitive figurative, or *geen*. Finally, one sense that can occur as transitive or intransitve (ellided object) is that of 'crochet'; it's so specific that it shouldn't be confused with others and would probably have high confidence.

## Expectations for the annotation

<!-- IDEA Maybe move this to a presentation of what the annotation actually looked like? -->
Here a first set of expectations for the annotations of the types has been summarized in six points, but discussion and revision are needed. They are formulated as predictions and followed by suggestions on how to confirm them. Some 'technical' terms are:

- **majority sense**, meaning the sense tag that most of the annotators assigned to a given token.
- **alternative sense/annotation**, meaning a sense tag assigned to a given token, different from the majority sense.
- **(be) confuse(d)**, meaning there is disagreement on the annotation.
<!-- Should still find out how to talk about the asymmetry: some senses can have alternatives or be alternatives or both. -->

<!-- NOTE: It could also be useful to find some literature into this kind of annotations. I'm identifying the anthropocentrism of some sense distinctions as particularly foregrounded, but other than some considerations in the metaphor literature I don't really have theoretical backup. On the other hand, what I've seen of the 'geen' annotations this far partially supports this intuition: some annotators would refuse to assign the *horde 1* tag to cases of *horde* + *KTM's/vrachtwagens/danceprojecten/insecten*. -->

### For all types

1. **Very specific senses will not be confused with more general senses**.
When the majority sense is a very specific one, the only alternative annotation will be *geen*.
Concretely:

- *haken 5* and *haken 3* will not be confused with each other nor with other senses.
- *heet 4* should not be confused with other senses.

<!-- 2. **Literal/concrete senses will be easier to agree upon than figurative senses** -->
<!-- If there are distinct options for literal and figurative senses, when majority sense is concrete/physical, alternative annotations will be rarely figurative. The literal option, instead, will be a more frequent alternative for the figurative majority cases. -->
<!-- Or: cases with more confusion in the annotation are probably not concrete, or just don't have enough context. -->

2. **Metaphor will be easier to identify than metonymy/specialization**
If the metaphoric sense is an option distinct from the concrete/literal one, it won't often be confused with the literal counterpart; annotators will agree it's figurative. For metonymy and specialisation, there will be more disagreement and less confidence.
Concretely:

- *blik 1.1* will be confused with *1.2* more than with *1.3*.
- *grijs 1* will be confused with *2* and *3* more than with *6*.
- *goedkoop 1* will be confused with *2* and *3* more than with *4*
- adjectives with metaphoric distinctions (*hoekig*, *dof*, *gekleurd*, *heilzaam*) will present less confusion than those with metonymic distinctions (*hachelijk*, *hoopvol*, *geestig*)

3. **Anthropocentric senses will be more easily distinguishable**.
If the definition explicitly restricts the application to people, it won't be alternative annotation with other, non anthropo-exclusive senses. Borderline cases, due probably to unspecified context, would have low confidence.
Concretely:

- There should be low confusion in *haten*, or at least low confidence in borderline cases
- *heet 5* should not be confused with others
- *grijs 4* should not be confused with other senses (except maybe 3, which is derived from it)
- *gekleurd 2* should not be confused with others
- *hoekig 3* should not be confused with others
- *dof 3* should not be confused with others
- *hoopvol* will present less confusion than *geestig* and they both will present less confusion than *hachelijk*

### Only for nouns

4. **Homonyms will not be confused with each other**.
When a majority sense is from one homonym, alternative annotations will be of the same homonym or *geen*.

## Only for verbs

The next two predictions overlap, and could explain different verb groupings (sometimes, different argument structure *implies* subject distinction, but which is more prominent?).

5. **Argument structure will be easier to identify than semantic differences.**
Senses that differ in argument structure will not be confused with each other (won't be each other's alternatives) as much as senses with the same argument structure but different kinds of objects. The distinction will be probably easier to make with reflexive than with intransitive cases.
Concretely: in general, transitive senses will be confused with each other but not with senses of a different argument structure (unless that sense is semantically very similar to that transitive sense). Cases that might generate confusion between different argument structures are:

- gral. trans. *haken* and fig. intrans. *haken* in cases of fig. trans. *haken*
- fig. trans. *harden* and refl. *harden*
- fig./abs. trans. *herstellen* and refl. *herstellen*
- spec. trans. *herstructrureren* and intrans. *herstructureren*
- *helpen 1* and intrans. *helpen*


6. **Senses that require different subjects will be easier to identify than senses that require different objects or prepositional arguments**
Senses with different subject restrictions won't be each other alternatives. I'm thinking that subject restrictions are often linked to animacy, while object restriction might be more subtle in these cases.
Concretely: in general, senses of any argument structure will not be confused with each other if one takes (mostly) animate subjects and the other one (mostly) inanimate subjects.

## The corpus and samples {#corpus}

The exploration of these samples of concordances also served for the calculation of the number of tokens we would have annotate. Regardless of the actual frequency of the items in the corpus, we extracted a minimum 240 tokens of each type (thinking of 6 batches of 40 tokens), and raised the amount to 280 if any of the senses had a relative frequency below 20% in the sample, to 320 if it was below 10%, and 360 if there were many senses and therefore some had a low frequency.
<!-- The table below illustrates this distribution. -->

<!-- SWEET: Our plan for the annotation is to distribute the sample of concordances in batches of 40 tokens and hire students to annotate one batch of each of 12 different types. Hiring 40 students, every token is annotated by 2 different students; since some of them are willing to annotate twice as much, some will be annotated by 3. -->

The sample of tokens was selected almost absolutely randomly. First all the instances of each type were extracted from the corpus; then, for each type as many _files_ as tokens we wanted to extract were selected, and from each file I randomly selected one token. Therefore, there are no two instances of the same lemma from the same file in the samples. There were, however, a few duplicates, due to repetition of the same fragment on different dates.

The corpus is a selection of the LeNC and TwNC corpora, which include newspapers articles from Flanders and the Netherlands. This selection, performed by @depascale_2019 with an eye on a lectally balanced corpus, contains 4,614,267 types and 519,996,217 tokens (roughly 520M, 260 from Flanders and 260 from the Netherlands). The articles in the subcorpus were published between 1999 and 2004 in both quality and popular newspapers from both countries.

## Annotation procedure {#annotation}

### Assigning batches to annotators

In October 2019, 48 students from the General Linguistics course of the 2nd year of the Bachelor in Linguistics in KU Leuven were recruited to work as annotators. Each of them was tasked with annotating 40 tokens of each of 12 types (at least three nouns, four adjectives and four verbs, plus one of either of the categories), a total of 480 tokens, for which we expected them to work an average of 10 hours, spread over 6 weeks. Students had the option of subscribing to double the number of tokens (and hours, and pay). Both the types and the sets of tokens were assigned randomly, while keeping in mind the part-of-speech distribution. It was the intention to shuffle the samples of each lemma before splitting them into batches, but something went wrong with the code and they were ordered by source.

The annotation involved three compulsory tasks and one normally optional. For each of the tokens, the annotators had to:

<!-- TODO add and refer appendix -->
1. assign a sense from a predefined set of definitions, as shown in Appendix XX. If none of the senses was satisfactory, they may choose a "None of the above" option;
2. express the confidence of their decision in a Likert scale of 6 values;
3. identify the words of the context that helped them assign a sense, comprising 15 tokens to the left and to the right of the target, disregarding sentence boundaries but respecting those of the article;
4. if they couldn't assign a sense, they had to explain why. If they did assign one, they still had the option of adding extra information or thoughts on the annotation process, but it was not compulsory.

### Annotation tool/interface

Since entering textual information in a spreadsheet can easily lead to typos and inconsistencies and, furthermore, annotating the relevant context words (cues) is challenging in such a tool, a user-friendly visual interface was designed that transforms button-output into a json file with all the information required.

<!-- COMBAK Make the interface available again in its original form??? -->
The [interface](http://montesmariana.github.io/Annotation/) had a menu of types and, for the selected type, two tabs: an overview of the concordance lines and an annotation workspace. In the annotation workspace, they could read each line individually, click on the button corresponding to the sense they wanted to assign, rate their confidence with star rating, click on the words they found useful and enter any other comments. The overview section didn't only let them see the whole set of tokens to analyze, but the target items changed color once they had been annotated and were themselves links to the Annotation tab for their concordance lines.

The interface was rendered as a webpage via Github Pages, but only processed the information: the annotators had to download (and if necessary upload) their progress as a JSON file and eventually send it by mail.

The goals of the interface were twofold. First, it would reduce typos and inconsistencies for values that should be straightforward and present little variation: it was much faster to design the interface than it would have been to check the typos in 480 tokens times 40 annotators. Second, it would make the annotation experience simpler and even more pleasant, letting the annotators focus their energies on the lexicographical task itself rather than in technicalities.

This is particularly evident for the task of selecting the relevant context words. With a spreadsheet, it would be either necessary to have separate rows per context words and annotate all of them (to make sure you are not forgetting any) or make a list of items in a cell of the row of the relevant concordance. That list would need to have something truly identifying of the context word, therefore not the form or the lemma but the position (since a same item could occur more than once in the context and not always have the same relevance), and counting them reliably would be time consuming and prone to errors. Clicking on the words so that the program itself lists the position of the relevant words, also making it visible which words were selected, solves both the easiness and reliability issues.

#### Known issues

The interface did have some issues, the consequences of which affect the output.

One technical issue was a bug in the code of the annotation, for which context words selected by an annotator might be replaced by other context words of the same wordform, but in a previous position. Once that bug was found, the annotators were warned, but not all of them necessarily checked their previous annotation very thoroughly. In any case, this only affects wordforms that occur more than once in the same concordance (which is not very often) and could be cleaned with some reasoning.

Another issue had to do with the format of the corpus, and could have been dealt with better. On the one hand, different sentences in the concordance were indiciated with a `<sentence></sentence>`{.html} but had no impact in the rendering of the concordance, they were just replaced by empty spaces. They could've been replaced by `<p></p>`{.html} tags. On the other hand, at some point of the corpus processing (before we had access to it), someone must have replaced all *&* with *and*, so that HTML entities like *\&quot;*, which would've translated into a quotation mark *"*, are rendered as *andquot;*, which was extremely confusing for the annotators, especially in already complicated concordances full of them. This issue was identified too late (and in any case, if the corpus already reads it as *andquot;* instead of *&quot;*, the confusion is for both).

<!-- #### Output -->

<!-- <!-- NOTE This should be rephrased, and maybe the data should be included in the tokenclouds repository adding this as documentation. --> -->

<!-- From each student, we received a file in json format where both their username and annotations were recorded. After checking that all tokens were annotated in all required variables (and that all 'geen' cases had comments), the results were turned into tables and merged. After a number of attempts, we have two main tables: -->

<!-- First, a [register of tokens](C:/Users/u0118974/Box Sync/Nederlands wolken/Output/Merges/token_annotation.tsv), where each row is a token (id: **token_id**) and the variables are only: -->

<!-- - **type**: the type that token belongs to; -->
<!-- - **batch**: the batch (set of 40 tokens) that token belongs to, named with the type plus a number; -->
<!-- - **majority_sense**: the majority sense, or the tag that most of the annotators agreed on; -->
<!--     - When the tag was 'geen' ("none of the above"), I classified the comments into kinds of justification that became alternative senses: *between* (doubt between given alternatives), *not_listed* (suggestion of a different alternative from those given), *unclear* (insufficient or confusing context, unknown words) and *wrong_lemma* (issues with lemmatization, part-of-speech tagging or even spelling, so that the concordance does not really correspond to the wanted target); -->
<!--     - when there was no agreement between two annotators (or three in the case of four annotations), the majority sense becomes *no_agreement*; -->
<!-- - **majority_agree**: the proportion (0-1) of annotators that voted for the majority sense; -->
<!-- - **majority_conf**: the mean confidence (standardized by username-type combination) of the agreeing annotations; -->
<!-- - **mean_conf**: the mean confidence (standardized by username-type combination) of all the annotations of the token. -->

<!-- Second, a [register of annotations](C:/Users/u0118974/Box Sync/Nederlands wolken/Output/Merges/long_tokens.tsv), where each row is a token-annotation combination (there is no row-id, but **token_id** identifies the tokens and **annotator** identifies the annotations as *ann_1*, *ann_2*, *ann_3* or *ann_4*). For each row, the following variables are registered: -->

<!-- - **type**: the type that token belongs to; -->
<!-- - **batch**: the batch (set of 40 tokens) that token belongs to, named with the type plus a number; -->
<!-- - **username**: the name of the annotator (easier for me to keep track of, but nothing to publish); -->
<!-- - **code**: codename of the annotator, combining the number of 'student' (set of batches of 12 different types) and number of annotator (matching the **annotator** variable); -->
<!-- - **annotators**: number of annotators who tagged the given token (normally 3). Not a very important variable; -->
<!-- - **original_sense**: the sense tag applied by the annotator; either a sense represented by the name of the type and a number, or *geen* for "none of the above"; -->
<!-- - **confidence**: the confidence assigned by the annotator, with minimum 0 (1 star) and maximum 5 (6 stars); -->
<!-- - **conf_std**: standardized confidence values, grouping by **username** and **type**[^conf_std]; -->
<!-- - **comments**: the comments given by the annnotators, which are *normally* empty, unless the sense is *geen* (some annotators also commented tokens where they assigned an actual sense tag); -->
<!-- - **geen_reason**: a classification of the comments given by the annotators, grouping them in the categories described before (*between*, *other_sense*, *unclear* and *wrong_lemma*); -->
<!-- - **sense**: the modified sense annotation, replacing the *geen* annotations of **original_sense** with the categories from **geen_reason**; -->
<!-- - **annotations**: the number of different values of **sense** for that given token (so that 1 equals to total agreement); -->
<!-- - **agree_nr**: the proportion (0-1) of annotators that assigned the **sense** of a given row to the **token_id** of that row; -->
<!-- - **agree_fct**: a categorical version of **agree_nr**, where *full* represents full agreement between annotators, *none* no agreement, *half* that two out of four agreed, *minority* that the current row has a disgreeing annotation of the token and *majority* if this annotation agrees with the majority for that token. -->
<!-- - There is no column with cues, but they are stored _somewhere_ so I can retrieve them when I want to start working on them. -->

<!-- [^conf_std]:  There is a lot of variation across annotators in how they used their confidence (even within the same set of tokens). There is also a wide variation depending on type, for each username. It also makes sense to use this criterion. -->

<!--chapter:end:viz_6.Rmd-->

# (PART) The language of clouds {-}

# The nature of clouds {#shapes}

## Introduction

Clouds come in many shapes. Like the cotton-like masses of droplets we see in our skies, the clouds of word occurrences generated by token-level distributional models may take different forms, depending on their density, their size and their distinctiveness. "Meaning is use", "Differences in usage correlate with differences in meaning", "You shall know a word by the company it keeps"^[Attributed to Ludwig Wittgenstein, Zellig Harris and J. R. Firth respectively, as discussed previously.] and other such catchy slogans sound intuitively accurate, but they hide a wealth of complexity and variation. Like meaning, context is far from orderly, and a myriad of words with different characteristics interact to generate the variation we see in these clouds.

In this chapter, we will try to make sense of the nephological topology, i.e. the variety of shapes that these clouds may take. For this purpose, we will classify `r sc("hdbscan")` clouds mapped to t-`r sc("sne")` representations in a way that can help us understand what we see when we see a cloud. The starting point is the shape that a researcher sees in the scatterplot, which will be visually mapped to meteorological cloud types and further described in technical terms.

In Section \@ref(theo2-rationale) we will discuss the rationale behind this particular classification and the tools used to operationalise these decisions. Section \@ref(cloud-patterns) will explore the technicalities that characterise the different types of clouds and their distribution across the lemmas. A more detailed description of each cloud type and their technical interpretations follows in Section \@ref(cloud-types), and finally we summarise the chapter in Section \@ref(theo2-summary).

## Rationale of the classification {#theo2-rationale}

In this topological classification, the term *cloud* shall refer to an `r sc("hdbscan")` cluster of a token-level model. The model itself, like a picture of the sky, might present multiple clouds of different types.

<!-- TODO add something about the relationship between `r sc("hdbscan")` and t-`r sc("sne")`, or how different parameters would lead to different results? -->

The distances between tokens result from a complex interplay of properties of their context words. Furthermore, both clustering and visualisation add a layer of interpretation meant to find patterns of similar tokens that are different to the other tokens. Sometimes those patterns are easy to find, which leads to very nice, interpretable clouds; sometimes they are very hard to find, resulting in lots of noise and/or less defined clouds.

The factors that interact to produce a group of similar tokens include the frequency of the context words, whether they co-occur within the sample and their type-level similarity.

<!-- TODO summarize the next to paragraphs, making a reference to the following chapter. -->
Clouds can be dominated by one context word that co-occurs with all the tokens in the cluster and does not occur outside the cluster. This perfect precision and recall of a context word for a cluster is not so common, but slightly perfect situations are quite common.

The cluster could instead be characterized by a group of context words that are similar at the type-level. They can be paradigmatic alternatives of each other or syntagmatic co-occurrences. The more similar two paradigmatic alternatives are at the type-level, the more they approximate one unique context word with their combined frequencies. In that sense, a cluster of 20 tokens co-occurring with the same context word may look the same as a cluster of 20 tokens co-occurring with 20 very similar context words.

Syntagmatic co-occurrences may enhance the similarity if their vectors are similar, or may be cause for splits when they are different. For example, 20 tokens co-occurring with the same context word, of which 10 occur with a very different context word and 10 don't, will probably be split in two clouds. This is hard interpret from mere recall and precision values of a single cluster, since a key aspect is the complementarity between clouds, i.e. both clouds in this example would share a representative word and be distinguished by whether they also include the other representative word or not.

That covers the paradigmatic and syntagmatic relationship between context words and their corresponding similarity at type-level. However, whether they will actually create different clouds, either at `r sc("hdbscan")` or, visually, on t-`r sc("sne")`, also depends on the frequency with which they co-occur and on the characteristics of the other context words. The *relative* frequencies and similarities plays a strong role in actually generating patterns. It's very complicated and I don't really know how to model it yet, but what I do know is that, whenever we interpret these clouds, technically or semantically, **all these things play a role**. Most often, a cluster dominated by one context word or dominated by three very similar context words are visually indistinguishable; whether two clouds have different main context words or they share one context word while they don't share another is also visually indistinguishable.

The clouds have been classified in in five categories plus a cross-cutting category. The classification is based on a combination of t-`r sc("sne")` visualization (perplexity 30) and `r sc("hdbscan")` (minPts = 8) and it would probably be different if other visualization techniques or clustering algorithms are used.

The main categories, which will be described in more detailed in Section \@ref(cloud-types), are, in descending degree of clarity:

- Cumulus: the most defined clouds;
- Stratocumulus: a slightly looser definition of still decent clouds;
- Cirrus: the weakest, smallest, less defined clouds;
- Cumulonimbus: massive clouds;
- Cirrostratus: the `r sc("hdbscan")` noise (so, not a cluster).

The inspiration for the names of the types of clouds is visual: the shapes that we would find when mapping the `r sc("hdbscan")` clusters to the t-`r sc("sne")` solution emulates the shapes of different types of meteorological clouds. Admittedly, for those who are familiar with meteorological types of clouds, this is not necessarily the most salient feature. Altitude, temperature and composition, instead, are more relevant in categorising clouds. As we will see in Section \@ref(cloud-patterns), it could be possible to map the (ranges of) $\varepsilon$ (epsilon) values to the altitudes of the clouds, via yet another metaphor, the one underlying the `r sc("hdbscan")` simplified trees, but its dubious how much it actually helps understanding. The metaphor should not be taken too far; that is not the goal of this exercise.

Technical criteria were defined in order to automatically categorise a large number of clusters. They are the result of both theoretical reasoning and trial and error, so that the final classification matches the intuitions derived from visual inspection. In other words: this classification should help us understand what we are looking at based on the shapes we identify, but technical, objective criteria were designed that approximate these intuitions for a larger scale analysis.
These criteria make use of (i) the noise category from `r sc("hdbscan")`, (ii) the relative size of the cluster, (iii) separability indices, (iv) cosine distances between the tokens and (v) $\varepsilon$ values.

Criteria (i) and (ii) are straightforward. Criterion (iv) refers to two measures developed within the `semvar` package [@R-semvar; @speelman.heylen_2017], `kNN` and `SIL`: they assess how well the items are clustered based on a distance matrix. In this case, we are looking for the match between the `r sc("hdbscan")` clusters, which take the role of classes, and the euclidean distances within the t-`r sc("tsne")` plot. Let's see how they work.

The first measure, `kNN`, is a separability index developed by @R-semvar based on the proportion of "same class items" among the $k$ nearest neighbours of an item. It answers the following question: looking at the `r sc("hdbscan")` clusters mapped to the t-`r sc("tsne")` plot: how pure are the clusters? Do they form tight groups of the same colour, or do they overlap (maybe with noise tokens)? Recall that this has no bearing on the semantic composition of the cluster: instead, it refers to the visual homogeneity of the cluster as mapped to the plot.

For our purposes, it makes sense to set $k$ to 8, the minimum number of tokens that a cluster would have based on the current `r sc("hdbscan")` parameters. As a result, for each token $x$, if the 8 tokens closest to $x$ *in the t-`r sc("tsne")` plot* belong to the same class (`r sc("hdbscan")` cluster) as $x$, then `kNN` = 1, and if none of them do, then `kNN` = 0, regardless of what other class(es) the other items belong to. When the proportions are mixed, the ranking of the neighbours plays a role: if the tokens of the same class as $x$ are closer to $x$, `kNN` will be higher; if tokens of a different class are closer, `kNN` will be lower. The `kNN` value for a cluster is the mean of the `kNN` assigned to each of its members. A high `kNN` means that there are few instances of a different class mixed in among the tokens of the clusters: in other words, the cloud is quite compact and pure.

The problem with `kNN` is that it is biased towards large clusters. The larger the cluster is, the higher the proportion of tokens that is entirely surrounded by items of the same cluster. For other purposes, this might not be a problem, but if the goal is to identify certain kinds of cloud shapes, clusters with the same `kNN` and different sizes will have very different properties. In order to counteract this bias, we include a `SIL` threshold.
`SIL`, or silhouette, is a popular measure of cluster quality that takes into account the distances between the members of a cluster and to the members outside that cluster [@rousseeuw_1987]. When the tokens inside a cluster are much closer to each other than to tokens outside the cluster, `SIL` is highest, with an upper bound of 1. If the cluster is very spread out and/or other clusters are very close by, e.g. because they overlap, `SIL` will go down. Thus, a combination of high `kNN` and high `SIL` results in more compact, homogeneous, isolated clusters.

Next to `kNN` and `SIL`, we also make use of the distances between the tokens belonging to the same cluster, and of the $\varepsilon$ values. The former refer to the original cosine distances between the tokens: the lower they are, the more similar the tokens are to each other. These may be different from the euclidean distances based on the t-`r sc("tsne")` plot.

Finally, $\varepsilon$ values are extracted from the `r sc("hdbscan")` clustering and were explained in Chapter \@(ref:hdbscan). The lower the $\varepsilon$ value, the denser the are of the token: the smaller the area covered by its nearest neighbours. Noise tokens have typically the highest $\varepsilon$ values: they are very disperse, and therefore the radius required to find 8 near neighbours is larger. The tokens of a cluster might have a variety of $\varepsilon$ values: the lower the $\varepsilon$, the closer it is to the core, i.e. the denser area of the cluster. To be clear, we are not making any claims about the technical or semantic interpretation of $\varepsilon$ right now. A brief discussion on this is given in Chapter \@ref(semantic-interpretation). Instead, the utility of these values lies in their straightforward mapping to the visual effects of the plot. If the $\varepsilon$ values of a clustered token are close to those of noise tokens, the cluster is, in a way, submerged in noise: `r sc("hdbscan")` is finding a coherence in them that t-`r sc("tsne")` does not. On the contrary, if the $\varepsilon$ values are much lower, the token stands out.

In practice, the steps are as follows; each step works on the remaining clouds after applying the previous steps.

1. The noise is categorised as a Cirrostratus cloud.
2. The clouds that cover at least 50% of the modelled tokens (including noise) are Cumulonimbus clouds.
3. The clouds with best definition are Cumulus clouds. They must at least have a `kNN` $\ge$ 0.75, `SIL` $\ge$ 0.5 and mean internal distance $\le$ 0.5. In addition, less than 10% of the tokens in the cluster may have a higher $\varepsilon$ than the lowest noise $\varepsilon$, or the noise in the model must cover less than 10% of the tokens.
4. The smaller clouds, i.e covering less than 10% of the modelled tokens, if 75% of the model is noise or `kNN` < 0.7, are Cirrus clouds.
5. The most decent of the remaining clouds are Stratocumulus. They must have `kNN` $\ge$ 0.7, `SIL` $\ge$ 0.5 or mean distance $\le$ 0.2. In addition, either more than half of the tokens have lower $\varepsilon$ than the noise tokens or no more than 10% of the modelled tokens are noise.
6. The remaining clouds are considered Cirrus.

In addition, the category of Hail groups the clouds with at least 8 identical tokens; these can belong to any of the other classes.

Table \@ref(tab:cloudFreqs) indicates the number of clouds, either in medoid models or across all models, belonging to each of the categories. Recall that most models have a Cirrostratus cloud, i.e. noise tokens, and that they may have up to one Cumulonimbus clouds, i.e. massive clouds. The rest of the clouds may occur more than once in the same model. The number of clouds that also belongs to the Hail category is given in parentheses.

```{r, cloudFreqs, anchor = "Table"}
countCTypes(medoid_data) %>% 
  full_join(countCTypes(cloud_data),
            by = "cloud_type",
            suffix = c("_medoids", "_all")) %>% 
  kable(
    col.names = c("Cloud type", "clouds in Medoids", "All clouds"),
    caption = "Number of clouds of each type per medoid or model in general; in parenthesis, the number of Hail clouds is specified.") %>% 
  kable_paper()
```

## Types of clouds {#cloud-types}

In this section, the different cloud shapes will be described in some detail. Their general look on a plot will be compared to pictures of meteorological clouds and we will offer a technical interpretation for them.



### Cumulus clouds

In meteorological terms, Cumulus clouds look puffy: they are our prototypical and ideal images of clouds. As token-level clusters, they also correspond to our ideal images of clusters: mostly roundish, visually salient because of their density and isolation. We would be able to find them even without colour-coding: both t-`r sc("tsne")` and `r sc("hdbscan")` agree that those tokens belong together.
In Figure \@ref(fig:cumulus), the four rightmost clusters, in green, light blue, yellow and blue, are Cumulus; the rest are Stratocumulus.

(ref:cumulus) Example of Cumulus cloud: inspiration on the left, plot example on the right. Picture Glg, edited by User:drini - photo taken by Glg, CC BY-SA 2.0 de, https://commons.wikimedia.org/w/index.php?curid=3443830.

```{r, cumulus, out.width = '50%', anchor = 'Figure', fig.show='hold', fig.cap='(ref:cumulus)'}
cum_Fscores <- cloud_data %>% split(f = .$cloud_type == "Cumulus") %>% map(pull, topFscore) %>% map(summary) %>% map(round, 3)
cum_pmi4 <- cloud_data %>% split(f = .$cloud_type == "Cumulus") %>% map(pull, pmi4) %>% map(summary) %>% map(round, 3)

cum_clouds <- medoid_data %>%
  filter(cloud_type == "Cumulus", lemma %in% c("schaal", "hachelijk", "stof")) %>%
  select(lemma, model, cluster, top_Cw, maincat)
sample_cum <- function(i) {
  sampleCtxt(cum_clouds$lemma[[i]],
             mnum = cum_clouds$model[[i]],
             clusn = cum_clouds$cluster[[i]]
             )
}

cloud_foto("cumulus")
plotBasic(sampleCloud("Cumulus"))
```

Cumulus clouds are defined by a number of different measures with strict values, after excluding Cirrostratus (noise) and Cumulonimbus (massive clouds).
First, the clusters need to have both `kNN` $\ge$ 0.75 and `SIL` $\ge$ 0.5 for the t-`r sc("sne")` solutions; as well as a mean pairwise cosine distance between the tokens of 0.5 or lower. The combination of these three strict thresholds ensures quite pure (they don't visually overlap with other clusters or noise), compact, isolated clusters. The final requirement makes sure that the cloud stands out against the noise. One of the ways it can achieve this is by having an $\varepsilon$ lower than the minimum noise $\varepsilon$ in at least 90% of the tokens: at least 9 out of 10 tokens stand out. However, in models without any noise or with very little, noise $\varepsilon$ values might be particularly high, so this threshold is not applied in models with less than 10% noise.

Most of these clouds are characterized by one context word with high precision and recall for the cluster. Specifically, the maximum $F$ of a context word for these clouds tends to be significantly higher than for any other type of cloud: 75% of these clouds have a context word with an $F$ of
`r cum_Fscores[["TRUE"]][["1st Qu."]]` or higher, while in 75% of the rest of the clouds the context word with highest $F$ is not higher than `r cum_Fscores[["FALSE"]][["3rd Qu."]]`. These top context words also tend to have high `r sc("pmi")`, but some may even have negative `r sc("pmi")`.

The lemmas with a higher proportion of Cumulus clouds than expected --- and, in contrast, a lower proportion of Cirrus clouds --- are *heffen* 'to levy/to lift', *hachelijk* 'dangerous/critical', *schaal* 'scale/dish', *gemeen* 'common/shared/mean' and *stof* 'substance/fabric/topic/dust'. They are all cases with strong collocational patterns of the kind discussed in Section \@ref(collocation). Lemmas that repel Cumulus clouds, on the other hand, such as *haten* 'to hate', *geestig* 'witty', *gekleurd* 'coloured' and *hoekig* 'angular', lack such collocational patterns and instead form more uniform, fuzzy pictures.

### Stratocumulus clouds

In meteorological terms, Stratus clouds are flat or smooth clouds: Stratocumulus clouds are then a flatter, less compact version of the Cumulus clouds discussed above. In Figure \@ref(fig:stratocumulus), all three clouds are Stratocumulus: from the large, disperse light blue cloud, to the more stretched orange and and the more compact green cloud that lost three points in the bottom right.

(ref:stratocumulus) Example of Stratocumulus cloud: inspiration on the left, plot example on the right. Picture by Joydeep - Own work, CC BY-SA 3.0, https://commons.wikimedia.org/w/index.php?curid=20357040.

```{r, stratocumulus, out.width = '50%', anchor = 'Figure', fig.show='hold', fig.cap='(ref:stratocumulus)'}
sizes <- cloud_data %>% split(f = .$cloud_type) %>% map(pull, size) %>% map(summary)
cloud_foto("stratocumulus")
plotBasic(sampleCloud("Stratocumulus"))
```

The definition of Stratocumulus clouds takes a number of different measures and applies less strict thresholds than for Cumulus clouds. First the Cirrostratus, Cumulonimbus and Cumulus must classified, and the smallest clouds, either in noisy models or without high `kNN`, must be reserved for Cirrus. On the remaining clouds we apply two filters. First, they must either have `kNN` $\ge$ 0.7, `SIL` $\ge$ 0.5 or mean pairwise cosine distance $\le$ 0.2. Second, either more than half the tokens have an $\varepsilon$ value below the minimum noise $\varepsilon$ value or the percentage of noise tokens in the model is lower than 10%.

Stratocumulus clouds are generally large: while 75% of either Cumulus or Cirrus have
`r sizes$Cirrus[['3rd Qu.']]` tokens or fewer, half the Stratocumulus have `r sizes$Stratocumulus[['Median']]` or more.
However, in comparison to Cirrus they tend to have lower type-token ratio of context words^[Either counting all context words in the cluster or just those that are enough to cover the tokens, in descending order of $F$.] and higher $F$ values of their representative context words. In addition, the mean cosine distance between the tokens tend to be comparable to that in Cirrus clouds, in spite of the difference in size: in other words, they are larger but more compact and more clearly defined.

While lemmas that prefer Cumulus clouds tend to avoid Cirrus clouds and vice versa, the relationship with Stratocumulus is not so straightforward. A preference for Cumulus tends to go hand in hand with a preference for Stratocumulus, as in the case of *heffen* 'to levy/to lift', but that is not necessarily the case. Both *gemeen* 'common/shared/mean' and *stof* 'substance/dust...' prefer Cumulus against either Cirrus or Stratocumulus, and *haten* 'to hate', which prefers Cirrus to Cumulus, does have a slight preference for Stratocumulus too. One lemma that prefers Stratocumulus over anything else is *heilzaam* 'healthy/beneficial', which is described in Section \@ref(heilzaam): even though its clusters tend to be dominated by clear collocates of the target, they are semantically heterogeneous.

### Cirrus

From a meteorological perspective, Cirrus clouds are high up and wispy. In these plots, the description translate to typically small, disperse clouds that we might not be able to isolate without the help of `r sc("hdbscan")`. In Figure \@ref(fig:cirrus), both clouds belong to this category. 

(ref:cirrus) Example of Cirrus cloud: inspiration on the left, plot example on the right. Picture by Dmitry Makeev - Own work, CC BY-SA 4.0, https://commons.wikimedia.org/w/index.php?curid=85153684.

```{r, cirrus, out.width = '50%', anchor = 'Figure', fig.show='hold', fig.cap='(ref:cirrus)'}
cloud_foto("cirrus")
plotBasic(sampleCloud("Cirrus"))
```

Cirrus clouds are defined as small clouds in noisy models or with a low `kNN`, i.e. substantial overlap between the cloud and other clusters or noise tokens, as seen in the plot, as well as the remainder of the clouds after defining the other four categories.
They are generally small, like Cumulus clouds: in a few cases they cover more than 100 points, in which case they could be considered Stratocumulus if their `SIL` was higher and either their `kNN` or the percentage of tokens below noise was higher too. In any case, 75% of these clouds have `r sizes$Cirrus[['3rd Qu.']]` tokens or fewer.
In spite of their size, they have a high type-token ratio of context words and a low top $F$, even compared to larger Stratocumulus: in other words, they tend not to be represented by single powerful collocates, and instead their tokens co-occur with many different words.

The weakness of their patterns should be seen as a tendency, rather than a law. They are more likely than Cumulus clouds to be semantically heterogeneous and hard to interpret, but it is not necessarily the case. In some lemmas with tendency to a more uniform internal structure, Cirrus clouds may group the few patterns that emerge at all.
Lemmas that prefer Cirrus clouds, such as *geestig* 'witty', *gekleurd* 'coloured', *hoekig* 'angular' and *haten* 'to hate', are precisely characterised by uniform-looking plots, low frequency collocates and weak patterns overall.

### Cumulonimbus

```{r, cnsummary, include = FALSE}
cn_summary <- cloud_data %>% filter(cloud_type == "Cumulonimbus") %>% pull(rel_size) %>% summary %>% round(3)
with_cn <- cloud_data %>%
  filter(str_detect(maincat, "Cumulonimbus"), !cloud_type %in% c("Cumulonimbus", "Cirrostratus")) %>%
  count(model, maincat, name = "n_clouds") %>%
  count(maincat, n_clouds, name = "n_models") %>%
  mutate(n_models = n_models/sum(n_models)) %>% arrange(desc(n_models))
by_number <- with_cn %>% group_by(n_clouds) %>% summarize(n_models = sum(n_models)) %>% deframe %>% round(3)
all_with_cumulus <- round(sum(pull(filter(with_cn, maincat == "Cumulonimbus-Cumulus"), n_models)), 3)
```

In the physical world, Cumulonimbus clouds are puffy (as indicated by *Cumulo-*) and bring rain and storm. They are massive, towering clouds that may reach as low as Cumulus clouds and as high as Cirrus clouds. In our models, the Cumulonimbus category (the largest cluster in Figure \@ref(fig:cumulonimbus)) is the least frequent, but when it does occur, it dominates the plot.
Almost all lemmas have at least one such medoid, and a few have two or up to four medoids.

Cumulonimbus clouds are minimally defined as clouds that cover at least 50% of the modelled tokens, including those discarded as noise. In practice, half of them cover at least `r cn_summary[['1st Qu.']]*100`% or more, reaching as much as `r cn_summary[['Max.']]*100`%.
Next to them, we typically have one more cluster (in `r sum(by_number['1'])*100`% of the cases);
occasionally we may have two (`r by_number['2']*100`%) or even up to 5.
The smaller cluster next to the massive Cumulonimbus tends to be a Cumulus, but all combinations are attested.

(ref:cumulonimbus) Example of Cumulonimbus cloud: inspiration on the left, plot example on the right. Picture by fir0002flagstaffotos [at] gmail.comCanon 20D + Canon 17-40mm f/4 L, GFDL 1.2, https://commons.wikimedia.org/w/index.php?curid=887553.

```{r, cumulonimbus, out.width = '50%', anchor = 'Figure', fig.show='hold', fig.cap='(ref:cumulonimbus)'}
cloud_foto("cumulonimbus")
plotBasic(sampleCloud("Cumulonimbus"))
cn_clouds <- medoid_data %>% filter(str_detect(maincat, "Cumulonimbus"), cloud_type == "Cumulus") %>%
  select(lemma, model, cluster, cloud_type, top_Cw)
cn_medoids <- cn_clouds %>% select(lemma, model) %>% distinct
typical_cn <- medoid_data %>% filter(str_detect(maincat, "Cumulonimbus"), cloud_type == "Cumulus",
                                    lemma %in% c("gemeen", "stof", "schaal", "spot")) %>%
  select(lemma, model, cluster, top_Cw)
sample_cn <- function(i) {
  sampleCtxt(cn_clouds$lemma[[i]],
             mnum = cn_clouds$model[[i]],
             cw = cn_clouds$top_Cw[[i]])
}
sample_topcws <- function(i, n = 5) {
  d[[cn_clouds$lemma[[i]]]]$medoidCoords[[cn_clouds$model[[i]]]]$cws %>%
    filter(cluster == cn_clouds$cluster[[i]]) %>% arrange(desc(precision)) %>%
    head(n) %>% pull(cw) %>%
    str_split(pattern = "/") %>% map_chr(1) %>%
    paste0("*", ., "*", collapse = ", ")
}
```

The most typical situation in which we encounter a Cumulonimbus cloud is when a small group of tokens is very tight, but very different from everything else, and the rest of the tokens are not distinctive enough to form different clusters. Most of these tokens are then grouped in this large, normally disperse Cumulonimbus clouds, which may seem to have inner structure captured by t-`r sc("sne")` but not by `r sc("hdbscan")`. The small group of tokens may be brought together by a set of similar context words (see Section \@ref(semantic-preference)), but most typically they represent an idiomati expression.

In fact, the lemmas with a strong preference for this format, *gemeen* 'common/shared/mean', *stof* 'substance/dust...' and *schaal* 'scale/dish', have a very clear idiomatic expression that tends to form the small Cumulus clouds, so that the differences among the rest of the tokens are smoothed. In contrast, lemmas that barely have any Cumulonimbus clouds, such as *herroepen* 'to retract/to annul', *hoekig* 'angular', *diskwalificeren* 'to disqualify' and *horde* 'horde/hurdle', for which at most 5% of the models have Cumulonimbus clouds, lack such a strong pattern and have, instead, groups with similar frequencies and mutual differences.

In the case of *gemeen*, the tight cloud represents the expression *grootste gemene deler* 'greatest common divisor': both *groot* 'big, great' and *deler* 'divisor' co-occur with a large number of tokens but are, at the type-level, different from each other and to everything else. As a result, the token-level vector of the *grootste gemene deler* 'greatest common divisor' tokens will be very similar to other tokens instantiating the same expression, and very different from everything else. Similarly, the most frequent responsible of this picture in the case of *stof* is *stof doen opwaaien* 'lit. to lift up the dust', an idiomatic expression referring to controversial actions and situations. *Schaal*, on the other hand, has two main idiomatic contexts that generate Cumulonimbus clouds, discussed in Section \@ref(schaal).

The rest of the tokens, i.e. the Cumulonimbus cloud itself, is not defined by either a strong dominating context word or group of similar context words, but instead is defined against this stronger, small cloud. Cumulonimbus clouds are not huge clouds of similar tokens, but a mass of tokens that is not structured enough in opposition to the distinctive small cloud that is next to it. It may have dense areas inside the cluster, but they are not semantically linked to each other. The reason they are a cluster is not because the tokens are similar to each other, as much as because the tokens in the small partner are similar to each other and different from everything else. The mean distance between tokens in a Cumulonimbus cloud is typically very large, sometimes as large as within Cirrostratus (noise), and significantly larger than within other kinds of clouds (although the few examples of Cirrus and Stratocumulus co-occurring with Cumulonimbus also have large mean distances).

For a discussion on the semantic interpretation of these clouds, see Section \@(openchoice).

### Cirrostratus

```{r, countnoise, include = FALSE}
only_noise <- nrow(filter(cloud_data, maincat == "Cirrostratus"))
n_models <- length(unique(cloud_data$model))
count_noise <- str_glue("{only_noise} ({round(only_noise/n_models)*100}%)")
```

In meteorological terms, Cirrostratus clouds are high (*Cirro-*) flat and smooth (*-stratus*) clouds. For our purposes, they just indicate the noise tokens. It lies in the background of (almost) all our clouds and constitutes 100% of two of our medoids. Considering the entirety of our models, `r count_noise` of them are fully Cirrostratus clouds (Figure \@ref(fig:cirrostratus)).

(ref:cirrostratus) Example of Cirrostratus cloud: inspiration on the left, model with 100% noise on the right. CC BY-SA 3.0, https://commons.wikimedia.org/w/index.php?curid=100381.

```{r, cirrostratus, out.width = '50%', anchor = 'Figure', fig.show='hold', fig.cap='(ref:cirrostratus)'}
cloud_foto("cirrostratus")
plotBasic(sampleCloud("Cirrostratus"))
```

It might be interesting to cluster the subset of tokens that make up these clouds, at least for some lemmas, but that is not pursued in these studies. It would require a deeper investigation of how `r sc("hdbscan")` works with these models, why are tokens sometimes not clustered and how it interacts with parameters like `minPts`. In these studies, we will not try to semantically interpret these clouds, but they are always present and affect how other clouds are defined. In any case, we can say that the same models that tend to prefer Cirrus clouds also tend to have more noise, or more models with only noise.

### Hail

```{r, count-altocumulus, include = FALSE}
prop_ac <- filter(cloud_data, cloud_type != "Cirrostratus") %>% pull(Hail) %>% mean %>% round(2)
### prop_meds <- filter(cloud_data, cloud_type != "Cirrostratus", medoid == model) %>% mutate(isAlt = Altocumulus == "Altocumulus") %>% pull(isAlt) %>% mean %>% round(2)
models_ac <- filter(cloud_data, Hail) %>% pull(model) %>% unique %>% length
```

The final, orthogonal category can apply to any cloud, and might even describe a section of a cloud rather than the full cloud. It responds to a different criterion, to highlight the occasional phenomenon of superdense clusters. In Figure \@ref(fig:hail), three of the clouds (light blue, yellow and red) are Cumulus, while the rest are Stratocumulus; all of them, except for the yellow and the green, present Hail, that is, extremely tight, dense circles of identical tokens. These are clouds with at least 8 ($minPts$) identical tokens, defined as having a cosine distance lower than `r 0.000001`.

As we can see in the blue cloud, one cluster may have more than one piece of Hail, as is the case in some Cumulonimbus clouds. In fact, in relative numbers, the cloud type with a higher tendency to generate Hail is the Cumulonimbus, which is very fitting for a cloud that bring storms.

`r prop_ac*100`% of the clouds of about any type, in `r models_ac` different models, have these characteristics.
<!-- They are mostly `LEMMAREL` models or `FOC-WIN:3 + FOC-POS:lex` models. -->
<!-- NOTE add data based on all models -->

(ref:hail) Example of cloud with hail: inspiration on the left, plot example on the right. Picture by Tiia Monto, CC BY-SA 3.0, https://commons.wikimedia.org/w/index.php?curid=88743807

```{r, hail, out.width = '50%', anchor = 'Figure', fig.show='hold', fig.cap='(ref:hail)'}
cloud_foto("hail")
plotBasic(list(lemma = "heet", model = "heet.LEMMAREL1.PPMIselection.LENGTHFOC.SOCPOSall"))
```

These conditions are prompted by a low number of context words per token and a low type-token ratio of these context words (see Figure \@ref(fig:cwFreq)). By definition, they are groups of identical tokens: they isolate extremely strong collocations but erase all nuance, e.g. distances between groups and centrality of specific examples.

(ref:cwFreq) Mapping between the type-token ratio of the context words and the mean number of context words per token in a cluster of a medoid, by whether the cloud has Hail.

```{r, cwFreq, fig.cap = '(ref:cwFreq)'}
complex_cor(
  medoid_data,
  cw_ttratio, mean_cws, Hail,
  "TTR",
  "Mean number of context words per token",
  "Presence of Hail"
)
```

Hail tends to emerge in very restrictive models where many tokens can be grouped together because they have identical vectors: they shared the few words that survived the thresholds. They often reveal the strongest context words, i.e. those that dominate in other clouds. They might match our expectations (like in *heet* 'hot', *herroepen* 'to retract/to annul', *hemels* 'heavenly') or not (like in *staal* 'steel' and *spot* 'scorn, spotlight', where prepositions and determiners stand out). In the latter case, the problem is not too much noise, in the sense of non-distinctive information, but either not enough noise or wrong selection.

When there is not enough noise, the many identical tokens form tight and distinct clouds which we would rather group together, and tokens that don't co-occur with the most powerful context words will be invisible. In that case, a slightly less restricted model might be more useful: in a way, noisy context words, often hopeless against the strong dominating context words, smooth the differences between the tokens to give us a more complex picture.

The wrong selection of context words means that context words that survive the restriction are not informative/useful, i.e. light verbs, determiners or prepositions. The same context words may become harmless noise in less restrictive models, but they might also keep their power if the less resilient context words are not strong enough. In that case, we might want to generate new models that exclude these particular context words, dependency relations, parts of speech, etc.

We might be tempted to consider these clouds idiomatic expressions: they match, visually, what we think a representation of idiomatic expressions would be like. Instead, they match groups of context words that occur very frequently in a very short distance (either in terms of bag-of-words or dependency relations) to the target. It tells us something about bigrams: about how often *niet* 'not' occurs close to *harden* 'to tolerate'; *van* 'of' to *staal* 'steel/sample', *op* 'on' to *spot* 'spotlight', or *hang_ijzer* 'iron' to *heet* 'hot'.

## Patterns across types of clouds {#cloud-patterns}

Beyond the features used as formal criteria to define the types of clouds, we can find patterns across other relevant features, which are shown in various figures in this section. In all cases, each point represents a cluster in a medoid. The same plots generated across all models are simply more cluttered versions.

The relevant features we will discuss include the representativeness of context words for clouds, their `r sc("pmi")` with the target lemma, the type/token ratio of context words co-occurring with the tokens in a cluster, the mean number of context words per token and the entropy of the clouds. Each concept will be explained in the description of their corresponding plots.

clouds may be represented by the set of context words co-occurring with the tokens that compose it. The relationship between each context word and the cluster may be described in terms of precision and recall: precision indicates the proportion of tokens co-occurring with the context word that also belong to the cluster, while recall indicates the proportion of tokens within the cluster that co-occur with that context word. For example, if all the tokens in a cluster co-occur with the definite article *de*, *de* has a recall of 1 for that cluster; but in all likelihood, these tokens only constitute around 40% of the tokens co-occurring with *de* across the sample, resulting in a precision of 0.4. Both values can be summarized in an $F$, which is defined as the (weighted) harmonic mean of precision and recall. In this case, the unweighted $F$, that is, where precision and recall are deemed equally important, equals `r hmean(1, 0.4)`. The higher the $F$, the better the representativeness of the context word in relation to the cluster: an $F$ of 1 indicates that all the tokens co-occurring with that word belong to that cluster, and all the tokens in that cluster co-occur with that word, while an $F$ of 0 indicates the absolute lack of overlap between the domain of the context word and the clouds.

When a context word has a high $F$ in relation to a cluster, we will say that the cluster is *dominated* by the context word. This is a handy term that will come up frequently as we describe types of clouds, and especially in Chapter \@(semantic-interpretation). In general, we will only consider context words that co-occur with at least two tokens within a cluster, to avoid inflating the value of single-shot context words.

Figure \@ref(fig:fscorePmi) shows, for each cluster, the relationship between the $F$ of its most representative context word and its `r sc("pmi")` with the corresponding target lemma based on a symmetric window of 4 words to either side. The types of clouds are mapped to the colour of the clouds and the fill of the marginal boxplots.

We can see that Cirrostratus clouds (noise) tend to have low $F$ context words, and that these tend to have very low `r sc("pmi")` values with the targets. Cumulus clouds, on the other side, have the highest $F$, i.e. they tend to be dominated by one context word, and these context words tend to have high `r sc("pmi")`. Cirrus and Cumulonimbus clouds have lower values than Cumulus clouds, while Stratocumulus spans in between.

(ref:fscorePmi) Mapping between the highest $F$ between the clouds of the medoids and a context word and that context word's `r sc("pmi")` with the target, by cloud type.

```{r, fscorePmi, fig.cap = '(ref:fscorePmi)'}
complex_cor(
  filter(medoid_data, !is.na(pmi4)),
  topFscore, pmi4, cloud_type,
  "Top $F$", "PMI 4", "Cloud type"
)
```

The type-token ratio (TTR) is a measure of complexity computed as the number of different context words, i.e. types, divided by the total number of occurrences, i.e tokens. A TTR of 1 indicates that all words are only used once, while a lower TTR results from few different words used multiple times. In this case, the lower the TTR, the fewer different context words are captured by the model for the tokens in the cluster.

In Figure \@ref(fig:cwFreq) we see the relationiship betwee the TTR and the mean number of context words per token, colour-coded by whether the clusters have Hail. In contrast, Figure \@ref(fig:ttFscore) shows the relationship between TTR and the maximum $F$ of the context words, colour-coded with the cloud types.

It is only a minority of the clouds, but it is clear that both the ratio and the number of context words per token play a role, with lower values for the Hail clouds.

<!-- NOTE maybe this should go in the Hail area?
-->

As we can see in Figure \@ref(fig:ttFscore), both Cirrus and Cirrostratus clouds tend to have a higher TTR than the rest; as we saw in Table \@ref(tab:cloudFreqs), fewer of them have Hail. In general, the more different context words we find in the cluster (the higher the TTR), the lower the $F$, but there is a really wide range of variation. In any case, each type of cloud has a different profile in this sense: Cirrostratus clouds have higher TTR and lower $F$, while the case of Cumulus clouds is the opposite; Stratocumulus and Cumulonimbus clouds have similar TTR to Cumulus but lower or much lower $F$, and Cirrus TTR comparable to Cirrostratus but much higher $F$.

(ref:ttFscore) Mapping between the highest $F$ between the clouds and a context word and the TTR between context words in the cluster, by cloud type.

```{r, ttFscore, fig.cap = '(ref:ttFscore)'}
complex_cor(
  medoid_data,
  cw_ttratio, topFscore, cloud_type,
  "Type/token ratio of context words",
  "Top $F$",
  "Cloud type"
)
```

<!-- Figure \@ref(fig:epsplotM) looks at the relationship between the minimum and maximum eps value within each cluster, coloured by type of cloud. The lower the eps value, the earlier the points were clustered in `r sc("hdbscan")`; clouds in the top-right corner (mostly Cirrostratus) are flat and late to cluster (indeed, they are noise), while clouds in the bottom-left corner are flat and earlier to cluster, and those in the top-left corner are much longer in the `r sc("hdbscan")` tree. Each point represents a cluster in a medoid; for data across all models, see Figure \@ref(fig:epsplotA). -->

<!-- Beyond the obvious note that Cirrostratus clouds tend to have high eps values, we can make a few other observations. First, Cumulonimbus clouds tend to be long: their minimum value tend to be low, and the maximum tends to be high. Second, Cumulus clouds tend to have slightly lower maximum values and Cirrus tend to have higher minimum values, which is to be expected given the requirements regarding eps. This is easily mapped to meteorological altitude, as described in Section \@ref(altitude). -->

<!-- NOTE rethink whether it's a good idea to plot eps like this -->

Finally, Figure \@ref(fig:cloudentropy) shows the entropy^[Computed with `entropy::entropy()`.] of clouds of different types in terms of the manually annotated senses senses, against the entropy across the whole model. Entropy is a measure of information, and in this case it is understood as follows: the higher the entropy, the more variation of senses and the more balanced the frequencies; the lower the entropy, the more one sense dominates. We would like the entropy of the cluster (the $y$-axis in Figure \@ref(fig:cloudentropy)) to be as low as possible, that is, for the cluster to be as homogeneous as possible in term of sense. At the same time, models with a higher initial entropy (due to the sense distribution of the lemmas they model) are bound to have clusters with higher entropy.

On the one hand, we can see that clouds of all types can occur on any model regardless of their sense distribution. On the other, we can see that the different clouds do have very different entropy values. Cumulus clouds are the most homogeneous, while the Cumulonimbus clouds tend to be as heterogeneous as Cirrostratus (noise) ---they might even have higher entropy than their models as a whole. Cumulus clouds in particular, but sometimes also Stratocumulus and maybe Cirrus, may be completely homogeneous regardless of the sense composition of the model itself, but they can also have higher entropy. Stratocumulus clouds tend to have slightly lower entropy than Cirrus clouds even though they also tend to be larger.

(ref:cloudentropy) Mapping between the entropy in a medoid and in a cluster by cloud type.

```{r, cloudentropy, fig.cap = '(ref:cloudentropy)'}
complex_cor(
  medoid_data,
  model_entropy, entropy, cloud_type,
  "Entropy of full model (or lemma)",
  "Entropy of cluster",
  "Cloud type",
  add_abline = T
)
```

<!-- The metaphorical mapping from these visually represented `r sc("hdbscan")` clouds to clouds is supported by two main characteristics, and in principle I wouldn't extend them any further (we might, playfully, but I won't expand on that). On the one hand, as the images in the respective sections show, we have a visual mapping, which should be helpful at least for those of us who don't know much about clouds to begin with. From this reductive, naive perspective, both the photographs and the plots evoke comparable images of discreteness, size and density. On the other hand, a more relevant characteristic of meteorological clouds can be mapped to the `r sc("hdbscan")` clouds: the altitude. In an `r sc("hdbscan")` result, points are characterized by an $\varepsilon$ value, which roughly indicates at which point of the algorithm it was grouped with another point (is that correct? I don't understand this fully, I will read more to make a better description), and the condensed tree representation shows points with higher eps on the top and those with lower eps to the bottom. The lower the eps, the denser the area around those points. Typically, noise tokens are going to have higher eps, and denser clouds are going to have lower eps. -->

<!-- This can be mapped to the altitude of the clouds, as shown in \@ref(fig:cloudtypes) (to compare, roughly, with the data in Figure \@ref(fig:epsplotM)). -->

```{r, ranges, include = FALSE}
ranges <- cloud_data %>% split(f = .$cloud_type == "Cumulonimbus") %>%
  map(mutate, range_eps = max_eps-min_eps) %>%
  map(pull, range_eps) %>% map(summary)
```


Cirrostratus and Cirrus are at the highest altitudes, which matches their tendency to have higher $\varepsilon$ values. In absolute $\varepsilon$ values, Cirrus clouds can be low, but (i) they are typically higher than Cumulus and Stratocumulus, (ii) their maximum value is typically higher as well and (iii) by definition they may have a substantial proportion of their points higher than the lowest Cirrostratus. The relative altitude between Cirrus and Cirrostratus is not perfectly mapped to $\varepsilon$: here, the difference lies mostly in the small, disperse visual shape of the Cirrus clouds against the more uniform, large outline of Cirrostratus.

Cumulus and Stratocumulus are at the lowest altitudes, which maps to their tendencies to have lower minimum and maximum eps values. Again, the difference between them is defined in terms of shape: Cumulus are normally more compact and defined.

Finally, Cumulonimbus are, in the meteorological sense, massive and towering. This maps to both the visual and $\varepsilon$ aspects: first, it covers a large proportion of the plot and visually dominates the model; second, it tends to cover a wide range of values. Figure \@ref(fig:epsplotM) already shows that its maximum values are larger than for other types of clouds, except for Cirrostratus, while the minimum values are lower and more diverse. This is more clearly explained in terms of the range: the difference between the minimum and maximum $\varepsilon$ within a cluster. Across all the clouds (not just in medoids), the minimum $\varepsilon$ range in a Cumulonimbus clouds is
`r round(ranges[["TRUE"]][["Min."]], 2)` and the maximum is
`r round(ranges[["TRUE"]][["Max."]], 2)`;
75% percent of these clouds have a range of `r round(ranges[["TRUE"]][["1st Qu."]], 2)` or more.
In contrast, considering all the other clouds (even Cirrostratus), the $\varepsilon$ ranges span between `r round(ranges[["FALSE"]][["Min."]], 2)` (i.e. all points have the same $\varepsilon$) and `r round(ranges[["FALSE"]][["Max."]], 2)`, and 75% of these clouds have a range of `r round(ranges[["FALSE"]][["3rd Qu."]], 2)` or less. In other words, these are massive, towering clouds.


(ref:cloudtypes) Graphical representation of meteorological clouds at different altitudes. By Christopher M. Klaus at w:en:Argonne National Laboratory - Own work by en:User:Klaus, Public Domain, https://commons.wikimedia.org/w/index.php?curid=2760873

```{r, cloudtypes, out.width = '100%', anchor = "Figure", fig.cap='(ref:cloudtypes)'}
cloud_foto("cloudtypes")
```

<!-- NOTE maybe mosaic plot? -->
There are more models with Cirrus clouds than with Cumulus clouds, but we can also see an inverse correlation between the proportion of models of a lemma that exhibit Cumulus clouds and the proportion that includes Cirrus clouds.

In the semantic interpretation, we will look at lemmas with particular tendencies, in terms of the proportion of models that have at least one cluster of a given type. I selected the most significant cases based on the $\chi^2$ test: marked preference is understood as a residual above 2 or below -2.

- Marked preference for Cumulus and against Cirrus: *heffen*, *hachelijk*, and *schaal*. Of these, *heffen* also has a marked preference for Stratocumulus.

- Marked preference for Cumulus and against Cirrus, but also against Stratocumulus: *gemeen*, also the one with most Cumulonimbus clouds, and *stof*.

- Marked preference for Stratocumulus and against Cirrus, but only slightly for Cumulus: *heilzaam*.

- Marked preference for Cirrus and against both Cumulus and Stratocumulus: *geestig*, *gekleurd* and *hoekig*.

- Marked preference for Cirrus and against Cumulus but only slightly for Stratocumulus: *haten*.

- Highest preference for Cumulonimbus: *gemeen* again, but also *stof* and *schaal* (with residuals above 5; a few more have residuals above 2).

## Summary {#theo2-summary}


Clouds come in many shapes. Like the cotton-like masses of droplets we see in our skies, the clouds of word occurrences generated by token-level distributional models may take different forms, depending on their density, their size and their distinctiveness. "Meaning is use", "Differences in usage correlate with differences in meaning", "You shall know a word by the company it keeps" and other such catchy slogans sound intuitively accurate, but they hide a wealth of complexity and variation. Like meaning, context is far from orderly, and a myriad of words with different characteristics interact to generate the variation we see in these clouds.

In this chapter, we will try to make sense of the nephological topology, i.e. the variety of shapes that these clouds may take. For this purpose, we will classify `r sc("hdbscan")` clouds mapped to t-`r sc("sne")` representations in a way that can help us understand what we see when we see a cloud. The starting point is the shape that a researcher sees in the scatterplot, which will be visually mapped to meteorological cloud types and further described in technical terms.

<!--chapter:end:theo_2.Rmd-->

# Nonsense or no senses? {#semantic-interpretation}

## Introduction

In linguistic terms, clouds may provide us with different types of information, both at syntagmatic and paradigmatic level. At the syntagmatic level, they may illustrate cases of collocation, colligation, semantic preference or even tendencies towards the open-choice principle. The paradigmatic level, on the other hand, codes the relationship between the clusters and dictionary senses, from heterogeneous clusters to those that represent prototypical contexts of a sense.

Given a naive understanding of the correlation between context and meaning, we would mostly expect, from the paradigmatic perspective, clusters that equal senses: each cluster would cover all the occurrences of a dictionary sense and only the occurrences of that sense. However, even if we relax the requirements, expecting clusters with mostly occurrences of one sense and covering most of the instances that are not discarded as noise, this does not arise often. Instead, even homogeneous clusters only group typical contexts within a sense, which, at the syntagmatic level, tend to correspond to collocations. In any case, as we will see in this chapter, the full picture is much more complex, and we may obtain much richer information than just lexical collocations representing typical contexts within a sense.

In this chapter, we will look into the types of syntagmatic and paradigmatic information that the clouds offer. Section \@ref(infotypes) starts with an overview of the different levels in each dimension and mentions a few examples of their interaction in a contingency table. We then elaborate with more detailed examples of each in situation in sections \@ref(collocation) through \@ref(openchoice), and round up with an overall summary in Section \@ref(theo1-summary).

## Types of information {#infotypes}

The linguistic information obtainable from the clusters can be understood from the syntagmatic perspective as co-occurrence patterns of different kinds, and from the paradigmatic perspective in relation to dictionary senses. Both dimensions interlace, resulting in a number of specific phenomena that we may encounter. The relationship is summarized in Table \@ref(tab:colsem); the syntagmatic or collocational dimension is represented by the columns and discussed in Section \@ref(collocationally), and the paradigmatic or semantic dimension is represented by the rows and discussed in Section \@ref(semantically).

```{r, colsem, anchor = 'Table'}
read_tsv(
  here::here("assets", "collocation-prototypicality-contingency_latex.txt"),
  # here::here("assets", "collocation-prototypicality-contingency_html.txt"),
  col_types = cols()
) %>% 
  knitr::kable(booktabs = T, escape = F,
               caption = "Contingency table between the collocational and semantic perspectives, with a few examples.",
               linesep = "\\addlinespace") %>% 
  kable_styling(full_width = T) %>% 
  column_spec(1, width = "6em", bold = T) %>% 
  column_spec(2:5, width = "8.5em")
```

### Collocational perspective {#collocationally}

In order to interpret the different levels of information that a syntagmatic or collocational perspective may offer us, we can make use of some theoretical concepts from the foundations of Corpus Linguistics. Some of the terms were already coined by @firth_1957a, but they were integrated in a framework for corpus analysis by @sinclair_1998 [124-125] and other publications. The framework includes, next to the node, i.e. our targets, four key components: one obligatory, semantic prosody, which will not be discussed here, and three more that will help us make sense of the observed output of the clouds: collocation, colligation and semantic preference.

In their simplest form, collocations are defined as the co-occurrence of two words within a certain span [@firth_1957a 13; @sinclair_1991 170; @sinclair_1998 15; @stubbs_2009 124]. They might be further filtered by the statistical significance of their co-occurrence frequency or by their strength of attraction (e.g. `r sc("pmi")`; see @mcenery.hardie_2012 [122-133] for a discussion). Even though a collocational relationship is asymmetric, that is, the co-occurrence with a more frequent word B may be more important for the less frequent word A than for B, the measures used to described it are most often symmetrical [@gries_2013].
When it comes to the interpretation of clouds, this category takes a different form and is definitely asymmetric. Considering models built around a target term or node, frequent, distinct context words are bound to make the tokens that co-occur with them similar to each other and different from the rest: they will generate clusters. Such context words do tend to have a high `r sc("pmi")` with the target, but, crucially, they stand out because they are a salient feature among the occurrences of the target, independently from how salient the target would be when modelling the collocate.
Concretely, we are talking about clusters defined by one context word or a group of co-occurring context words with a high F-score in relation to the cluster. In other words, most of the tokens in a cluster co-occur with the same word(s), and most of the tokens co-occurring with these words are in that cluster. These context words can be interpreted as collocates of the target.

Unlike in most collocational studies, where you study a list of words that co-occur (significantly) frequently with your target node, vector space models allow you to find whether these context words exclude each other or also co-occur within the context of the target. In fact, we might even find more complex collocational patterns, made of multiple context words.

Whereas collocation is understood as a relationship between words (and, traditionally, as a relationship between word forms), colligation is defined as a relationship between a word and grammatical categories or syntactic patterns [@firth_1957a 14; @sinclair_1998 15; @stubbs_2009 124]. In order to capture proper colligations as clusters, we would need models in which parts of speech or maybe dependency patterns are used as features, which is not the case in these studies. However, by rejecting a strict separation between syntax and lexis
(for everything is semantics in Cognitive Linguistics),
we can make a grammatically-oriented interpretation of collocations with function words, such as frequent prepositions, the *om te* combination, or the passive auxiliary. Given this caveat, we will talk about lexically instantiated colligations when we encounter clusters dominated by items that indicate a specific grammatical function.

Semantic preference is defined as the relationship between a word and semantically similar words [@sinclair_1998 16; @stubbs_2009 125; @mcenery.hardie_2012 138-140]. Within traditional collocational studies, this implies grouping collocates, that is, already frequently co-occurring items, based on semantic similarity, much as colligation can be the result of grouping collocates based on their grammatical categories. Compared to collocation, its identification requires more interpretation on the part of the researcher.
In the interpretation of individual clusters, semantic preference appears in clusters that are not dominated by a single collocate or group of co-occurring collocates, but are instead defined by a group of infrequent context words with similar type-level vectors and for which we can give a semantic interpretation. (Cases of similar context words without a semantic interpretation are quite rare, and normally involve pronouns or adverbs.) This is a key contribution of token-level distributional models that may remain inaccessible in traditional collocational studies: next to powerful collocates that group virtually identical occurrences, we can identify patterns in which the context words are not the exact same but are similar enough to emulate a collocate. 

The three notions described above assume identifiable patterns: occurrences that are similar enough to a substantial number of other occurrences, and different enough from other occurrences, to generate a cluster. Going back to @sinclair_1991's founding notions, we are assuming the domination of the idiom principle:

> ...a language user has available to him or her a large number of semi-preconstructed phrases that constitute single choices, even though they might appear to be analysable into segments. [@sinclair_1991 110]

The opposite situation would be given by the open-choice principle:

> At each point where a unit is completed (a word or a phrase or a clause), a large range of choice opens up and the only restraint is grammaticalness. [@sinclair_1991 109]

The idiom principle and the open-choice principle are supposed to organise the lexicon and the production of utterances. But if, instead, they are understood as poles in the continuum of collocational behaviour, they can help us interpret the variety of shapes that we encounter within and across lemmas. Lemmas that tend to find identifiable clusters, with strong collocations, lexically instantiated colligations or sets with semantic preference, can be said to respond to the idiom principle. In contrast, lemmas that exhibit large proportions of noise tokens, and small, diffuse clusters (Cirrus clouds, mostly), can be said to tend towards the open-choice principle. They don't necessarily lack structure, but whatever structure they have is less clear than for other lemmas, and harder to capture with these models.

With this reasoning, next to the three categories described above, we include near-open choice as a fourth category, meant to include the clouds that do not conform to either of the clearer formats.

### Semantic perspective {#semantically}

In terms of the relationship between the `r sc("hdbscan")` clusters and the manually annotated dictionary senses, we can initially distinguish between heterogeneous clusters, i.e. those that do not exhibit a clear preference for one sense, and homogeneous clusters. Secondly, the homogeneous clusters may cover all the (clustered) tokens of a given sense, or only part of it, i.e. a (proto)typical context of the sense. Additionally, said (proto)typical context may highlight a certain aspect or dimension of the meaning of the target, different from that highlighted by a different context.

As a result, the semantic dimension covers four different types of situations. The first one, i.e. heterogeneous clusters or clusters with multiple senses, would normally be interpreted as bad modelling, if we consider the senses a gold standard and the target of our models. It is also the most frequent interpretation of the clouds that, syntagmatically, tend towards the open-choice principle. Nonetheless, they can also occur in other kinds of clouds, and as such illustrate the mismatch between contextual and semantic structure: clear contextual patterns do not imply dictionary senses.
The second type of situation, i.e. clusters that perfectly match senses, is the normal expectation from a naive point of view that would truly equate patterns to dictionary cases. It is quite rare and often indicative of fixed expressions or very particular meanings.

Rather than full senses, contextual patterns tend to represent typical contexts of a sense. 
The notion of prototypicality in Cognitive Semantics is related to the perspective that categories need not be discrete and uniform and its application to the semasiological structure of lemmas and their meanings [@geeraerts_1988]. At the extensional level, which in this case covers the domains or contexts of application of our target item, categories may be defined by a varied set of overlapping features (i.e. context words) and have fuzzy boundaries and/or degrees of membership. The central or more prototypical members of this category exhibit more of these overlapping features; the fewer features co-occur with an item, the weaker its connection to the category.
As they appear in the clouds, a sense may exhibit one typical context that is much more frequent and clear that the rest, or multiple typical contexts with various frequencies. Unfortunately, neither t-`r sc("sne")` nor `r sc("hdbscan")` provide a reliable mapping between quantitative properties and relative centrality *of* the clusters. In contrast, we can identify central cases within an `r sc("hdbscan")` cluster based on their membership probability, which, as we saw before, is the normalized core distance within a cluster. Items with a higher membership probability lie in a denser area of an `r sc("hdbscan")` cluster, and therefore have more items similar to it than the items in sparser areas. They do not necessarily occur in the euclidean centre in the t-`r sc("sne")` plot, but might form one or more dense cores closer towards an edge instead. In addition, we can distinguish between rather uniform clusters, in which all members have a similar weight, from more diverse clusters with dense cores and sparse peripheries.

Extensional prototypicality works at multiple levels. We could identify (proto)typical instances/contexts of a lemma, of a particular sense, or of a dimension of a sense. In this last case, we run into an interaction with intensional prototypicality. On the one hand, we find multiple extensionally prototypical patterns, i.e. two or more groups of attestations that instantiate different patterns. On the other, each of these patterns correlates with a different semantic dimension or aspect, wich means that that meaning dimension is salient (intensional prototypicality) to that pattern.

### Interaction between dimensions

As we can see in Table \@ref(tab:colsem), the interaction between the four levels of each dimension result in a 4x4 table with all but two cells filled with at least one example. Naturally, not all the combinations are equally frequent or interesting; the most salient one is certainly the collocation that indicates the prototypical context of a sense. But this does not mean that the rest of the phenomena should be ignored: we can still find interesting and useful information with other shapes of clouds, other contextual patterns, other semantic structure.

In the following sections, we will look in detail at examples of each attested combination. Each section will focus on one level of the collocational dimension, and will be further subdivided by the levels of the semantic dimension.
The examples will be illustrated with scatterplots in which the colours represent `r sc("hdbscan")` clusters, the shapes indicate manually annotated dictionary senses, and the transparency, the $\varepsilon$ value from `r sc("hdbscan")`. The senses are not specified in the legends, but the clusters will be named with the context word that represents it best. Concretely, for each cluster we calculate the precision and recall of the context words that co-occur with its tokens, i.e. the proportion of tokens in the cluster that co-occur with the word, as well as the proportion of tokens co-occurring with the word that are included in the cluster. From these values, an F-score is calculated: the higher it is, the better a cue the context word is for the cluster. The highest value, 1, indicates that all the tokens co-occurring with that word, and only the tokens co-occurring with that word, are within that cluster. The context word with the highest F-score, and that F-score, are used to name the clusters in these plots.

## Collocation {#collocation}

The first level of the collocational or syntagmatic dimension is that of the collocation: clusters dominated by one context word or a group of co-occurring context words. They are most likely to be found as Cumulus clouds, but also as Stratocumulus clouds or, very rarely, Cirrus clouds.

### Heterogeneous clouds {#heilzaam}

Albeit infrequently, collocations might transcend senses, that is, they might be frequent and even distinctive of a lemma without showing a preference for a specific sense.
The most clear example is found in *heilzaam* 'healthy, beneficial', which can literally mean that something is beneficial for the health or be applied, metaphorically, to other domains as well. Its clusters are typically dominated by one context word that is not indicative of any one sense: mostly *werking* 'effect' and *effect* (adding, in some models, the less frequent *invloed* 'influence'). Some examples of are shown in (@heilzaam_werking_1) through (@heilzaam_effect_2), which represent the 'healthy' and broader 'beneficial' senses respectively.

(@heilzaam_werking_1) `r sampleCtxt("heilzaam", mnum = 1, stag = "heilzaam_1", cw = "werking/noun")`

(@heilzaam_werking_2) `r sampleCtxt("heilzaam", mnum = 1, stag = "heilzaam_2", cw = "werking/noun")`

(@heilzaam_effect_1) `r sampleCtxt("heilzaam", mnum = 1, stag = "heilzaam_1", cw = "effect/noun")`

(@heilzaam_effect_2) `r sampleCtxt("heilzaam", mnum = 1, stag = "heilzaam_2", cw = "effect/noun")`

The model is shown in Figure \@ref(fig:heilzaam): the clusters dominated by *werking* 'effect', *effect* and *invloed* 'influence' are shown in yellow, light blue and green, respectively, and the manually annotated senses are mapped to the shapes: the literal 'healthy' senses is coded in circles, and the general sense, in triangles.
Within the *werking* cluster, the literal tokens (as in (@heilzaam_werking_1)) are the majority and tend towards the left side of the cloud, while the general ones (like (@heilzaam_werking_2)) tend towards the right side. There is a preference for the literal sense, especially considering that across the full sample the general sense is more frequent, but it is far from homogeneous. The balance is even more striking within the *effect* cluster, where tokens from both senses can be found.

Such a picture is pervasive across multiple models of *heilzaam* 'beneficial'. It is not necessarily the case that they do not capture words representative of 'physical health', as the vague organization within the *werking* 'effect' cluster suggests. But the most salient context words are not precisely discriminative of these two senses.

(ref:heilzaam) Cloud of *heilzaam*: `r names(d$heilzaam$medoidCoords)[[1]]`.

```{r, heilzaam, fig.cap = '(ref:heilzaam)'}
plotWithCws('heilzaam', 1)
```

This is an issue if we come to the modelling expecting lexical collocates, such as *werking* 'werking', *effect*, and *invloed* 'influence', to unequivocally represent different dictionary senses. On the other hand, *ben* 'to be' and *werk* 'to work, to have an effect' (of which *werking* is a nominalisation), co-occur with the tokens in the orange cluster, dominated by the general sense, and less so outside this cluster; see examples (@heilzaam_ben_2) and (@heilzaam_werk_2).
In other words, the most frequent nouns modified by *heilzaam* 'beneficial' tend to occur in attributive constructions (particularly *een heilzame werking hebben* 'to have a beneficial/healing effect/power' and *de heilzame werking van* 'the beneficial/healing effect/power of') and for either sense, whereas the predicative constructions present a wider variety of nouns and a stronger tendency towards the general sense.

(@heilzaam_ben_2) `r sampleCtxt("heilzaam", mnum = 1, stag = "heilzaam_2", cw = "ben/verb")`

(@heilzaam_werk_2) `r sampleCtxt("heilzaam", mnum = 1, stag = "heilzaam_2", cw = "werk/verb")`

It is an empirical question how often the contexts grouped in `r sc("hdbscan")` clouds are homogeneous in terms of particular semantic phenomena, but *heilzaam* shows that it is not something to take for granted. What is more, it illustrates how neither a high `r sc("pmi")` nor their selection as cues by human annotators guarantee that a context word distinguishes predefined senses.
When it comes to `r sc("pmi")`, it is understandable: the measure is meant to indicate how distinctive a context word is of the type as a whole, in comparison to other types. It does not take into account how distinctive it is of a group of occurrences against another group of occurrences of the same type.
When it comes to cueness annotation, however, we could have expected a more reliable selection, but apparently the salience of these context words is too high for the annotators to notice that it is not distinctive of the different senses.

### Dictionary clouds {#schaal}

```{r, schaalcounts, include = FALSE}
schaal1_counts <- d$schaal$medoidCoords[[1]]$coords %>% filter(sense == "schaal_1") %>% count(cluster) %>% deframe
```

In a few cases we can see clusters characterized by one dominant context word that perfectly match a sense, or at least its clustered tokens. These are normally fixed expressions, at least to a degree: the definition itself may require a specific expression, such as *representatieve staal* 'representative sample'.

An interesting example is shown in Figure \@ref(fig:schaal), a model of the noun *schaal* 'scale, dish'. In the plot, the 'scale' homonym is represented by circles ('a range of values, e.g. the scale of Richter, a scale from 1 to 5'), squares ('magnitude, e.g. on a large scale') and a few triangles ('ratio, e.g. a scale of 1:20'), whereas the 'dish' homonym is represented by crosses ('shallow wide dish') and crossed squares ('dish of a scale').
Both the 'range' and the 'dish of scale' senses, exemplified in (@schaal_richter) and (@schaal_gewicht), have a perfect match (or almost) with an `r sc("hdbscan")` cluster, represented by a context word with perfect F-score. All the *schaal* tokens co-occurring with *Richter* are grouped in the red cluster, and cover almost the full range of attestations of the 'range' sense, and all the tokens co-occurring with *gewicht* 'weight' are grouped in the light blue cluster and cover all the attestations of a 'dish of a scale' sense.

(@schaal_richter) `r sampleCtxt("schaal", mnum = 1, cw = "Richter/name")`

(@schaal_gewicht) `r sampleCtxt("schaal", mnum = 1, cw = "gewicht/noun")`

In a way, the phenomenon indicates a fixed, idiomatic expression: a combination of two or more words that fully represents a sense. However, the picture is more nuanced.
First, technically, the 'range' sense can potentially occur with more context words than *Richter*. In fact, one of the examples given to the annotators is *schaal van Celsius* 'Celsius scale', as well a pattern like the one found in (@schaal_nonrichter), one of the orange circles at the top of Figure \@ref(fig:schaal). However, in the corpus used for these studies, *Celsius* does not co-occur with *schaal* in a symmetric window of 4; moreover, of the `r sum(schaal1_counts)` tokens of this sense attested in this model, `r schaal1_counts[["1"]]` co-occur with *Richter*, `r schaal1_counts[["2"]]` match with the pattern from (@schaal_nonrichter), and the rest exhibit less fixed patterns or the infrequent *glijdende schaal* 'slippery slope' construction. The few matching (@schaal_nonrichter) are more readily clustered with other tokens co-occurring with the preposition *op*, such as (@schaal_nationaal). In other words, in the register of newspapers, the 'range' sense of *schaal* is almost completely exhausted in the *schaal van Richter* 'Richter scale' expression.

(@schaal_nonrichter) `r sampleCtxt("schaal", mnum = 1, clusn = 2, cw = "statistisch/adj")`

(@schaal_nationaal) `r sampleCtxt("schaal", mnum = 1, clusn = 2, cw = "nationaal/adj")`

Second, the 'dish of a scale' sense need to be used in the metaphorical expression illustrated in (@schaal_gewicht), but that is indeed the case in our data. Next to *gewicht* 'weight', these tokens also mostly co-occur with *leg* 'to lie, to place' or, in lesser degree, with *werpen* 'to throw'. Even in other models, this cluster tends to be built around the co-occurrence with *gewicht* 'weight', normally excluding tokens that only co-occur with *leg* 'to lie, to place', which do not belong to the same sense any more.

(ref:schaal) Cloud of *schaal*: `r names(d$schaal$medoidCoords)[[1]]`.

```{r, schaal, fig.cap = '(ref:schaal)'}
plotWithCws('schaal', 1)
```

These examples don't disprove the possibility of clouds dominated by a collocate perfectly covering a sense, as long as we keep in mind the characteristics and limitations of the corpus we are studying and the difference between describing "how a sense is used" and "how a sense is used *in this particular corpus*.

### (Proto)typical context {#prototypical-clouds}

The most frequent phenomenon --- especially among Cumulus clouds --- is a cluster dominated by one context word or group of co-occurring context words that represents a (proto)typical context of a sense. It may be *the* prototypical context, if the rest of the the sense is discarded as noise or spread around less clear clusters, but we might also find multiple clusters representing different typical contexts of the same sense. Neither t-`r sc("sne")` nor `r sc("hdbscan")` can tell whether one of these contexts is more central than the other, at least not in the same sense we would expect from prototype theory. Denser areas of tokens, as perceived by `r sc("hdbscan")`, are those where many tokens are very similar to each other. The more tokens are similar, and the more similar they are, the denser the area. As we will see in this example, this is not a good proxy for prototypicality.

One of the most clear examples of this phenomenon is found in *heffen* 'to levy/to lift', whose typical objects are also characteristic of its two main senses (see Figure \@ref(fig:heffen)). On the one hand, the 'to levy' sense occurs mostly with *belasting* 'tax', *tol* 'toll'^[Typical of the Netherlandic sources, since tolls are not levied in Flanders.], and *accijns* 'excise', as shown in (@heffen_belasting) through (@heffen_accijns). Their frequencies are large enough to form three distinct clusters, which tend to merge in the following levels of the `r sc("hdbscan")` hierarchy, that is, they are closer to each other than to the clusters of the other sense. On the other hand, the 'to lift' sense occurs with *glas* 'glass', where the final expression *een glas(je) heffen op* takes a metonymical meaning 'to give a toast to' (see (@heffen_glas)), and with the body-parts *hand*, *arm* and *vinger* 'finger', in which they might take other metonymical meanings. The latter group does not really belong to this "collocation" category but to "semantic preference" (see Section \@ref(semantic-preference)).

(@heffen_belasting) `r sampleCtxt("heffen", mnum = 1, cw = "belasting/noun")`

(@heffen_tol) `r sampleCtxt("heffen", mnum = 1, cw = "tol/noun")`

(@heffen_accijns) `r sampleCtxt("heffen", mnum = 1, cw = "accijns/noun")`

(@heffen_glas) `r sampleCtxt("heffen", mnum = 1, clusn = 4, stag = "heffen_1", cw = "op/prep")`

As we can see in Figure \@ref(fig:heffen), the model is very successful at separating the two senses, and the clusters are semantically homogeneous: the most relevant collocates of *heffen* 'to levy/to lift' are distinctive of one or the other of its senses. Crucially, no single cluster is even close to covering a full sense; instead, each of them represents a prototypical pattern that stands out due to its frequency, internal coherence and distinctiveness.
It seems reasonable to map the clusters to prototypical patterns because of their frequency and distinctiveness, but we should be careful about how we apply the results of the modelling to this kind of semantic analysis. From the perspective of prototype theory, a feature of a category is more central if it is more frequent, i.e. it is shared by more members, while a member is more central if it exhibits more of the defining features of the categories. As such, within the 'to levy' sense, the *belasting heffen* 'to levy taxes' pattern is the most central, and tokens instantiating such a pattern will be more central. In contrast, `r sc("hdbscan")` prioritises dense areas, that is, groups of tokens that are very similar to each other. Thus, membership probabilities, which we might be tempted to use as proxy for centrality, indicate internal consistency, lack of variation. From such a perspective, given that *belasting heffen* 'to levy taxes' is more frequent and applies to a wider variety of contexts than the other two patterns of 'to levy', its area is less dense, and its tokens have lower membership probabilities within a compound 'to levy' clusters.

In other words, the models can offer us typical patterns of a lemma and of its senses and tell us how distinctive they are from each other and how much internal variation they present. Beyond this information, they don't map in a straightforward manner to our understanding of prototypicality.

(ref:heffen) Cloud of *heffen*: `r names(d$heffen$medoidCoords)[[1]]`.

```{r, heffen, fig.cap = '(ref:heffen)'}
plotWithCws('heffen', 1)
```

It must be noted that clusters defined by collocations may not be just characterized by one single context word, but by multiple partially co-occurring context words. A clear example is *hachelijk* 'dangerous' (Figure \@ref(fig:hachelijk)), where both senses are characterized by prototypical contexts, exemplified in (@hachelijk_onderneming) through (@hachelijk_positie): *onderneming* 'undertaking', *zaak* 'business' and *avontuur* 'adventure' for the 'potentially dangerous' sense, *moment*, *situatie* 'situation', and *positie* 'position' for the 'critical' sense.
These six frequent context words are paradigmatic alternatives of each other, all taking the slot of the modified noun, the entity characterized as dangerous or critical. However, unlike its very type-level near neighbour *situatie* 'situation', *positie* 'position' may also co-occur with *bevrijd* 'to free' (and *uit* 'from') and, additionally, with *brandweer* 'firefighter', typically in Belgian contexts. The frequency of these co-occurrences in the sample, next to the type-level dissimilarity between these three lexical items, splits the co-occurrences with *positie* 'position' in three clusters (in light blue, green and red in Figure \@ref(fig:hachelijk)), based on these combinations.

(@hachelijk_onderneming) `r sampleCtxt("hachelijk", mnum = 1, clusn = 1, cw = "onderneming/noun")`

(@hachelijk_zaak) `r sampleCtxt("hachelijk", mnum = 1, clusn = 7, cw = "zaak/noun")`

(@hachelijk_avontuur) `r sampleCtxt("hachelijk", mnum = 1, clusn = 5, cw = "avontuur/noun")`

(@hachelijk_moment) `r sampleCtxt("hachelijk", mnum = 1, clusn = 6, cw = "moment/noun")`

(@hachelijk_situatie) `r sampleCtxt("hachelijk", mnum = 1, clusn = 8, cw = "situatie/noun")`

(@hachelijk_positie) `r sampleCtxt("hachelijk", mnum = 1, clusn = 2, cw = "positie/noun")`

It would be hard to argue about the relative centrality of the three *positie* clusters. They result from the combination of three features, and each cluster exhibits a different degree of membership based on how many of these overlapping features it co-occurs with. At the same time, they have a distinctive regional distribution. Based on this data, we might said that a prototypical context of *hachelijke posities* 'dangerous/critical positions' in Flanders is a situation in which firefighters free someone/something from them, while this core is not present, or at least not nearly as relevant, in the Netherlandic data. We might also say that the same situation is not typical of *hachelijke situaties* 'dangerous/critical situations', and this therefore presents a (local) distributional difference between two types that otherwise, at corpus level, are near neighbours.

(ref:hachelijk) Cloud of *hachelijk*: `r names(d$hachelijk$medoidCoords)[[1]]`.

```{r, hachelijk, fig.cap = '(ref:hachelijk)'}
plotWithCws('hachelijk', 1)
```

### Profiling {#stof}

Clusters dominated by a context word may not only represent a typical context within a sense, but also one that highlights a different dimension of such sense than other clusters. This is not extremely frequent and requires an extra layer of interpretation, but it is an additional explanation to some of the clustering solutions.

One example is given by the 'substance' meaning of *stof*<!-- , represented as circles in Figure \@ref(fig:stof) -->.
Within this sense, we tend to find clusters dominated by *gevaarlijk* 'dangerous', *schadelijk* 'harmful' (which also attracts *kankerwekkend* 'carcinogenic') and *giftig* 'poisonous' (which often attracts *chemisch* 'chemical'). These dominant context words are nearest neighbours at type-level, and the clusters they govern belong to the same branch in the `r sc("hdbscan")` hierarchy.

However, we can find additional information, among the context words that co-occur with them, which suggests that frequency is not the only responsible for their separated clusters. Concretely, the tokens in the cluster dominated by *schadelijk* 'harmful' tend to focus on the environment and composition of substances, as indicated by the co-occurrence with *uitstoot* 'emissions', *lucht* 'air', *stank* 'stench' and *bevat* 'to contain'; meanwhile, those in the cluster dominated by *giftig* 'poisonous' focus on the context of drugs or profile the liberation of substances, with context words such as *vorm* 'to form', *kom_vrij* 'to be released' and *drugsbebruik* 'drug use'.
This effect of the less frequent context words is one of the consequences of less restrictive models: at some levels of analysis, one word (*gevaarlijk* 'dangerous', *schadelijk* 'harmful'...) might be enough to disambiguate the target, but this extra information enriches our understanding of how the words are actually used. It is also contextualized information: not just about how *stof* 'substance' is used and what it means, but how it is used (and what it means) when in combination with certain frequent collocates.

<!-- (ref:stof) Cloud of *stof*: `r names(d$stof$medoidCoords)[[3]]`. -->

<!-- ```{r, stof, fig.cap = '(ref:stof)'} -->
<!-- plotWithCws('stof', 3) -->
<!-- ``` -->

## Lexically instantiated colligation {#colligation}

Even without relying on part-of-speech tags or dependency relationships as features for our models, we can obtain grammatical information from lexical collocates. For example, the passive auxiliary *word* indicates passive constructions, as well as the somewhat less frequent preposition *door*, which indicates an explicit agent, much like *by* in English. Other constructions might also be indicated by key function words, such as *om* and *te* for --- roughly --- the Dutch equivalent of a *to*-Infinitive clause, *dat* 'that' for relative clauses, *dan* 'than' for comparatives, and prepositions. The patterns that emerge from clusters with lexically instantiated colligation may cross the boundaries of dictionary senses, resulting in heterogeneous clusters, match senses, or indicate a prototypical configuration within a sense. The following subsections explore examples of these different phenomena.

### Heterogeneous clusters

The verb *herstructureren* 'to restructure' was annotated with three sense tags emerging from a combination of specialisation, i.e. whether it's specifically applied to companies, and argument structure, distinguishing between transitive and intransitive *herstructureren*. The intransitive sense is always specific --- companies restructure, undergo a process of restructure.

Models are typically not very successful at disentangling these three senses, or any one of them, for that matter. Instead, the clusters that emerge tend to highlight either the semantic or the syntactic dimension, disregarding the other one.

The lexical items that most frequently dominate clusters of *herstructureren* 'to restructure' are the passive auxiliary *word*, *bedrijf* 'company', *grondig* 'thorough(ly)', and the pair of prepositions *om* and *te*, as illustrated in (@herstructureren_worden) through (@herstructureren_omte).

(@herstructureren_worden) `r sampleCtxt("herstructureren", mnum = 7, clusn = 1, cw = "word/verb")`

(@herstructureren_bedrijf) `r sampleCtxt("herstructureren", mnum = 7, clusn = 2, cw = "bedrijf/noun")`

(@herstructureren_omte) `r sampleCtxt("herstructureren", mnum = 7, clusn = 3, cw = "om/comp")`

The two nouns never co-occur, and only occasionally co-occur with *word* or the *om te* construction, which themselves co-occur a few times. Both *grondig* 'thorough(ly)' and *bedrijf* 'company' are good cues for the company-specific senses, but may occur with either transitive or intransitive constructions. In contrast, *word* is a good cue for transitive (specifically, passive) constructions, but may occur with either the company-specific or the general sense. Finally, *om te* may be attested in either of the three senses. The stark separation of the clusters in Figure \@ref(fig:herstructureren) would seem to suggest opposite poles, but that is not the case at the semantic level. In fact, unlike Figures \@ref(fig:heffen) or \@ref(fig:hachelijk), for example, the clusters are merely slightly denser areas in a rather uniform, noise mass of tokens, and would be much harder for the naked human eye to capture without `r sc("hdbscan")` input. Instead, cluster indicates us poles of contextual behaviour which themselves may code a semantic dimension, in the case of the *bedrijf* 'company' cluster, or a syntactic one, as in the lexically instantiated colligation clusters.

(ref:herstructureren) Cloud of *herstructureren*: `r names(d$herstructureren$medoidCoords)[[7]]`.

```{r, herstructureren, fig.cap = '(ref:herstructureren)'}
plotWithCws('herstructureren', 7)
```

### Dictionary clouds

While a rare thing, we might be able to find a cluster dominated by a grammatical pattern that matches a dictionary sense. One clear case is the reflexive sense of *herhalen* 'to repeat', characterized by its co-occurrence with *zich* 'itself', in `FOC-POS:all | LEMMAREL` models, especially `PPMI:weight`^[`LEMMAPATH` models also capture it, but somehow don't build clusters around it.]. In the model shown in Figure \@ref(fig:herhalen), it is the clearest cluster, isolated below as a group of red squares. Looking closely, we can see that it is made of two halves: a small one on the left, in which the tokens also co-occur with *geschiedenis*, and a bigger one on the right, where they do not. This particular model is very restrictive: it normally captures only one or two context words per token, which is all that we need to capture this particular sense.

(ref:herhalen) Cloud of *herhalen*: `r names(d$herhalen$medoidCoords)[[7]]`.

```{r, herhalen, fig.cap = '(ref:herhalen)'}
plotWithCws('herhalen', 7)
```

We expected this result in other lemmas with purely reflexive senses as well, but it is not easy to achieve. In the case of *diskwalificeren* 'to disqualify', in contrast, the very infrequent reflexive sense is typically (but not always) absorbed within the transitive sense that matches it semantically, i.e. the non sports-related sense.
Alternatively, a lexically instantiated colligation may prefer a certain sense without exhausting its attestations: in that case, it represents a prototypical context, as shown in the following section.

### (Proto)typical context

The verb *herinneren* has two main senses defined by well defined constructions: either an intransitive construction co-occurring with the preposition *aan*, meaning 'to remind', or a reflexive construction meaning 'to remember'; a third, transitive sense is also attested but very infrequently.
In practice, it is sometimes rendered as three equally sized clouds, as shown in Figure \@ref(fig:herinneren): the orange cluster is characterized by the preposition *aan* (see (@herinneren_aan)), the green one by the subject and reflexive first person pronouns *ik* and *me* (see (@herinneren_ik)), and the yellow one by the third person reflexive pronoun *zich* (see (@herinneren_zich)). A smaller group of tokens co-occurring with *eraan*, a compound of the particle *er* and *aan* (see example (@herinneren_aan)), may form its own clusters, like the light blue one in Figure \@ref(fig:herinneren), or be absorbed by one of the larger ones.

(@herinneren_aan) `r sampleCtxt("herinneren", mnum = 3, clusn = 4)`

(@herinneren_ik) `r sampleCtxt("herinneren", mnum = 3, clusn = 3)`

(@herinneren_zich) `r sampleCtxt("herinneren", mnum = 3, clusn = 1)`

(@herinneren_eraan) `r sampleCtxt("herinneren", mnum = 3, clusn = 2)`

As the shape coding in the plot indicates, the clusters are semantically homogeneous^[With the exception of three tokens in the first-person cluster also co-occurring with *aan*, and one instantiating *ik zal herinnerd worden als* 'I will be remembered as'.], because these function words are perfect cues for the senses. The rest of the co-occurring context words do not make a difference: they are not strong enough, in the face of these pronouns and prepositions, to originate further salient structure. Nonetheless, both the *aan* and *eraan* clusters on one side, and the pronoun-based clusters on the other, belong to the same sense. Thus, what these lexically instantiated colligation clusters represent is a typical or salient pattern within each sense.

(ref:herinneren) Cloud of *herinneren*: `r names(d$herinneren$medoidCoords)[[3]]`.

```{r, herinneren, fig.cap = '(ref:herinneren)'}
plotWithCws('herinneren', 3)
```

### Profiling

Like clusters defined by collocations, clusters defined by lexically instantiated colligations can also represent a typical context that highlights a specific dimension of the sense of the target. One such case is found in the 'horde' sense of *horde*, whose most salient collocates in this corpus are  *toerist* 'tourist' and *journalist*. The two collocates are quite similar to each other at type-level, but the rest of the context words in their clusters point towards a different dimension of the 'horde' sense: hordes of journalists, photographers and fans (other nouns present in the same cluster) will surround and follow celebrities, as suggested by the co-occurrence of *omring* 'to surround', *wacht_op* 'to wait' and *achtervolg* 'to chase', among others. In contrast, hordes of tourists will instead flood and move around in the city, with words such as *stroom_toe* 'to flood' and *stad* 'city'.

As it stands, the situation is equivalent to the case of *stof* 'substance' described above. However, in the models that capture function words like the one shown in Figure \@ref(fig:horde), the profiling in these clusters is strengthened by lexically instantiated colligations. The *journalist* cluster is dominated by the preposition *door*, which signals explicit agents in passive constructions, as indicated for the orange cluster; the passive auxiliary *word* also occurs, albeit less frequently. Meanwhile, the *toerist* 'tourist' cluster includes tokens co-occurring with *naar* 'towards'. The prepositions are coherent with the dimensions of 'horde' highlighted by each of the clusters. Crucially, they don't co-occur with all the tokens that also co-occur with *journalist* and *toerist* 'tourist' respectively, but the nouns and prepositions complement each other instead.

(ref:horde) Cloud of *horde*: `r names(d$horde$medoidCoords)[[4]]`.

```{r, horde, fig.cap = '(ref:horde)'}
plotWithCws('horde', 4)
```

## Semantic preference {#semantic-preference}

Clusters that are not clearly dominated by one context word or group of co-occurring context words, be they lexical collocations or lexically instantiated colligations, may still be the result of coherent distributional and semantic patterns. Representing first-order context words with their type-level vectors allows infrequent near neighbours to join forces and approximate the effect of one context word with their cumulative frequency. These context words may occur one to four times in the sample, that is, in about one every hundred occurrences of the target, but together with other similar context words, they form a visible pattern.

### Heterogeneous clusters

Just like we can have clusters dominated by one context word that is not characteristic of one sense, we can have clusters dominated by multiple similar context words that are not characteristic of any sense. This is the case of names of colours and clothing terms^[A similar group of context words is responsible for joining the 'fabric' and 'lit. dust' senses of *stof*, even across homonyms.] co-occurring with *grijs* 'gray', which in a model like the one show in Figure \@ref(fig:grijs) also includes *haar* 'hair'.
As a result, *grijs* 'gray' tokens referring to concrete grey objects in general and, specifically, to grey/white hair, form the light blue cluster on the top right of the figure. Note that, visually, the two senses occupy opposite halves of this cluster: the *haar* 'hair' tokens occupy their own space, but the type-level similarity of the context word to the names of colours and clothing terms makes them indistinguishable to `r sc("hdbscan")`.

(ref:grijs) Cloud of *grijs*: `r names(d$grijs$medoidCoords)[[4]]`.

```{r, grijs, fig.cap = '(ref:grijs)'}
plotWithCws('grijs', 4)
```

A second example is the set of juridical terms in *herroepen*, which means 'to retract' when the object is a statement or opinion, and 'to annul' when it is a law or decision. In our newspaper corpus, it is often used in a broad legal or juridical context. However, one of the most frequent collocates of *herroepen* within this field is *uitspraak*, which can either mean 'verdict', therefore invoking the 'to annul' sense, or 'statement', to which 'to retract' applies (see (@uitspraak_1) and (@uitspraak_2)). Unfortunately, the broader context is not clear enough for the models to disambiguate the appropriate meaning of *uitspraak herroepen* in each instance. At the type-level, *uitspraak* is very close to a number of context words of the juridical field, namely *rechtbank* 'court', *vonnis* 'sentence', *veroordeling* 'conviction', etc. Together, they constitute the semantic preference of the light blue cluster in Figure \@ref(fig:herroepen), which, similar to the *grijs haar* 'gray/white hair' situation above, is visually split between the tokens co-occurring with *uitspraak* and those co-occurring with the rest of the juridical terms.

(@uitspraak_1) `r sampleCtxt("herroepen", mnum = 6, stag = "herroepen_1", cw = "uitspraak/noun")`

(@uitspraak_2) `r sampleCtxt("herroepen", mnum = 6, stag = "herroepen_2", cw = "uitspraak/noun")`

The result is understandable and interpretable: the context words co-occurring with the tokens in the light blue cluster belong to a semantically coherent set and are distributional near neighbours. The problem is that, in the sample, the sense of *uitspraak* that occurs the most is not the juridical one like in (@uitspraak_1) but 'statement' like in (@uitspraak_2), therefore representing a different sense of *herroepen* than its juridical siblings. In some models, the two groups are split as different clusters, but in those like the one shown in Figure \@ref(fig:herroepen), they generate a heterogeneous cluster generated by semantic preference.

Interestingly, *verklaring* 'statement' and *bekentenis* 'confession' could be considered part of the same semantic field as well, in broad terms. However, they belong to a different frame within the same field of legal action --- a different stage of the process ---, and, correspondingly, their type-level vectors are different and they tend to represent distinct, homogeneous clusters (in green in the figure).

(ref:herroepen) Cloud of *herroepen*: `r names(d$herroepen$medoidCoords)[[6]]`.

```{r, herroepen, fig.cap = '(ref:herroepen)'}
plotWithCws('herroepen', 6)
```

### Dictionary clusters

A few senses can be completely clustered by groups of similar context words.
One of these cases was already discussed in the context of *schaal* 'scale' tokens: in models that exclude *Richter* because of its part-of-speech tag *name*, the tokens co-occurring with it can alternatively be grouped by *kracht* 'power', *aardbeving* 'earthquake' and related context words. As in the case of *Richter* as dominating collocate, the semantic field of earthquakes is not part of the definition of the 'range' sense of *schaal*, but the dominating semantic pattern within the corpus under study.

Another example is found in *haken*, where the 'to make someone trip' sense is characterized by a variety of football-related terms (*strafschop* 'penalty kick', *penalty*, *scheidsrechter* 'referee', etc.), and the very infrequent 'crochet' sense, by *brei* 'to knit', *naai* 'to sew', *hobby* and similar words. They are represented as dark blue squares and light blue crossed squares in Figure \@ref(fig:haken) respectively. As indicated by the name of the dark blue cluster, the passive auxiliary *word* is also characteristic of the 'to make someone trip' cluster and very rarely occurs outside of it: lexically instantiated colligation working together with the clear semantic preference of the cloud.

(ref:haken) Cloud of *haken*: `r names(d$herroepen$medoidCoords)[[6]]`.

```{r, haken, fig.cap = '(ref:haken)'}
plotWithCws('haken', 1)
```

### (Proto)typical context

There are several examples of clusters defined by semantically similar infrequent context words representing typical contexts of a sense. In Figure \@ref(fig:grijs), for example, the dark blue cluster is represented by cars, mostly indicated by *Mercedes* and *Opel*, next to other brands. In the case of lemmas like *dof* 'dull', some models might dedicate different clusters to specific collocates, such as *klink* 'to sound', *knal* 'bang', *klap* 'clap', *dreun* 'pounding', while others group them together in one large cluster defined by a semantic preference indicative of a sense, e.g. sounds.

A typical semantic group attested in different lemmas is culinary: found with *schaal* 'dish' and with *heet* 'hot', the red cluster in Figure \@ref(fig:heet). In the case of *heet* 'hot', almost all the tokens co-occurring in this cluster refer to literally hot foods and drinks, although the full expression might be idiomatic, like in (@heet_soep), and only a few of them belong to the much less frequent sense 'spicy'. In other models, the tokens co-occurring with *soep* 'soup' and/or those co-occurring with *water* tokens might form separate clusters.

(@heet_soep) `r sampleCtxt("heet", mnum = 3, cw = "soep/noun")`

In addition, *aardappel* 'potato' is at type-level a near neighbour of the context words in this semantic group, but it still tends to form its own cluster, due both to its frequency and the distinctiveness of its larger cotext (e.g. the co-occurrence with *schuif_door* 'pass on'). Like other expressions annotated with the 'hot to the touch' sense (circles in the figure), including *hete hangijzer* 'hot irons' in yellow and *hete adem (in de nek)* 'hot breath (on the neck)' in light blue, *hete aardappel* 'hot potato' is used metaphorically. In the strict combination of adjective and noun, the meaning of *heet* proper is still 'hot to the touch': it is the combination itself that is then metaphorised [see @geeraerts_2006e 198-220 for a discussion].
<!-- NOTE try to get the original paper? -->
The context words themselves are frequent and distinctive enough to generate clusters of their own with the tokens that co-occur with them, but *aardappel* 'potato' tends to stick close to the culinary cluster or even merge with it.

(ref:heet) Cloud of *heet*: `r names(d$heet$medoidCoords)[[3]]`.

```{r, heet, fig.cap = '(ref:heet)'}
plotWithCws('heet', 3) +
  scale_shape_manual(values = c(16, 17, 18, 0, 2, 4, 8), guide = "none")
```

### Profiling

The adjective *geldig* 'valid' can relate to a legal or regulated acceptability, which is its most frequent sense in the sample, or may have a broader application, to entities like *redenering* 'reasoning'. By definition, and like for most of the lemmas studied here, each sense matches some form of semantic preference. In addition, models of this lemma reveal semantic preference patterns within the frequent, specific sense, each of which, in turns, highlights a different dimension of this sense. These patterns may be only identified as areas in the t-`r sc("sne")` plots or, in models like the one shown in Figure \@ref(fig:geldig), as clusters.

The green cluster is characterized by context words such as *rijbewijs* 'driving license', *paspoort* 'passport' and other forms of identification, as well as verbs like *leg_voor* 'to present', *heb* 'to have' and *bezit* 'to possess'. In other words, it represents contexts in which someone has to demonstrate possession of a valid identification document, as shown in (@geldig_voorleggen). The light blue and yellow clusters, on the other hand, co-occur with other kinds of documents (*ticket*, *abonnement* 'subscription'), *euro*, the preposition *tot* 'until', and times (*maand* 'month', *jaar* 'year', numbers, etc.). In this case, the price of the documents and the duration of their validity are more salient, as illustrated in (@geldig_lang).

(@geldig_voorleggen) `r sampleCtxt("geldig", mnum = 1, clusn = 4, cw = "leg_voor/verb")`

(@geldig_lang) `r sampleCtxt("geldig", mnum = 1, clusn = 3)`

(ref:geldig) Cloud of *geldig*: `r names(d$geldig$medoidCoords)[[1]]`.

```{r, geldig, fig.cap = '(ref:geldig)'}
plotWithCws('geldig', 1)
```

## Near-open choice {#openchoice}

The clouds described up to now in this chapter can be easily interpreted in terms of dominating context words or semantic domains. We would expect this always to be the case: if `r sc("hdbscan")` identifies a cluster, there must be structure; if there is structure, there must be an underlying pattern; if there is an underlying pattern, it can be meaningfully interpreted. Unfortunately, this is not always the case. `r sc("hdbscan")` clusters can also be formed in opposition: as we saw before in the case of the Cumulonimbus clouds, i.e. the massive clusters covering at least half the sampled tokens, the grouping criterion might be a negative definition. There is a strong pattern, and everything else that does not conform is dumped together. In other situations, whatever structure the `r sc("hdbscan")` picks up on is very faint, compared to the Cumulus skies we may find in *heffen* and *hachelijk* (see Section \@(ref:prototypical-clouds)). At present, we do not understand the relationship between `r sc("hdbscan")` and token-level distributional models well enough to understand why these less interpretable clusters emerge and how meaningful they really are.

One of the possible interpretations of these kinds of clusters, from the linguistic point of view, is that some patterns are closer to the "open choice" side of the spectrum, while the cases discussed in Section \@ref(collocation) are closer to the "idiom" side. The open-choice and idiom principle were not really presented as poles of a continuum, but they do help as interpretative tool to make sense of the variation in cloud shapes within a lemma and across lemmas. We cannot split the data studied here between models that follow the idiom principle and those that don't, because the degree to which the distributional behaviour of each lemma can be explained by the idiom principle is different.
When we generate a list of collocations for an item, we see the most relevant patterns; when we read sorted concordances, we focus on the similarities that stand out. Next to these patterns, token-level distributional models represent their strength.

In this section we will look at examples of clusters that cannot be interpreted in terms of dominating context words or semantic domains. Most of these result in heterogeneous clusters, especially Cumulonimbus clouds, but occasionally they might cover only one sense or a part of it. We will discuss how they come to be and what this means for the semantic and technical interpretation of the output. Most importantly, these examples are the first steps to describing and understanding token-level distributional models and their visual representations, in terms of `r sc("hdbscan")` and t-`r sc("sne")`.

### Heterogeneous clusters {#blik}

The most common situation in clusters that are not explained by a dominant context word or semantic preference, especially when they are Cumulonimbus clouds, is that they are semantically heterogeneous. These massive clouds occur in models where a small number of tokens that are very similar to each other --- typically idiomatic expressions, but not necessarily --- stand out as a cluster, and everything else either belongs to the same massive cluster or is noise. In many cases there is barely any noise left, while in others `r sc("hdbscan")` does seem to find a difference between the many, varied tokens in the Cumulonimbus clouds and those that are left as noise.

One such example is the Cumulonimbus cloud of *blik* in Figure \@ref(fig:blik), shown in orange. The small Cumulus clouds to either side are represented by the co-occurrence of *werp* 'to throw' and *richt* 'to aim', which indicate prototypical instances of *blik* 'gaze' (see (@blik_werpen) and (@blik_richten)). Very few tokens are excluded as noise --- the patterns they form seem to be too different from the clustered tokens to merge with them, but too infrequent to qualify as a cluster on their own.

(@blik_werpen) `r sampleCtxt("blik", mnum = 3, clusn = 1, cw = "werp/verb")`

(@blik_richten) `r sampleCtxt("blik", mnum = 3, clusn = 2, cw = "richt/verb")`

The orange cluster may seem homogeneous because of the predominance of the circles, but that is simply an effect of the large frequency of the 'gaze' sense, which can also occur in contexts like (@blik_1). The other sense of the 'gaze' homonym, 'perspective', as shown in (@blik_3), and of the 'tin' homonym (see (@blik_5)), are also part of this massive heterogeneous cluster. If anything brings these tokens together, other than the fact that they normally do not match the patterns in (@blik_werpen) and (@blik_richten), is that they typically co-occur with *een* 'a, an', *de* 'the', *met* 'with', *op* 'on', and other frequent prepositions, or more than one at the same time. These frequent, partially overlapping, and not so meaningful patterns bring all those tokens together and, to a degree, set them apart.

(@blik_1) `r sampleCtxt("blik", mnum = 3, clusn = 3, stag = "blik_1", cw = "oog/noun")`

(@blik_3) `r sampleCtxt("blik", mnum = 3, clusn = 3, stag = "blik_3")`

(@blik_5) `r sampleCtxt("blik", mnum = 3, clusn = 3, stag = "blik_5", cw = "voedsel/noun")`

(ref:blik) Cloud of *blik*: `r names(d$blik$medoidCoords)[[3]]`.

```{r, blik, fig.cap = '(ref:blik)'}
plotWithCws('blik', 3)
```

### Dictionary clusters {#huldigen}

It might seem pointless to look for meaning in clusters that do not respond to either dominating context words or semantically similar context words, but for some lemmas, it might make sense. Such is the case of the model of *huldigen* shown in Figure \@ref(fig:huldigen).

Like with other transitive verbs, the senses of this lemma are characterized by the kind of direct objects they can take. When the direct object of *huldigen* is an idea or opinion, it means 'to hold, to believe': in our sample, typical cases include *principe* 'principle', *standpunt* 'point of view' and *opvatting* 'opinion' (see examples (@huldigen_principe) through (@huldigen_opvatting)). The three of them are near neighbours at type level, but frequent enough to lead their own Cumulus clouds in most models.
In other contexts, *huldigen* means 'to celebrate, to pay homage', and the role of patient is normally filled by human beings (see examples (@huldigen_kampioen) and (@huldigen_1)). In practice, the variety of nouns that can take this place is much larger than for the other sense, and as a result, the clusters that cover this sense are less compact and defined than the clusters representing the other sense. And yet, the Cumulonimbus shown in yellow in Figure  \@ref(fig:huldigen) almost perfectly represent the 'to pay homage' sense. How is that possible?

(@huldigen_principe) `r sampleCtxt("huldigen", mnum = 6, cw = "principe/noun")`

(@huldigen_standpunt) `r sampleCtxt("huldigen", mnum = 6, cw = "standpunt/noun")`

(@huldigen_opvatting) `r sampleCtxt("huldigen", mnum = 6, cw = "opvatting/noun")`

(@huldigen_kampioen) `r sampleCtxt("huldigen", mnum = 6, cw = "kampioen/noun")`

(@huldigen_1) `r sampleCtxt("huldigen", mnum = 6, clusn = 1)`

(ref:huldigen) Cloud of *huldigen*: `r names(d$huldigen$medoidCoords)[[6]]`.

```{r, huldigen, fig.cap = '(ref:huldigen)'}
plotWithCws('huldigen', 6)
```

One of the factors playing a role in the layout of this model is that the co-occurrences with *principe* 'principle', *standpunt* 'point of view' and *opvatting* 'opinion' exhaust about half the attestation of the 'to hold, to believe' sense. The rest of the tokens are too varied and typically fall into noise. The variety within the 'to pay homage' sense cannot compete against the stark differences between these clusters and everything else. Nonetheless, there is some form of structure within the sense that differentiates it from the equally varied remaining tokens of 'to hold, to believe', and that is a family resemblance structure.

No single semantic field is enough to cover the variety of contexts in which *huldigen* 'to pay homage' occurs in our sample: instead, we find different aspects of variations on the prototypical situation of ceremonies organized by sports- and city organizations in public places, in honour of successful athletes.
In order to get a better picture of the syntagmatic relationships between the context words within the cluster, we can represent them in a network, show in Figure \@ref(fig:huldigennet). Each node represents one of the 150 most frequent context words co-occurring with tokens from the orange cloud in Figure \@ref(fig:huldigen), and it is connected to each of the context words with which it co-occurs in a token. The thickness of the edges represents the frequency with which the context words co-occur within the sample; the size of the nodes summarizes that frequency, and the size of the label roughly represents the frequency of the context word among the tokens in the cluster.

The most frequent context word is the passive auxiliary *word*: it is the only context word captured in the tokens of the dense core on the upper right corner of the cloud, and co-occurs with about half the tokens of this cluster. A number of different, less frequent context words, partially co-occur with it, such as *kampioen* 'champion', *stadhuis* 'city hall' and *sport_raad* 'sports council'. They subsequently generate their own productive branches in the family resemblance network. Crucially, this shows how we might have a token that co-occurs with *verdienstelijk* 'deserving' and *sport_raad* 'sports council' and one that co-occurs with *gemeente_bestuur* 'municipal administration' and *officieel* 'official', both as part of the same cluster.

Semantically and distributionally, the context words plotted in this network belong to different, loosely related fields, such as sports (*kampioen* 'champion', *winnaar* 'winner', *sport_raad* 'sports council'), town administration (*stad_bestuur*, *gemeente_bestuur* 'city administration') and temporal expressions (*jaar* 'year', *weekend*). The predominance of the passive auxiliary *word* --- lexically instantiated colligation ---, the presence of unified semantic fields --- multiple semantic preferences --- and the family resemblance among tokens resulting from an intricate network of co-occurrences, manages to model the more subtle, complex semantic structure of *huldigen* 'to pay homage'.

(ref:huldigennet) Network of context words of the *huldigen* 'to pay homage cluster.

```{r, huldigennet, fig.cap = '(ref:huldigennet)'}
network_cws(d$huldigen$medoidCoords[[6]]$coords %>% filter(cluster == 1))
```

## Summary {#theo1-summary}

In linguistic terms, clouds may provide us with different types of information, both at syntagmatic and paradigmatic level. At the syntagmatic level, they may illustrate cases of collocation, colligation, semantic preference or even tendencies towards the open-choice principle. The paradigmatic level, on the other hand, codes the relationship between the clusters and dictionary senses, from heterogeneous clusters to those that represent prototypical contexts of a sense.

Given a naive understanding of the correlation between context and meaning, we would mostly expect, from the paradigmatic perspective, clusters that equal senses: each cluster would cover all the occurrences of a dictionary sense and only the occurrences of that sense. However, even if we relax the requirements, expecting clusters with mostly occurrences of one sense and covering most of the instances that are not discarded as noise, this does not arise often. Instead, even homogeneous clusters only group typical contexts within a sense, which, at the syntagmatic level, tend to correspond to collocations. In any case, as we will see in this chapter, the full picture is much more complex, and we may obtain much richer information than just lexical collocations representing typical contexts within a sense.

In this chapter, we will look into the types of syntagmatic and paradigmatic information that the clouds offer. Section \@ref(infotypes) starts with an overview of the different levels in each dimension and mentions a few examples of their interaction in a contingency table. We then elaborate with more detailed examples of each in situation in sections \@ref(collocation) through \@ref(openchoice), and round up with an overall summary in Section \@ref(theo1-summary).

<!--chapter:end:theo_1.Rmd-->

# No sky fits all

> There is no optimal solution across items; what works well for one item does not necessarily do so for others, and the same holds for semantic phenomena like the ones that we used to select the set of items under consideration.

## Introduction

There is no magic trick to extract neat, semantically homogeneous clouds from the wild sea of corpus attestations. As we have seen in Chapter \@ref(shapes), the clouds can take a number of different shapes, depending the variability of the context words that co-occur with the target, their frequency and the similarity between them. Chapter \@ref(semantic-interpretation) further shows us that these clusters may have various interpretations, both from a syntagmatic perspective and from a paradigmatic perspective, resulting in a diverse net of phenomena. In this chapter, we will look at the relationship between these results and the parameter settings that produce them.

In consonance to the previous analyses, there is no golden law to be drawn from here. There is no set of parameter settings that reliably returns the best output: not for specific parts of speech, nor for specific semantic phenomena.
We will show this variability in two sections: in Section \@ref(hoopstof) we will compare the medoids of *hoop* 'hope/heap' and *stof* 'substance/dust...' that best model homonymy in each lemma, while Section \@ref(paramranking) will look at the shape that the same parameter configuration takes in many different models.

## A pile of dust {#hoopstof}

As we mentioned in Chapter \@ref(ann), we have modelled 7 homonymous and polysemous nouns, with the intention of studying the relationship between parameter settings and granularity of meaning. We expected certain parameters to be better at modelling differences between homonyms and others to be able to capture, at least in some cases, the more subtle differences between senses of a homonym. However, even though homonymy should be relatively easy to model [e.g. the examples in @schutze_1998; @yarowski_1995], the results are not so straightforward. As an example, let's look at the medoids of *hoop* 'hope, heap' and *stof* 'substance, dust...' that most successfully model the manual annotation.

Figure \@ref(fig:besthomonym) shows the best medoid of each of the lemmas, in terms of semantic homogeneity of the clusters. By mapping the sense tags to colours, we can see that each of them has a rather well defined, homogeneous area in the t-`r sc("sne")` plot. It should be noted, however, that the areas are relatively uniform, and we would be hard pressed to find such a clear structure without any colour-coding. In fact, `r sc("hdbscan")` only highlights a the most salient areas, covering, for example, only the center of the light blue island in the left plot.

(ref:besthomonym) Best medoids of *hoop* (`r names(getBest("hoop"))`) and *stof* (`r names(getBest("stof"))`).

```{r, besthomonym, fig.cap="(ref:besthomonym)"}
plot_grid(plotlist = map(c("hoop", "stof"), ~plotSenses(.x, names(getBest(.x)))), labels = c("hoop","stof"))
```

The senses plotted to the colours are coded with numbers to avoid cluttering. The senses of *hoop* are, for the first homonym, [1] literal 'heap, pile' and [2] general 'heap, bunch', and for the second homonym, [3] 'hope'. The first homonym of *stof* includes [1] 'substance', [2] 'fabric' and [3] 'topic', while the second covers [4] literal 'dust' and [6] idiomatic 'dust'. There is no sense [5] because it was not attested.
Some examples will be given below.

The parameters that result in these models are in fact very different, although their second-order configuration is equivalent: `LENGTH:FOC + SOC-POS:all`. As a result, the dimensionality of the token-level vectors is quite low: 833 for *hoop* and 483 for *stof*.

The model that works best for *hoop* is the only medoid that manages to group the tokens of the 'heap' homonym away from the larger mass of 'hoop' tokens (in green), with even a neat moat in between. If we sacrifice the infrequent literal 'heap' sense, the split is indeed outstanding. This is achieved by a `LEMMAPATH:weight` model: it uses syntactic information, selects the context words connected up to three steps away from the target, and weights the contribution of each item on that distance, regardless of the precise nature of the syntactic relationship, part-of-speech information or `r sc("pmi")`. The weights are illustrated with the superscripts in examples (@hoop_pathweight2) and (@hoop_pathweight3).

In (@hoop_pathweight2), the indefinite determiner *een* and the modified noun *onzin* 'nonsense' are directly linked to the target *hoop* as dependent and head respectively, so they are taken by the model and receive the highest weight. The first occurrence of the verb *is* is the head of its subject *onzin* 'nonsense', hence two steps away of the target: it is included and receives a slightly lower weight. The particle *er*, which is tagged as a modifier of *is*, and the second instance of *is*, as head of the subordinate clause, are three steps away from the target, and therefore obtain a low weight. The rest of the context is ignored by this model.

Example (@hoop_pathweight3) offers a much more complex picture, particularly because the link between the target *hoop* 'hope' and the verb *spreek_uit* 'to express' (split in *sprak* and its particle *uit*), is short. As the core of the dependency tree, the main verb opens the path to many other elements in the sentence.

<!-- TODO create proper info -->
(@hoop_pathweight2) Er^3^ is^2^ een^1^ **hoop** onzin^1^, talent is^3^ niet iedereen gegeven.

_There is a **lot of** nonsense, talent is not given to everyone._

(*Algemeen Dagblad*, 2001/01/27, Art. 78)

(@hoop_pathweight3) De^3^ trainer^2^ van^3^ FC Utrecht sprak^1^ verder^2^ de^1^ **hoop** uit^2^ dat^1^ hij^3^ binnenkort weer eens mag^2^ investeren^3^ van de clubleiding.

_The trainer of FC Utrecht also expressed the **hope** that the club management would allow him to invest again soon._

(*NRC Handelsblad*, 2004/05/24, Art. 93)

A key point for this lemma is that *hoop* 'hope', represented by (@hoop_pathweight3), is a mass noun, and therefore tends to occur with the definite determiner *de* (40% of the cases). In contrast, *hoop* 'heap', represented by (@hoop_pathweight2), tends to occur with *een* 'a(n)' (64 out of 76 occurrences). This correlation is hard to extract with a bag-of-words model, which would either filter out function words such as the determiners, or include all determiners, related to the target or not, thus drowning this pattern in noise.

In contrast, the parameter settings that work best for *stof* are `FOC-POS:lex + FOC-WIN:5`, i.e. they capture the nouns, verbs, adjectives and adverbs within 5 slots to each side of the target, as long as they are within the limits of the sentence and their PMI with the target lemma is positive (`BOUND:yes + PPMI:selection`). In the case of (@stof_5lex_3), for example, the model selects *discussie* 'discussion' and *lever_op* 'to bring about, to return', in italics in the transcription. Words that might follow after the period would be excluded by this model, as are those before *film* 'movie'. Within the window span of 5 words to each side, *die* 'that', *na* 'after', *veel* 'much' and *tot* 'to' are excluded because of the part-of-speech filter. Finally, the nouns *film* 'movie' and *afloop* 'end, conclusion', which survive the window size and part-of-speech filters, are excluded by the association strength filter, since their PMI value in relation to *stof* is lower than 0.

(@stof_5lex_3) Dit is een perfect voorbeeld van een film die na afloop veel **stof** tot *discussie* *oplevert*.

_This is a perfect example of a film that afterwards leaves a lot [of **stuff**] to discuss._

(*Algemeen Dagblad*, 2003/12/11, Art. 58)

If we look for it, we can find a good representation of granularity of meaning for *hoop* in Figure \@ref(fig:besthomonym). In the case of *stof*, however, the senses are quite well distinguished but the homonyms are not.
First, most of the *stof_6* 'idiomatic dust' tokens group quite nicely in some sort of appendix to the main cloud. These tokens, which are by definition idiomatic uses of *stof*, tend to be very tightly grouped in most models. An example can be seen in (@stof_sense6). Notably, they also include a few literal tokens that also co-occur with one of the defining context words, i.e. *doe* 'to make' and *waai_op* 'to lift'.

(@stof_sense6) `r sampleCtxt("stof", 3, stag = "stof_6")`

The rest of the tokens seem to be organized by sense with subtle borders in between. The most frequent sense, *stof_1* 'substance', even includes a few independent islands on top, already discussed in Section \@ref(stof).

Most interestingly, *stof_2* 'fabric' and *stof_4* 'dust', in light blue and yellow respectively, go together, even though they belong to different homonyms. In fact, HDBSCAN merges them together in one cluster, as we will see in Figure \@ref(fig:popular1). This is not entirely surprising, given that both senses tend to co-occur with quite concrete context words, such as names for materials and colours (see for example (@stof_sense2) and (@stof_sense4)), while the 'substance' sense is more chemically-oriented and the 'topic' sense illustrated in (@stof_5lex_3) co-occurs with the semantic domain of communication instead.

(@stof_sense2) `r sampleCtxt("stof", 3, stag = "stof_2")`

(@stof_sense4) `r sampleCtxt("stof", 3, stag = "stof_4")`

This description should suffice to understand how very different parameter configurations are necessary to model such different lemmas. The fact that both of them are homonyms is not enough: other aspects of their structure, such as the kind of contextual features that characterise each sense or homonym, play a role.

What we have not shown is that other models are not as good. What would come out from applying the parameter settings that work best for one lemma onto the other? This we see in Figure \@ref(fig:switchbest).

(ref:switchbest) Model of *hoop* with the parameters that work best for *stof* (`r names(getBest("stof"))`) and vice versa (`r names(getBest("hoop"))` of *stof*)

```{r, switchbest, fig.cap="(ref:switchbest)"}
plot_grid(plotlist = map2(c("hoop", "stof"), c("stof", "hoop"), ~plotSenses(.x, names(getBest(.y)))), labels = c("hoop","stof"))
```

Indeed, swapping the configurations returns unsatisfying results. In the case of *hoop*, we see a similar picture to many other models: a plot overrun by by 'hope', with maybe an area with more 'literal heap' tokens, while the 'general heap' tokens, that were so nicely separated in Figure \@ref(fig:besthomonym), are mixed and distributed across one hemisphere. In the case of *stof*, we keep having a large 'substance' area in orange, an isolated blue section for the idiomatic 'dust' and a shy green peninsula of 'topic' tokens, but the concrete senses, 'fabric' and 'dust', are disperse and mixed.

Even for a fairly straightforward task as discriminating homonyms, parameters that succeed in one lemma fail in the other. This is unrelated to the number or frequency of the senses. Instead, it is inextricably linked to the particular distributional behaviour of each lemma. While *stof* can find collocations or semantic preferences that, to various degrees, represent (parts of) senses, the lexical contexts of *hoop* are too varied to generate clear clusters. On the other hand, a syntactically informed model can identify determiners as a relevant feature of *hoop*, while the same information seems less interesting in regard to *stof*.

## Weather forecast gone crazy {#paramranking}

```{r, include = FALSE, warning = FALSE}
popular_medoid <- "BOWbound5lex.PPMIselection.LENGTHFOC.SOCPOSall"
to_show <- c("heet", "stof", "dof", "huldigen", "haten", "hoop", "heilzaam", "hachelijk")
popular_plots <- map(
  setNames(to_show, to_show),
  ~plotWithCws(.x, str_glue("{.x}.{popular_medoid}"), colorguide = "none"))
plotPopular <- function(...) plot_grid(plotlist = popular_plots[c(...)], labels = c(...))
```

Parameter settings do not have an equal effect across all models. First, their quantitative impact may vary: while in some lemmas `FOC-POS:lex` models are very distinctive, in others `FOC-WIN` makes a greater difference. Sometimes the models with `LENGTH:5000 + SOC-POS:all` are the most singular of all: they are consistently messy, and the type-level distances between all pairs of context words is huge. What generates these patterns does not lie on the part-of-speech of the target or the semantic phenomena we expect in it. Moreover, it is unrelated to the possibility of success of the models. There is variation, and we do not understand it yet.

<!-- TODO add proper graph :) -->

Qualitatively, the same set of parameter settings can generate multiple different solutions, depending on the distributional properties of the lemma being modelled. We already saw this in the comparison between Figures \@ref(besthomonym) and \@ref(switchbest): what works best for one lemma will not necessary give a decent result in another. In this section, we briefly look look at the same parameters that result in the best *stof* model, applied to various other lemmas. The colour-coding matches the `r sc("hdbscan")` clusters, and the shapes, the annotated senses.

In Figure \@ref(fig:popular1), we see the same model for *heet* 'hot' and *stof* 'substance, dust...'. The model of *heet* 'hot' has 12 clusters^[For colours to be distinct enough, the palette has only 9 values; the rest of the clusters are also gray.], with roughly equal proportion of Cumulus, Stratocumulus and Cirrus clouds. Most of them are collocation clusters representing typical patterns within a sense, but we also find cases of semantic preference and a few of near-open choice and heterogeneity. The *stof* 'substance, dust...' model looks roughly similar: it has 7 relatively homogeneous clusters: the three Stratocumulus on the upper left are the collocation clusters discussed in Section \@ref(stof) and, next to the red Cirrus defined by semantic preference, they represent typical uses of the 'substance' sense. The rest of the clusters, as discussed above, are more heterogeneous.

<!-- TODO Add that for heet these are not different senses: they are idiomatic expressions, unlike in stof -->

(ref:popular1) Models of *heet* and *stof* with the `FOC-POS:lex + FOC-WIN:5 + PPMI:selection` and `LENGTH:FOC + SOC-POS:all`.

```{r, popular1, fig.cap="(ref:popular1)"}
plotPopular("heet", "stof", "dof", "huldigen")
```

The lemmas shown in Figure \@ref(fig:pupular2), *dof* 'dull' and *huldigen* 'to believe/to pay homage', look rather similar to each other but very different from the ones in Figure \@ref(fig:popular1). Even though *dof* 'dull', not unlike *heet*, tends to have multiple clusters characterised by collocations with different types of sounds, it takes a different shape in this model. The metaphorical sense represented by the collocation with *ellende* 'misery' forms a neat orange Cumulus on one side; the semantic preference for sounds gives rise to the homogeneous light blue Stratocumulus below, and the rest of the tokens, both those related to the visual sense and the rest of the metaphorical ones, gather in the heterogeneous green Stratocumulus. As we have seen before, *huldigen* also has some strong collocates, but in this model, the tokens of 'to believe', led by *principe* 'principle', *opvatting* 'opinion' and *standpunt* 'point of view', take part of an extremely homogeneous orange Stratocumulus, while most of the 'to pay homage' sense covers the light blue Cumulonimbus, like in the case described in Section \@ref(huldigen).

(ref:popular2) Models of *dof* and *huldigen* with the `FOC-POS:lex + FOC-WIN:5 + PPMI:selection` and `LENGTH:FOC + SOC-POS:all`.

```{r, popular2, fig.cap="(ref:popular2)"}
plotPopular("dof", "huldigen")
```

The lemmas in Figure \@ref(fig:popular3), *haten* 'to hate' and *hoop* 'hope/heap', show yet another configuration generated by the same parameter settings. Except for the green Stratocumulus in *haten*, roughly dominated by *mens* 'human, people', the rest of the clouds are Cirrus clouds: small, heterogeneous, characterised by many different words.

(ref:popular3) Models of *haten* and *hoop* with the `FOC-POS:lex + FOC-WIN:5 + PPMI:selection` and `LENGTH:FOC + SOC-POS:all`.

```{r, popular3, fig.cap="(ref:popular3)"}
plotPopular("haten", "hoop")
```


## Summary {#theo3-summary}

The output is not directly predictable from the parameter settings of a model. Clouds can take many shapes, lemmas exhibit different distributional patterns, and these patterns can have different semantic interpretations. The parameter settings that model one phenomenon best, in a certain model, will not necessarily model the same phenomenon in another lemma, or anything else of interest for that matter. 
The same parameter settings can result in drastically different shapes across lemmas, or even if the shapes are similar and they are the result of comparable distributional behaviours, they might have different semantic interpretations.

<!--chapter:end:theo_3.Rmd-->

# (PART) Conclusion {-}

# Cloudspotter's guide

[General tips and so on]

<!--chapter:end:con_1.Rmd-->

# Avenues for further research


<!--chapter:end:con_2.Rmd-->

# Conclusion


<!--chapter:end:con_3.Rmd-->


`r if (knitr::is_html_output()) '
# References {-}
'`
<div id="refs"></div>

# (APPENDIX) Appendix {-}

# Definitions
<!-- TODO Add frequency, maybe annotation data? Decide on order -->
<!-- NOTE This is not properly rendered in pdf, but it is rendered -->

```{r, "basic-def-function", include = FALSE}
definitions <- readxl::read_excel(here::here("assets", "definitions.xlsx"))
show_def <- function(l) {
  tb_def <- definitions %>% filter(lemma == l) %>%
    rename(sense = code, Dutch = definition, English = definition_translation) %>%
    flextable(col_keys = c("sense", "Dutch", "English")) %>%
    flextable::compose(j = "Dutch",
                       value = as_paragraph(Dutch, ": ",
                                            as_chunk(example, props = officer::fp_text(italic = TRUE)))) %>%
    flextable::compose(j = "English",
                       value = as_paragraph(English, ": ",
                                            as_chunk(example_translation, props = officer::fp_text(italic = TRUE)))) %>%
    autofit()
  
  cat("## ", l, "{-}    \n")
  cat(knitr::knit_print(tb_def))
  cat("    \n")
}
```

```{r, echo = FALSE, results = 'asis'}
for(lemma in unique(definitions$lemma)) show_def(lemma)
```


<!--chapter:end:references.Rmd-->

