
@incollection{divjak.fieller_2014,
	address = {Amsterdam ; Philadelphia},
	series = {Human cognitive processing},
	title = {Cluster analysis: {Finding} structure in linguistic data},
	isbn = {978-90-272-2397-5},
	number = {volume 43},
	booktitle = {Corpus methods for semantics: quantitative studies in polysemy and synonymy},
	publisher = {John Benjamins Publishing Company},
	author = {Divjak, Dagmar and Fieller, Nick},
	editor = {Glynn, Dylan and Robinson, Justyna A.},
	year = {2014},
	keywords = {Cognitive grammar, Computational linguistics, Corpora (Linguistics), Polysemy, Semantics},
	pages = {405--441},
}

@article{heylen.etal_2015,
	title = {Monitoring polysemy: {Word} space models as a tool for large-scale lexical semantic analysis},
	volume = {157},
	issn = {00243841},
	shorttitle = {Monitoring polysemy},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0024384114002770},
	doi = {10.1016/j.lingua.2014.12.001},
	language = {en},
	urldate = {2017-08-23},
	journal = {Lingua},
	author = {Heylen, Kris and Wielfaert, Thomas and Speelman, Dirk and Geeraerts, Dirk},
	month = apr,
	year = {2015},
	pages = {153--172},
}

@article{schutze_1998,
	title = {Automatic {Word} {Sense} {Discrimination}},
	volume = {24},
	issn = {0891-2017},
	abstract = {This paper presents context-group discrimination, a disambiguation algorithm based on clustering. Senses are interpreted as groups (or clusters) of similar contexts of the ambiguous word. Words, contexts, and senses are represented in Word Space, a high-dimensional, real-valued space in which closeness corresponds to semantic similarity. Similarity in Word Space is based on second-order co-occurrence: two tokens (or contexts) of the ambiguous word are assigned to the same sense cluster if the words they co-occur with in turn occur with similar words in a training corpus. The algorithm is automatic and unsupervised in both training and application: senses are induced from a corpus without labeled training instances or other external knowledge sources. The paper demonstrates good performance of context-group discrimination for a sample of natural and artificial ambiguous words.},
	number = {1},
	journal = {Computational Linguistics},
	author = {Schütze, Hinrich},
	month = mar,
	year = {1998},
	pages = {97--123},
	file = {ACM Full Text PDF:C\:\\Users\\u0118974\\Zotero\\storage\\HMM4UICW\\Schütze - 1998 - Automatic Word Sense Discrimination.pdf:application/pdf},
}

@article{giora_1997,
	title = {Understanding figurative and literal language: {The} graded salience hypothesis},
	volume = {8},
	issn = {0936-5907, 1613-3641},
	shorttitle = {Understanding figurative and literal language},
	url = {http://www.degruyter.com/view/j/cogl.1997.8.issue-3/cogl.1997.8.3.183/cogl.1997.8.3.183.xml?format=INT},
	doi = {10.1515/cogl.1997.8.3.183},
	number = {3},
	urldate = {2012-06-25},
	journal = {Cognitive Linguistics},
	author = {Giora, Rachel},
	month = jan,
	year = {1997},
	pages = {183--206},
}

@book{langacker_2008a,
	address = {Oxford ; New York},
	title = {Cognitive grammar: a basic introduction},
	isbn = {978-0-19-533196-7 0-19-533196-6 978-0-19-533195-0 0-19-533195-8},
	shorttitle = {Cognitive grammar},
	publisher = {Oxford University Press},
	author = {Langacker, Ronald W.},
	year = {2008},
	keywords = {Cognitive grammar},
}

@incollection{rohrer_2007,
	address = {Oxford ; New York},
	series = {Oxford handbooks},
	title = {Embodiment and experientialism},
	isbn = {978-0-19-514378-2},
	language = {en},
	booktitle = {The {Oxford} handbook of cognitive linguistics},
	publisher = {Oxford University Press},
	author = {Rohrer, Tim},
	editor = {Geeraerts, Dirk and Cuyckens, H.},
	year = {2007},
	keywords = {Cognitive grammar},
	pages = {25--47},
}

@article{parodi_2008,
	title = {Lingüística de corpus: una introducción al ámbito.},
	volume = {46},
	language = {es},
	number = {1},
	journal = {Revista de Lingüística Teórica y Aplicada},
	author = {Parodi, Giovanni},
	year = {2008},
	pages = {93--119},
}

@incollection{geeraerts_2005,
	address = {Berlin ; New York},
	series = {Cognitive linguistics research},
	title = {Lectal variation and empirical data in {Cognitive} {Linguistics}},
	isbn = {978-3-11-018617-8 3-11-018617-9},
	number = {32},
	booktitle = {Cognitive linguistics: internal dynamics and interdisciplinary interaction},
	publisher = {Mouton de Gruyter},
	author = {Geeraerts, Dirk},
	editor = {Ruiz de Mendoza Ibáñez, Francisco José and Peña Cervel, M. Sandra},
	year = {2005},
	keywords = {Cognitive grammar},
	pages = {163--189},
}

@article{arppe.etal_2010,
	title = {Cognitive {Corpus} {Linguistics}: five points of debate on current theory and methodology},
	volume = {5},
	issn = {1749-5032, 1755-1676},
	shorttitle = {Cognitive {Corpus} {Linguistics}},
	url = {http://www.euppublishing.com/doi/abs/10.3366/cor.2010.0001},
	doi = {10.3366/cor.2010.0001},
	language = {en},
	number = {1},
	urldate = {2016-06-14},
	journal = {Corpora},
	author = {Arppe, Antti and Gilquin, Gaëtanelle and Glynn, Dylan and Hilpert, Martin and Zeschel, Arne},
	month = may,
	year = {2010},
	pages = {1--27},
}

@book{biber.etal_1998,
	address = {Cambridge ; New York},
	series = {Cambridge approaches to linguistics},
	title = {Corpus linguistics: investigating language structure and use},
	isbn = {978-0-521-49622-3 978-0-521-49957-6},
	shorttitle = {Corpus linguistics},
	publisher = {Cambridge University Press},
	author = {Biber, Douglas and Conrad, Susan and Reppen, Randi},
	year = {1998},
	keywords = {Computational linguistics, Corpora (Linguistics), Linguistic analysis (Linguistics)},
}

@incollection{mittelberg_2007,
	address = {Amsterdam ; Philadelphia},
	series = {Human cognitive processing},
	title = {Methodology for multimodality: {One} way of working with speech and gesture data},
	isbn = {978-90-272-2371-5 978-90-272-2372-2},
	number = {v. 18},
	booktitle = {Methods in cognitive linguistics},
	publisher = {John Benjamins Pub},
	author = {Mittelberg, Irene},
	editor = {Gonzalez-Marquez, Monica and Mittelberg, Irene and Coulson, Seana and Spivey, Michael J.},
	year = {2007},
	note = {OCLC: ocm73743935},
	keywords = {Cognitive grammar, Research Methodology},
	pages = {225--249?},
}

@incollection{grondelaers.etal_2007,
	address = {Amsterdam ; Philadelphia},
	series = {Human cognitive processing},
	title = {A case for a cognitive corpus linguistics},
	isbn = {978-90-272-2371-5 978-90-272-2372-2},
	number = {v. 18},
	booktitle = {Methods in cognitive linguistics},
	publisher = {John Benjamins Pub},
	author = {Grondelaers, Stefan and Geeraerts, Dirk and Speelman, Dirk},
	editor = {Gonzalez-Marquez, Monica and Mittelberg, Irene and Coulson, Seana and Spivey, Michael J.},
	year = {2007},
	note = {OCLC: ocm73743935},
	keywords = {Cognitive grammar, Research Methodology},
	pages = {149--168?},
}

@incollection{waugh.etal_2007,
	address = {Amsterdam ; Philadelphia},
	series = {Human cognitive processing},
	title = {Multiple empirical approaches to a complex analysis of discourse},
	isbn = {978-90-272-2371-5 978-90-272-2372-2},
	number = {v. 18},
	booktitle = {Methods in cognitive linguistics},
	publisher = {John Benjamins Pub},
	author = {Waugh, Linda and Fonseca-Greber, Bonnie and Vickers, Caroline and Eröz, Betil},
	editor = {Gonzalez-Marquez, Monica and Mittelberg, Irene and Coulson, Seana and Spivey, Michael J.},
	year = {2007},
	note = {OCLC: ocm73743935},
	keywords = {Cognitive grammar, Research Methodology},
	pages = {120--148?},
}

@book{mcenery.hardie_2012,
	address = {Cambridge ; New York},
	series = {Cambridge textbooks in linguistics},
	title = {Corpus linguistics: method, theory and practice},
	isbn = {978-0-521-83851-1 978-0-521-54736-9},
	shorttitle = {Corpus linguistics},
	abstract = {"Corpus linguistics is the study of language data on a large scale - the computer-aided analysis of very extensive collections of transcribed utterances or written texts. This textbook outlines the basic methods of corpus linguistics, explains how the discipline of corpus linguistics developed, and surveys the major approaches to the use of corpus data. It uses a broad range of examples to show how corpus data has led to methodological and theoretical innovation in linguistics in general. Clear and detailed explanations lay out the key issues of method and theory in contemporary corpus linguistics. A structured and coherent narrative links the historical development of the field to current topics in 'mainstream' linguistics. Practical activities and questions for discussion at the end of each chapter encourage students to test their understanding of what they have read and an extensive glossary provides easy access to definitions of technical terms used in the text"--},
	publisher = {Cambridge University Press},
	author = {McEnery, Tony and Hardie, Andrew},
	year = {2012},
	note = {OCLC: ocn732967848},
	keywords = {Corpora (Linguistics)},
}

@article{gablasova.etal_2017,
	title = {Collocations in {Corpus}-{Based} {Language} {Learning} {Research}: {Identifying}, {Comparing}, and {Interpreting} the {Evidence}: {Collocations} in {Corpus}-{Based} {Language} {Learning} {Research}},
	issn = {00238333},
	shorttitle = {Collocations in {Corpus}-{Based} {Language} {Learning} {Research}},
	url = {http://doi.wiley.com/10.1111/lang.12225},
	doi = {10.1111/lang.12225},
	abstract = {This article focuses on the use of collocations in language learning research (LLR).
Collocations, as units of formulaic language, are becoming prominent in our understanding
of language learning and use; however, while the number of corpus-based LLR
studies of collocations is growing, there is still a need for a deeper understanding of
factors that play a role in establishing that two words in a corpus can be considered to
be collocates. In this article we critically review both the application of measures used
to identify collocability between words and the nature of the relationship between two
collocates. Particular attention is paid to the comparison of collocability across different
corpora representing different genres, registers, or modalities. Several issues involved
in the interpretation of collocational patterns in the production of first language and
second language users are also considered. Reflecting on the current practices in the
field, further directions for collocation research are proposed.},
	language = {en},
	urldate = {2017-02-22},
	journal = {Language Learning},
	author = {Gablasova, Dana and Brezina, Vaclav and McEnery, Tony},
	month = feb,
	year = {2017},
}

@book{mcenery.etal_2010,
	address = {London},
	edition = {Reprinted},
	series = {Routledge applied linguistics},
	title = {Corpus-based language studies: an advanced resource book},
	isbn = {978-0-415-28623-7},
	shorttitle = {Corpus-based language studies},
	language = {eng},
	publisher = {Routledge},
	author = {McEnery, Tony and Xiao, Richard and Tono, Yukio},
	year = {2010},
	note = {OCLC: 642930078},
	keywords = {Computational linguistics, Corpora (Linguistics), Aufsatzsammlung, Englisch, Korpus, Lehrbuch, Linguistics Methodology},
	file = {Table of Contents PDF:C\:\\Users\\u0118974\\Zotero\\storage\\EUMQ27M5\\McEnery et al. - 2010 - Corpus-based language studies an advanced resourc.pdf:application/pdf},
}

@article{stubbs_1995,
	title = {Collocations and semantic profiles: {On} the cause of the trouble with quantitative studies},
	volume = {2},
	issn = {0929-998X, 1569-9765},
	shorttitle = {Collocations and semantic profiles},
	url = {http://www.jbe-platform.com/content/journals/10.1075/fol.2.1.03stu},
	doi = {10.1075/fol.2.1.03stu},
	abstract = {Current work on lexical collocations uses two ideas: (i) words have
distinctive semantic profiles or "prosodies"; and (ii) the strength of
association between words can be measured in quantitative terms.
These ideas can be combined to provide comparative semantic
profiles of words, which show the frequent and characteristic collocates
of node words, and make explicit the semantic relations between
the collocates.
Using data from corpora of up to 120 million words, it is shown
that the lemma CAUSE occurs in predominantly "unpleasant" collocations,
such as cause of the trouble and cause of death. A case study
of this lemma is used to illustrate quantitative methods for investigating
collocations. Various methods proposed in the literature are
of great practical value in establishing collocational sets, but their
theoretical basis is less clear. Brief comparative semantic profiles
are given for related lemmas, e.g. REASON and CONSEQUENCE.
Implications for the relation between system and use are discussed.},
	language = {en},
	number = {1},
	urldate = {2017-02-15},
	journal = {Functions of Language},
	author = {Stubbs, Michael},
	year = {1995},
	pages = {23--55},
}

@inproceedings{church.hanks_1989,
	title = {Word association norms, mutual information, and lexicography},
	url = {http://portal.acm.org/citation.cfm?doid=981623.981633},
	doi = {10.3115/981623.981633},
	abstract = {The term word association is used in a very particular sense in the psycholinguistic literature.
(Generally speaking, subjects respond quicker than normal to the word nurse if it follows a highly associated word such as doctor.)
We will extend the term to provide the basis for a statistical description of a variety of interesting linguistic phenomena, ranging from semantic relations of the doctor/nurse type (content word/content word)
to lexico-syntactic co-occurrence constraints between verbs and prepositions (content word/function word).
This paper will propose an objective measure based on the information theoretic notion of mutual information, for estimating word association norms from computer readable corpora.
(The standard method of obtaining word association norms, testing a few thousand :mbjects on a few hundred words, is both costly and unreliable.)
The proposed measure, the association ratio, estimates word association norms directly from computer readable corpora, making it possible to estimate norms for tens of thousands of words.},
	language = {en},
	urldate = {2017-02-15},
	booktitle = {{ACL} '89: {Proceedings} of the 27th annual meeting on {Association} for {Computational} {Linguistic}},
	publisher = {Association for Computational Linguistics},
	author = {Church, Kenneth Ward and Hanks, Patrick},
	month = jun,
	year = {1989},
	pages = {76--83},
}

@book{sinclair_1991,
	address = {Oxford},
	edition = {3. impr},
	series = {Describing {English} language},
	title = {Corpus, concordance, collocation},
	isbn = {978-0-19-437144-5},
	language = {eng},
	publisher = {Oxford Univ. Press},
	author = {Sinclair, John},
	year = {1991},
	note = {OCLC: 174383705},
	keywords = {Englisch, Korpus, Computerlinguistik, Kollokation},
	file = {Table of Contents PDF:C\:\\Users\\u0118974\\Zotero\\storage\\WJIXUDHQ\\Sinclair - 1995 - Corpus, concordance, collocation.pdf:application/pdf},
}

@incollection{geeraerts_2006b,
	address = {Berlin},
	series = {Applications of cognitive linguistics},
	title = {Methodology in {Cognitive} {Linguistics}},
	isbn = {978-3-11-018950-6 3-11-018951-8 978-3-11-018951-3 3-11-018950-X 978-3-11-018950-6},
	language = {eng},
	number = {1},
	booktitle = {Cognitive linguistics: current applications and future perspectives},
	publisher = {Mouton de Gruyter},
	author = {Geeraerts, Dirk},
	editor = {Kristiansen, Gitte and Achard, Michel and Dirven, René and Ruiz de Mendoza Ibáñez, Francisco José},
	year = {2006},
	keywords = {Cognitive grammar, Aufsatzsammlung, Cognitive grammar Methodology, Kognitive Linguistik, Methodology},
	pages = {21--49},
}

@incollection{geeraerts_2015a,
	address = {Berlin, München, Boston},
	title = {Lexical semantics},
	isbn = {978-3-11-029202-2},
	urldate = {2017-08-11},
	booktitle = {Handbook of {Cognitive} {Linguistics}},
	publisher = {DE GRUYTER},
	author = {Geeraerts, Dirk},
	editor = {Dabrowska, Ewa and Divjak, Dagmar},
	month = jan,
	year = {2015},
	doi = {10.1515/9783110292022-014},
	pages = {273--295},
}

@book{geeraerts_2006e,
	address = {Berlin ; New York},
	series = {Cognitive linguistics research},
	title = {Words and other wonders: papers on lexical and semantic topics},
	isbn = {978-3-11-019042-7},
	shorttitle = {Words and other wonders},
	number = {33},
	publisher = {Mouton de Gruyter},
	author = {Geeraerts, Dirk},
	year = {2006},
	keywords = {Semantics, Lexicology},
}

@incollection{ibarretxe-antunano.valenzuela_2016a,
	address = {Barcelona},
	edition = {2da},
	series = {Autores, textos y temas {Filosofía}},
	title = {Lingüística cognitiva: origen, principios y tendencias},
	isbn = {978-84-15260-37-0},
	language = {spa},
	booktitle = {Lingüística cognitiva},
	publisher = {Anthropos Ed},
	author = {Ibarretxe-Antuñano, Iraide and Valenzuela, Javier},
	editor = {Ibarretxe-Antuñano, Iraide and Valenzuela, Javier},
	year = {2016},
	note = {OCLC: 813204430},
	keywords = {Kognitive Linguistik},
	pages = {13--38},
}

@article{lenci_2018,
	title = {Distributional {Models} of {Word} {Meaning}},
	volume = {4},
	issn = {2333-9683, 2333-9691},
	url = {http://www.annualreviews.org/doi/10.1146/annurev-linguistics-030514-125254},
	doi = {10.1146/annurev-linguistics-030514-125254},
	language = {en},
	number = {1},
	urldate = {2018-04-12},
	journal = {Annual Review of Linguistics},
	author = {Lenci, Alessandro},
	month = jan,
	year = {2018},
	pages = {151--171},
	file = {Lenci - 2018 - Distributional Models of Word Meaning.pdf:C\:\\Users\\u0118974\\Zotero\\storage\\YRSW9EUM\\Lenci - 2018 - Distributional Models of Word Meaning.pdf:application/pdf},
}

@inproceedings{kiela.clark_2014,
	address = {Gothenburg},
	title = {A {Systematic} {Study} of {Semantic} {Vector} {Space} {Model} {Parameters}},
	abstract = {We present a systematic study of parameters used in the construction of semantic vector space models. Evaluation is carried out on a variety of similarity tasks, including a compositionality dataset, using
several source corpora. In addition to recommendations
for optimal parameters, we present some novel findings, including a similarity metric that outperforms the alternatives on all tasks considered.},
	booktitle = {Proceedings of the 2nd {Workshop} on {Continuous} {Vector} {Space} {Models} and their {Compositionality}},
	publisher = {ACL},
	author = {Kiela, Douwe and Clark, Stephen},
	month = apr,
	year = {2014},
	pages = {21--30},
}

@book{manning.schutze_1999,
	address = {Cambridge, Mass},
	title = {Foundations of statistical natural language processing},
	isbn = {978-0-262-13360-9},
	publisher = {MIT Press},
	author = {Manning, Christopher D. and Schütze, Hinrich},
	year = {1999},
	keywords = {Computational linguistics, Statistical methods},
}

@article{turney.pantel_2010,
	title = {From {Frequency} to {Meaning}: {Vector} {Space} {Models} of {Semantics}},
	volume = {37},
	abstract = {Computers understand very little of the meaning of human language. This profoundly limits our ability to give instructions to computers, the ability of computers to explain their actions to us, and the ability of computers to analyse and process text. Vector space models (VSMs) of semantics are beginning to address these limits. This paper surveys the use of VSMs for semantic processing of text. We organize the literature on VSMs according to the structure of the matrix in a VSM. There are currently three broad classes of VSMs, based on term–document, word–context, and pair–pattern matrices, yielding three classes of applications. We survey a broad range of applications in these three categories and we take a detailed look at a speciﬁc open source project in each category. Our goal in this survey is to show the breadth of applications of VSMs for semantics, to provide a new perspective on VSMs for those who are already familiar with the area, and to provide pointers into the literature for those who are less familiar with the ﬁeld.},
	language = {en},
	journal = {Journal of Artificial Intelligence Research},
	author = {Turney, Peter D and Pantel, Patrick},
	year = {2010},
	pages = {141--188},
	file = {Turney y Pantel - From Frequency to Meaning Vector Space Models of .pdf:C\:\\Users\\u0118974\\Zotero\\storage\\K6IMW9S3\\Turney y Pantel - From Frequency to Meaning Vector Space Models of .pdf:application/pdf},
}

@inproceedings{heylen.etal_2012,
	address = {Avignon},
	title = {Looking at word meaning. {An} interactive visualization of {Semantic} {Vector} {Spaces} for {Dutch} synsets},
	abstract = {In statistical NLP, Semantic Vector Spaces (SVS) are the standard technique for the automatic modeling of lexical semantics. However, it is largely unclear how these black-box techniques exactly capture word meaning. To explore the way an SVS structures the individual occurrences of words, we use a non-parametric MDS solution of a token-by-token similarity matrix. The MDS solution is visualized in an interactive plot with the Google Chart Tools. As a case study, we look at the occurrences of 476 Dutch nouns grouped in 214 synsets.},
	language = {en},
	booktitle = {Proceedings of the eacl 2012 {Joint} {Workshop} of {LINGVIS} \& {UNCLH}},
	author = {Heylen, Kris and Speelman, Dirk and Geeraerts, Dirk},
	year = {2012},
	pages = {16--24},
	file = {Heylen et al. - Looking at word meaning. An interactive visualizat.pdf:C\:\\Users\\u0118974\\Zotero\\storage\\UU62B9N7\\Heylen et al. - Looking at word meaning. An interactive visualizat.pdf:application/pdf},
}

@article{hilpert.correiasaavedra_2017,
	title = {Using token-based semantic vector spaces for corpus-linguistic analyses: {From} practical applications to tests of theoretical claims},
	volume = {16},
	issn = {1613-7027, 1613-7035},
	shorttitle = {Using token-based semantic vector spaces for corpus-linguistic analyses},
	url = {http://www.degruyter.com/view/j/cllt.ahead-of-print/cllt-2017-0009/cllt-2017-0009.xml},
	doi = {10.1515/cllt-2017-0009},
	abstract = {This paper presents token-based semantic vector spaces as a tool that can be applied in corpus-linguistic analyses such as word sense comparisons, comparisons of synonymous lexical items, and matching of concordance lines with a given text. We demonstrate how token-based semantic vector spaces are created, and we illustrate the kinds of result that can be obtained with this approach. Our main argument is that token-based semantic vector spaces are not only useful for practical corpus-linguistic applications but also for the investigation of theory-driven questions. We illustrate this point with a discussion of the asymmetric priming hypothesis (Jäger and Rosenbach 2008). The asymmetric priming hypothesis, which states that grammaticalizing constructions will be primed by their lexical sources but not vice versa, makes a number of empirically testable predictions. We operationalize and test these predictions, concluding that token-based semantic vector spaces yield conclusions that are relevant for linguistic theory-building.},
	language = {en},
	number = {2},
	urldate = {2018-11-20},
	journal = {Corpus Linguistics and Linguistic Theory},
	author = {Hilpert, Martin and Correia Saavedra, David},
	month = sep,
	year = {2017},
	file = {Hilpert y Correia Saavedra - 2017 - Using token-based semantic vector spaces for corpu.pdf:C\:\\Users\\u0118974\\Zotero\\storage\\RKE3RQAB\\Hilpert y Correia Saavedra - 2017 - Using token-based semantic vector spaces for corpu.pdf:application/pdf},
}

@book{levshina_2015,
	address = {Amsterdam ; Philadelphia},
	title = {How to do linguistics with {R}: data exploration and statistical analysis},
	isbn = {978-90-272-1224-5 978-90-272-1225-2},
	shorttitle = {How to do linguistics with {R}},
	publisher = {John Benjamins Publishing Company},
	author = {Levshina, Natalia},
	year = {2015},
	keywords = {Computational linguistics, Linguistics, Methodology, Statistical methods, Software},
}

@phdthesis{depascale_2019,
	address = {Leuven},
	type = {{PhD} {Dissertation}},
	title = {Token-based vector space models as semantic control in lexical lectometry},
	url = {https://lirias.kuleuven.be/retrieve/549451},
	abstract = {Type-based distributional semantics as embodied in vector space models has proven to be a successful method for the retrieval of near-synonyms in large corpora. These words have then been used as variants of lexical sociolinguistic variables (e.g.: return and winst for the concept profit) in lectometric studies, that is, the aggregate-level study of lexical distances between linguistic varieties, in particular pluricentric languages such as Dutch. However, a limitation of type-based vector space models is that all senses of a word are lumped together into one vector representation, making it harder to control for polysemy and subtle contextual distinctions. In addition, operating at the lexeme level, these type-based vector space models are not able to pick out the relevant corpus occurrences that are the input for the lectometric distance calculations. The main goal of this PhD project is to introduce token-based vector space models in lexical lectometric research, in order to gain better semantic control during the composition of lexical variables. Token-based vector space models address the abovementioned shortcomings by disambiguating different senses of lexical variants. This technique is able to model the semantics of individual tokens (i.e. 'usage occurrences') of a word in a corpus and to represent them as token clouds in multidimensional vector space, with clusters of tokens revealing distinct senses of the word. By superimposing the token clouds of the lexical variants, one can distinguish which meanings are shared by near-synonyms and determine the 'semantic envelope of variation' of the lexical alternation. For instance, the variant return is polysemous in Netherlandic Dutch, with the two readings 'profit' and 'return game', but not in Belgian Dutch, where only the 'profit' sense is found. By isolating the cluster of tokens with the meaning 'profit' one can identify the near-synonymous tokens of the variants winst and return. The fine-tuning of vector space model-based lectometry targeted in this PhD contributes to the scaling up of lexical variationist research, by providing methods for dealing with corpora whose size exceeds manual analysis. At the same time, token-based models comply with the need of detailed analysis by allowing to zoom in on the behavior of individual tokens in order to determine more subtle contextual distinctions. This PhD project is situated in a larger research endeavor ("Nephological Semantics - Using token clouds for meaning detection in variationist linguistics", BOF C1 project 3H150305) that aims at detailed understanding of token-based vector representations for lexical, semantic and variational research.},
	language = {eng},
	urldate = {2019-11-08},
	school = {KU Leuven},
	author = {De Pascale, S.},
	collaborator = {Marzo, S. and Speelman, D.},
	year = {2019},
}

@book{geeraerts.etal_1994,
	address = {Berlin, New York},
	title = {The {Structure} of {Lexical} {Variation}: {Meaning}, {Naming}, and {Context}},
	isbn = {978-3-11-087306-1},
	shorttitle = {The {Structure} of {Lexical} {Variation}},
	url = {https://www.degruyter.com/doi/10.1515/9783110873061},
	urldate = {2020-06-04},
	publisher = {DE GRUYTER MOUTON},
	author = {Geeraerts, Dirk and Grondelaers, Stefan and Bakema, Peter},
	month = jan,
	year = {1994},
	doi = {10.1515/9783110873061},
	file = {Geeraerts et al. - 1994 - The Structure of Lexical Variation Meaning, Namin.pdf:C\:\\Users\\u0118974\\Documents\\Bibliography\\Geeraerts et al. - 1994 - The Structure of Lexical Variation Meaning, Namin.pdf:application/pdf},
}

@incollection{stubbs_2015,
	address = {London},
	edition = {1},
	title = {Corpus semantics},
	isbn = {978-1-315-68553-3},
	url = {https://www.taylorfrancis.com/books/9781315685533},
	urldate = {2020-06-05},
	booktitle = {The {Routledge} {Handbook} of {Semantics}},
	publisher = {Routledge},
	author = {Stubbs, Michael},
	editor = {Riemer, Nick},
	month = jul,
	year = {2015},
	doi = {10.4324/9781315685533},
	pages = {106--122?},
}

@incollection{newman_2015,
	address = {London},
	edition = {1},
	title = {Semantic shift},
	isbn = {978-1-315-68553-3},
	url = {https://www.taylorfrancis.com/books/9781315685533},
	urldate = {2020-06-05},
	booktitle = {The {Routledge} {Handbook} of {Semantics}},
	publisher = {Routledge},
	author = {Newman, John},
	editor = {Riemer, Nick},
	month = jul,
	year = {2015},
	doi = {10.4324/9781315685533},
	pages = {266--280?},
}

@book{card.etal_1999,
	address = {San Francisco, Calif},
	series = {The {Morgan} {Kaufmann} series in interactive technologies},
	title = {Readings in information visualization: using vision to think},
	isbn = {978-1-55860-533-6},
	shorttitle = {Readings in information visualization},
	publisher = {Morgan Kaufmann Publishers},
	author = {Card, Stuart K. and Mackinlay, Jock D. and Shneiderman, Ben},
	year = {1999},
	keywords = {Information visualization, Computer graphics, Image processing},
}

@inproceedings{shneiderman_1996,
	title = {The {Eyes} {Have} {It}: {A} {Task} by {Data} {Type} {Taxonomy} for {Information} {Visualizations}},
	booktitle = {{IEEE} {Visual} {Languages}},
	author = {Shneiderman, Ben},
	year = {1996},
	pages = {96--13},
}

@incollection{wielfaert.etal_2019,
	address = {Stanford, California},
	series = {{CSLI} lecture notes},
	title = {Visual {Analytics} for {Parameter} {Tuning} of {Semantic} {Vector} {Space} {Models}},
	isbn = {978-1-68400-034-0 978-1-68400-033-3 978-1-68400-035-7},
	number = {no. 220},
	booktitle = {{LingVis}: visual analytics for linguistics},
	publisher = {CSLI Publications, Center for the Study of Language and Information},
	author = {Wielfaert, Thomas and Heylen, Kris and Speelman, Dirk and Geeraerts, Dirk},
	editor = {Butt, Miriam and Hautli-Janisz, Annette and Lyding, Verena},
	year = {2019},
	keywords = {Linguistics, Information visualization, LANGUAGE ARTS \& DISCIPLINES / Linguistics / General, Visual analytics},
	pages = {215--245},
}

@article{newman_2011,
	title = {Corpora and cognitive linguistics},
	volume = {11},
	issn = {1984-6398},
	url = {http://www.scielo.br/scielo.php?script=sci_arttext&pid=S1984-63982011000200010&lng=en&tlng=en},
	doi = {10.1590/S1984-63982011000200010},
	abstract = {Corpora are a natural source of data for cognitive linguists, since corpora, more than any other source of data, reflect "usage" - a notion which is often claimed to be of critical importance to the field of cognitive linguistics. Corpora are relevant to all the main topics of interest in cognitive linguistics: metaphor, polysemy, synonymy, prototypes, and constructional analysis. I consider each of these topics in turn and offer suggestions about which methods of analysis can be profitably used with available corpora to explore these topics further. In addition, I consider how the design and content of currently used corpora need to be rethought if corpora are to provide all the types of usage data that cognitive linguists require.
          , 
            Corpora são uma fonte natural de dados para a linguística cognitiva, uma vez que, estes, mais que qualquer outra fonte de dados, refletem "o uso" - a noção que é frequentemente apontada como tendo importância crítica para o campo da linguística cognitiva. Corpora são relevantes para todos os principais tópicos de interesse da linguística cognitiva: metáfora, polissemia, sinonímia, protótipos e análise construcional. Neste artigo, considerarei cada um desses tópicos e oferecerei sugestões sobre quais os métodos de análise podem ser utilizados com os corpora disponíveis para melhor se explorarem esses tópicos. Adicionalmente, discuto como a arquitetura e o conteúdo dos corpora atualmente disponíveis necessitam ser repensados se pretenderem oferecer todos os tipos de dados de uso necessários às análises da linguística cognitiva.},
	number = {2},
	urldate = {2020-09-22},
	journal = {Revista Brasileira de Linguística Aplicada},
	author = {Newman, John},
	year = {2011},
	pages = {521--559},
	file = {Texto completo:C\:\\Users\\u0118974\\Zotero\\storage\\2RGIUC35\\Newman - 2011 - Corpora and cognitive linguistics.pdf:application/pdf},
}

@inproceedings{smilkov.etal_2016,
	title = {Embedding {Projector}: {Interactive} {Visualization} and {Interpretation} of {Embeddings}},
	url = {arXiv:1611.05469},
	author = {Smilkov, Daniel and Thorat, Nikhil and Nicholson, Charles and Reif, Emily and Viégas, Fernanda B. and Wattenberg, Martin},
	year = {2016},
	note = {\_eprint: 1611.05469},
}

@inproceedings{lapesa.evert_2013,
	address = {Sofia, Bulgaria},
	title = {Evaluating {Neighbor} {Rank} and {Distance} {Measures} as {Predictors} of {Semantic} {Priming}},
	url = {https://www.aclweb.org/anthology/W13-2608},
	urldate = {2020-11-10},
	booktitle = {Proceedings of the {Fourth} {Annual} {Workshop} on {Cognitive} {Modeling} and {Computational} {Linguistics} ({CMCL})},
	publisher = {Association for Computational Linguistics},
	author = {Lapesa, Gabriella and Evert, Stefan},
	month = aug,
	year = {2013},
	pages = {66--74},
	file = {Full Text PDF:C\:\\Users\\u0118974\\Zotero\\storage\\8GBI63C4\\Lapesa y Evert - 2013 - Evaluating Neighbor Rank and Distance Measures as .pdf:application/pdf},
}

@article{hilpert.flach_2020,
	title = {Disentangling modal meanings with distributional semantics},
	issn = {2055-7671},
	url = {https://doi.org/10.1093/llc/fqaa014},
	doi = {10.1093/llc/fqaa014},
	abstract = {This article investigates the collocational behavior of English modal auxiliaries such as may and might with the aim of finding corpus-based measures that distinguish between different modal expressions and that allow insights into why speakers may choose one over another in a given context. The analysis uses token-based semantic vector space modeling (Heylen et al., 2015, Monitoring polysemy. Word space models as a tool for large-scale lexical semantic analysis. Lingua, 157: 153–72; Hilpert and Correia Saavedra, 2017, Using token-based semantic vector spaces for corpus-linguistic analyses: From practical applications to tests of theoretical claims. Corpus Linguistics and Linguistic Theory) in order to determine whether different modal auxiliaries can be distinguished in terms of their collocational profiles. The analysis further examines whether different senses of the same auxiliary exhibit divergent collocational preferences. The results indicate that near-synonymous pairs of modal expressions, such as may and might or must and have to, differ in their distributional characteristics. Also, different senses of the same modal expression, such as deontic and epistemic uses of may, can be distinguished on the basis of distributional information. We discuss these results against the background of previous empirical findings (Hilpert, 2016, Construction Grammar and its Application to English, 2nd edn. Edinburgh: Edinburgh University Press, Flach, in press, Beyond modal idioms and modal harmony: a corpus-based analysis of gradient idiomaticity in modal-adverb collocations. English Language and Linguistics) and theoretical issues such as degrees of grammaticalization (Correia Saavedra, 2019, Measurements of Grammaticalization: Developing a Quantitative Index for the Study of Grammatical Change. PhD Dissertation, Université de Neuchâtel) and the avoidance of synonymy (Bolinger, 1968, Entailment and the meaning of structures. Glossa, 2(2): 119–27).},
	number = {fqaa014},
	urldate = {2020-12-08},
	journal = {Digital Scholarship in the Humanities},
	author = {Hilpert, Martin and Flach, Susanne},
	month = may,
	year = {2020},
	file = {Full Text PDF:C\:\\Users\\u0118974\\Zotero\\storage\\PFI628UU\\Hilpert en Flach - 2020 - Disentangling modal meanings with distributional s.pdf:application/pdf;Snapshot:C\:\\Users\\u0118974\\Zotero\\storage\\YXNIE5BL\\5836757.html:text/html},
}

@article{perek_2018,
	title = {Recent change in the productivity and schematicity of the way-construction: {A} distributional semantic analysis},
	volume = {14},
	issn = {1613-7027, 1613-7035},
	shorttitle = {Recent change in the productivity and schematicity of the way-construction},
	url = {http://www.degruyter.com/view/journals/cllt/14/1/article-p65.xml},
	doi = {10.1515/cllt-2016-0014},
	abstract = {{\textless}section class="abstract"{\textgreater}{\textless}h2 class="abstractTitle text-title my-1" id="d1175e2"{\textgreater}Abstract{\textless}/h2{\textgreater}{\textless}p{\textgreater}This paper presents a corpus-based study of recent change in the English {\textless}em{\textgreater}way{\textless}/em{\textgreater}-construction, drawing on data from the 1830s to the 2000s. Semantic change in the distribution of the construction is characterized by means of a distributional semantic model, which captures semantic similarity between verbs through their co-occurrence frequency with other words in the corpus. By plotting and comparing the semantic domain of the three senses of the construction at different points in time, it is found that they all have gained in semantic diversity. These findings are interpreted in terms of increases in schematicity, either of the verb slot or the motion component contributed by the construction.{\textless}/p{\textgreater}{\textless}/section{\textgreater}},
	language = {en},
	number = {1},
	urldate = {2020-12-08},
	journal = {Corpus Linguistics and Linguistic Theory},
	author = {Perek, Florent},
	month = apr,
	year = {2018},
	note = {Publisher: De Gruyter Mouton
Section: Corpus Linguistics and Linguistic Theory},
	pages = {65--97},
	file = {Full Text PDF:C\:\\Users\\u0118974\\Zotero\\storage\\5VLYNBJI\\Perek - 2018 - Recent change in the productivity and schematicity.pdf:application/pdf;Snapshot:C\:\\Users\\u0118974\\Zotero\\storage\\N27TPI3A\\article-p65.html:text/html},
}

@article{perek_2016,
	title = {Using distributional semantics to study syntactic productivity in diachrony: {A} case study},
	volume = {54},
	issn = {0024-3949, 1613-396X},
	shorttitle = {Using distributional semantics to study syntactic productivity in diachrony},
	url = {http://www.degruyter.com/view/journals/ling/54/1/article-p149.xml},
	doi = {10.1515/ling-2015-0043},
	abstract = {{\textless}section class="abstract"{\textgreater}{\textless}h2 class="abstractTitle text-title my-1" id="d1015e2"{\textgreater}Abstract{\textless}/h2{\textgreater}{\textless}p{\textgreater}This paper investigates syntactic productivity in diachrony with a data-driven approach. Previous research indicates that syntactic productivity (the property of grammatical constructions to attract new lexical fillers) is largely driven by semantics, which calls for an operationalization of lexical meaning in the context of empirical studies. It is suggested that distributional semantics can fulfill this role by providing a measure of semantic similarity between words that is derived from lexical co-occurrences in large text corpora. On the basis of a case study of the construction “V {\textless}em{\textgreater}the hell out of{\textless}/em{\textgreater} NP”, e.g., {\textless}em{\textgreater}You scared the hell out of me{\textless}/em{\textgreater}, it is shown that distributional semantics not only appropriately captures how the verbs in the distribution of the construction are related, but also enables the use of visualization techniques and statistical modeling to analyze the semantic development of a construction over time and identify the determinants of syntactic productivity in naturally occurring data.{\textless}/p{\textgreater}{\textless}/section{\textgreater}},
	language = {en},
	number = {1},
	urldate = {2020-12-22},
	journal = {Linguistics},
	author = {Perek, Florent},
	month = jan,
	year = {2016},
	note = {Publisher: De Gruyter Mouton
Section: Linguistics},
	pages = {149--188},
	file = {Full Text PDF:C\:\\Users\\u0118974\\Zotero\\storage\\WBDDWTVV\\Perek - 2016 - Using distributional semantics to study syntactic .pdf:application/pdf;Snapshot:C\:\\Users\\u0118974\\Zotero\\storage\\PSNWLMMP\\article-p149.html:text/html},
}

@article{perek.hilpert_2017,
	title = {A distributional semantic approach to the periodization of change in the productivity of constructions},
	volume = {22},
	issn = {1384-6655, 1569-9811},
	url = {https://www.jbe-platform.com/content/journals/10.1075/ijcl.16128.per},
	doi = {10.1075/ijcl.16128.per},
	abstract = {This paper describes a method to automatically identify stages of language change in diachronic corpus data, combining variability-based neighbour clustering, which offers objective and reproducible criteria for periodization, and distributional semantics as a representation of lexical meaning. This method partitions the history of a grammatical construction according to qualitative stages of productivity corresponding to different semantic sets of lexical items attested in it. Two case studies are presented. The first case study on the hell-construction (“Verb the hell out of NP”) shows that the semantic development of a construction does not always match that of its quantitative aspects, like token or type frequency. The second case study on the way-construction compares the results of the present method with those of collostructional analysis. It is shown that the former measures semantic changes and their chronology with greater precision. In sum, this method offers a promising approach to exploring semantic variation in the lexical fillers of constructions and to modelling constructional change.},
	language = {en},
	number = {4},
	urldate = {2020-12-22},
	journal = {International Journal of Corpus Linguistics},
	author = {Perek, Florent and Hilpert, Martin},
	month = jan,
	year = {2017},
	note = {Publisher: John Benjamins},
	pages = {490--520},
	file = {Ingediende versie:C\:\\Users\\u0118974\\Zotero\\storage\\AYFCU3UI\\Perek en Hilpert - 2017 - A distributional semantic approach to the periodiz.pdf:application/pdf},
}

@incollection{stefanowitsch_2010,
	address = {Berlin ; New York},
	series = {Cognitive linguistics research},
	title = {Empirical cognitive semantics: {Some} thoughts},
	isbn = {978-3-11-022641-6},
	number = {46},
	booktitle = {Quantitative methods in cognitive semantics: corpus-driven approaches},
	publisher = {De Gruyter Mouton},
	author = {Stefanowitsch, Anatol},
	editor = {Glynn, Dylan and Fischer, Kerstin},
	year = {2010},
	note = {OCLC: ocn664354356},
	keywords = {Cognitive grammar, Computational linguistics, Semantics, Kongress},
	pages = {355--380},
}

@incollection{glynn_2010,
	address = {Berlin ; New York},
	series = {Cognitive linguistics research},
	title = {Corpus-driven {Cognitive} {Semantics}. {Introduction} to the field},
	isbn = {978-3-11-022641-6},
	number = {46},
	booktitle = {Quantitative methods in cognitive semantics: corpus-driven approaches},
	publisher = {De Gruyter Mouton},
	author = {Glynn, Dylan},
	editor = {Glynn, Dylan and Fischer, Kerstin},
	year = {2010},
	note = {OCLC: ocn664354356},
	keywords = {Cognitive grammar, Computational linguistics, Semantics, Kongress},
	pages = {1--42*},
}

@incollection{koptjevskaja-tamm.sahlgren_2014,
	address = {Berlin, Boston},
	title = {Temperature in the word space: {Sense} exploration of temperature expressions using word-space modelling},
	isbn = {978-3-11-031755-8},
	shorttitle = {Temperature in the word space},
	url = {https://www.degruyter.com/view/books/9783110317558/9783110317558.231/9783110317558.231.xml},
	urldate = {2020-12-25},
	booktitle = {Aggregating {Dialectology}, {Typology}, and {Register} {Analysis}},
	publisher = {DE GRUYTER},
	author = {Koptjevskaja-Tamm, Maria and Sahlgren, Magnus},
	editor = {Szmrecsanyi, Benedikt and Wälchli, Bernhard},
	month = jan,
	year = {2014},
	doi = {10.1515/9783110317558.231},
	pages = {231--267},
	file = {Koptjevskaja-Tamm en Sahlgren - 2014 - Temperature in the word space Sense exploration o.pdf:C\:\\Users\\u0118974\\Zotero\\storage\\5YCM5DQ7\\Koptjevskaja-Tamm en Sahlgren - 2014 - Temperature in the word space Sense exploration o.pdf:application/pdf},
}

@incollection{montes.heylen_Submitted,
	title = {Visualizing {Distributional} {Semantics}},
	copyright = {All rights reserved},
	isbn = {978-3-11-068734-7},
	language = {en},
	publisher = {Mouton De Gruyter},
	author = {Montes, Mariana and Heylen, Kris},
	editor = {Tay, Dennis and Pan, Molly Xie},
}

@book{bolognesi_2020,
	address = {Amsterdam},
	series = {Converging {Evidence} in {Language} and {Communication} {Research}},
	title = {Where {Words} {Get} their {Meaning}: {Cognitive} processing and distributional modelling of word meaning in first and second language},
	isbn = {978-90-272-0801-9 978-90-272-6042-0},
	shorttitle = {Where {Words} {Get} their {Meaning}},
	url = {http://www.jbe-platform.com/content/books/9789027260420},
	language = {en},
	urldate = {2021-04-27},
	publisher = {John Benjamins Publishing Company},
	author = {Bolognesi, Marianna},
	month = nov,
	year = {2020},
	doi = {10.1075/celcr.23},
}

@phdthesis{sahlgren_2006,
	address = {Stockholm},
	title = {The word-space model: using distributional analysis to represent syntagmatic and paradigmatic relations between words in high-dimensional vector spaces},
	shorttitle = {The word-space model},
	language = {en},
	school = {Dep. of Linguistics, Stockholm Univ. [u.a.]},
	author = {Sahlgren, Magnus},
	year = {2006},
	note = {OCLC: 255579201},
	file = {Sahlgren - 2006 - The word-space model using distributional analysi.pdf:C\:\\Users\\u0118974\\Zotero\\storage\\XJFN3AZD\\Sahlgren - 2006 - The word-space model using distributional analysi.pdf:application/pdf},
}

@article{BERT,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2021-04-27},
	journal = {arXiv:1810.04805 [cs]},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	note = {arXiv: 1810.04805},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\u0118974\\Zotero\\storage\\JPIUWCYQ\\Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\u0118974\\Zotero\\storage\\85K98SLY\\1810.html:text/html},
}

@misc{montes.qlvl_2021,
	title = {{QLVL}/{NephoVis}: {Stratocumulus}},
	copyright = {Open Access},
	shorttitle = {{QLVL}/{NephoVis}},
	url = {https://zenodo.org/record/4726926},
	abstract = {Relatively viable version, especially for levels 1 and 2.},
	urldate = {2021-04-29},
	publisher = {Zenodo},
	author = {Montes, Mariana and QLVL},
	month = apr,
	year = {2021},
	doi = {10.5281/ZENODO.4726926},
}

@article{harris_1954,
	title = {Distributional structure},
	volume = {10},
	number = {2-3},
	journal = {Word},
	author = {Harris, Zellig S.},
	year = {1954},
	note = {Publisher: Taylor \& Francis},
	pages = {146--162},
}

@inproceedings{campello.etal_2013,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Density-{Based} {Clustering} {Based} on {Hierarchical} {Density} {Estimates}},
	isbn = {978-3-642-37456-2},
	doi = {10.1007/978-3-642-37456-2_14},
	abstract = {We propose a theoretically and practically improved density-based, hierarchical clustering method, providing a clustering hierarchy from which a simplified tree of significant clusters can be constructed. For obtaining a “flat” partition consisting of only the most significant clusters (possibly corresponding to different density thresholds), we propose a novel cluster stability measure, formalize the problem of maximizing the overall stability of selected clusters, and formulate an algorithm that computes an optimal solution to this problem. We demonstrate that our approach outperforms the current, state-of-the-art, density-based clustering methods on a wide variety of real world data.},
	language = {en},
	booktitle = {Advances in {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {Springer},
	author = {Campello, Ricardo J. G. B. and Moulavi, Davoud and Sander, Joerg},
	editor = {Pei, Jian and Tseng, Vincent S. and Cao, Longbing and Motoda, Hiroshi and Xu, Guandong},
	year = {2013},
	keywords = {Cluster Tree, Core Object, Density Threshold, Hierarchical Cluster Method, Minimum Span Tree},
	pages = {160--172},
	file = {Springer Full Text PDF:C\:\\Users\\u0118974\\Zotero\\storage\\S5V7562X\\Campello et al. - 2013 - Density-Based Clustering Based on Hierarchical Den.pdf:application/pdf},
}

@misc{mcinnes.etal_2016,
	title = {How {HDBSCAN} {Works} — hdbscan 0.8.1 documentation},
	url = {https://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html},
	urldate = {2021-04-30},
	author = {McInnes, Leland and Healy, John and Astels, Steve},
	year = {2016},
	file = {How HDBSCAN Works — hdbscan 0.8.1 documentation:C\:\\Users\\u0118974\\Zotero\\storage\\3RQD77MC\\how_hdbscan_works.html:text/html},
}

@article{levy.etal_2015,
	title = {Improving {Distributional} {Similarity} with {Lessons} {Learned} from {Word} {Embeddings}},
	volume = {3},
	url = {https://www.aclweb.org/anthology/Q15-1016},
	doi = {10.1162/tacl_a_00134},
	abstract = {Recent trends suggest that neural-network-inspired word embedding models outperform traditional count-based distributional models on word similarity and analogy detection tasks. We reveal that much of the performance gains of word embeddings are due to certain system design choices and hyperparameter optimizations, rather than the embedding algorithms themselves. Furthermore, we show that these modifications can be transferred to traditional distributional models, yielding similar gains. In contrast to prior reports, we observe mostly local or insignificant performance differences between the methods, with no global advantage to any single approach over the others.},
	urldate = {2021-04-30},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Levy, Omer and Goldberg, Yoav and Dagan, Ido},
	year = {2015},
	pages = {211--225},
	file = {Full Text PDF:C\:\\Users\\u0118974\\Zotero\\storage\\WBH54C2Q\\Levy et al. - 2015 - Improving Distributional Similarity with Lessons L.pdf:application/pdf},
}

@article{lapesa.evert_2014,
	title = {A {Large} {Scale} {Evaluation} of {Distributional} {Semantic} {Models}: {Parameters}, {Interactions} and {Model} {Selection}},
	volume = {2},
	issn = {2307-387X},
	shorttitle = {A {Large} {Scale} {Evaluation} of {Distributional} {Semantic} {Models}},
	url = {https://doi.org/10.1162/tacl_a_00201},
	doi = {10.1162/tacl_a_00201},
	abstract = {This paper presents the results of a large-scale evaluation study of window-based
Distributional Semantic Models on a wide variety of tasks. Our study combines a
broad coverage of model parameters with a model selection methodology that is
robust to overfitting and able to capture parameter interactions. We show that
our strategy allows us to identify parameter configurations that achieve good
performance across different datasets and tasks.},
	urldate = {2021-04-30},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Lapesa, Gabriella and Evert, Stefan},
	month = dec,
	year = {2014},
	pages = {531--546},
	file = {Full Text PDF:C\:\\Users\\u0118974\\Zotero\\storage\\38LFFRYM\\Lapesa and Evert - 2014 - A Large Scale Evaluation of Distributional Semanti.pdf:application/pdf;Snapshot:C\:\\Users\\u0118974\\Zotero\\storage\\98QUEUFB\\A-Large-Scale-Evaluation-of-Distributional.html:text/html},
}

@article{devries.etal_2019,
	title = {{BERTje}: {A} {Dutch} {BERT} {Model}},
	shorttitle = {{BERTje}},
	url = {http://arxiv.org/abs/1912.09582},
	abstract = {The transformer-based pre-trained language model BERT has helped to improve state-of-the-art performance on many natural language processing (NLP) tasks. Using the same architecture and parameters, we developed and evaluated a monolingual Dutch BERT model called BERTje. Compared to the multilingual BERT model, which includes Dutch but is only based on Wikipedia text, BERTje is based on a large and diverse dataset of 2.4 billion tokens. BERTje consistently outperforms the equally-sized multilingual BERT model on downstream NLP tasks (part-of-speech tagging, named-entity recognition, semantic role labeling, and sentiment analysis). Our pre-trained Dutch BERT model is made available at https://github.com/wietsedv/bertje.},
	urldate = {2021-05-01},
	journal = {arXiv:1912.09582 [cs]},
	author = {de Vries, Wietse and van Cranenburgh, Andreas and Bisazza, Arianna and Caselli, Tommaso and van Noord, Gertjan and Nissim, Malvina},
	month = dec,
	year = {2019},
	note = {arXiv: 1912.09582},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\u0118974\\Zotero\\storage\\NMLQUI35\\de Vries et al. - 2019 - BERTje A Dutch BERT Model.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\u0118974\\Zotero\\storage\\IN2WYIBF\\1912.html:text/html},
}

@article{glynn_2016,
	title = {Quantifying polysemy: {Corpus} methodology for prototype theory},
	volume = {50},
	issn = {1614-7308},
	shorttitle = {Quantifying polysemy},
	url = {http://www.degruyter.com/document/doi/10.1515/flin-2016-0016/html},
	doi = {10.1515/flin-2016-0016},
	abstract = {This study addresses the methodological problem of result falsification in Cognitive Semantics, specifically in the descriptive analysis of semasiological variation, or “polysemy”. It argues that manually analysed corpus data can be used to describe models of semantic structure. The method proposed is quantified, permitting repeat analysis. The operationalisation of a semasiological structure employed in the study takes the principle of semantic features and applies them to a contextual analysis of usage-events, associated with the lexeme under scrutiny. The feature analysis, repeated on a large collection of occurrences, results in a set of metadata that constitutes the usage-profile of the lexeme. Multivariate statistics are then employed to identify patterns in those metadata. The case study examines 500 occurrences of the English lexeme annoy . Three basic senses are identified as well as a more complex array of semantic variations linked to morpho-syntactic context of usage.},
	language = {en},
	number = {2},
	urldate = {2021-05-01},
	journal = {Folia Linguistica},
	author = {Glynn, Dylan},
	month = nov,
	year = {2016},
	note = {Publisher: De Gruyter
Section: Folia Linguistica},
	pages = {413--447},
	file = {Snapshot:C\:\\Users\\u0118974\\Zotero\\storage\\XTYYGQ7B\\html.html:text/html},
}

@incollection{firth_1957a,
	address = {Oxford},
	series = {Special volume of the {Philological} {Society}},
	title = {A synopsis of linguistic theory 1930-1955},
	isbn = {978-0-631-11300-3},
	url = {http://cs.brown.edu/courses/csci2952d/readings/lecture1-firth.pdf},
	urldate = {2021-05-03},
	booktitle = {Studies in {Linguistic} {Analysis}},
	publisher = {Blackwell},
	author = {Firth, John Rupert},
	editor = {Firth, John Rupert},
	year = {1957},
	pages = {1--32},
	file = {lecture1-firth.pdf:C\:\\Users\\u0118974\\Zotero\\storage\\QXEJHCGN\\lecture1-firth.pdf:application/pdf},
}

@incollection{langacker_1988,
	address = {Amsterdam},
	title = {An overview of cognitive grammar},
	volume = {50},
	isbn = {978-90-272-3544-2 978-90-272-8619-2},
	url = {https://benjamins.com/catalog/cilt.50.03lan},
	language = {en},
	urldate = {2021-05-04},
	booktitle = {Current {Issues} in {Linguistic} {Theory}},
	publisher = {John Benjamins Publishing Company},
	author = {Langacker, Ronald W.},
	editor = {Rudzka-Ostyn, Brygida},
	year = {1988},
	doi = {10.1075/cilt.50.03lan},
	pages = {3--48},
}

@incollection{kaufman.rousseeuw_1990,
	address = {Hoboken, NJ, USA},
	series = {Wiley {Series} in {Probability} and {Statistics}},
	title = {Partitioning {Around} {Medoids} ({Program} {PAM})},
	isbn = {978-0-470-31680-1 978-0-471-87876-6},
	url = {http://doi.wiley.com/10.1002/9780470316801.ch2},
	language = {en},
	urldate = {2021-05-04},
	booktitle = {Finding {Groups} in {Data}: {An} {Introduction} to {Cluster} {Analysis}},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Kaufman, Leonard and Rousseeuw, Peter J.},
	collaborator = {Kaufman, Leonard and Rousseeuw, Peter J.},
	month = mar,
	year = {1990},
	doi = {10.1002/9780470316801.ch2},
	pages = {68--125},
}

@inproceedings{vieu.etal_2015,
	address = {Tübingen, Germany},
	title = {Quantitative methods for identifying systematic polysemy classes},
	abstract = {In this paper we report the results of four experiments conducted to extract lists of nouns that exhibit inherent polysemy from corpus data following semiautomatic and automatic procedures. We compare the methods used and the results obtained. We argue that quantitative methods can be used to distinguish different classes of polysemous nouns in the language on the basis of the variability of copredication contexts.},
	language = {en},
	author = {Vieu, Laure and Jezek, Elisabetta and di Pavia, Universita and de Cruys, Tim Van},
	year = {2015},
	file = {Vieu et al. - Quantitative methods for identifying systematic po.pdf:C\:\\Users\\u0118974\\Zotero\\storage\\AIUWXWIZ\\Vieu et al. - Quantitative methods for identifying systematic po.pdf:application/pdf},
}

@inproceedings{decruys.etal_2013,
	address = {Atlanta, Georgia, USA},
	title = {A {Tensor}-based {Factorization} {Model} of {Semantic} {Compositionality}},
	abstract = {In this paper, we present a novel method for the computation of compositionality within a distributional framework. The key idea is that compositionality is modeled as a multi-way interaction between latent factors, which are automatically constructed from corpus data. We use our method to model the composition of subject verb object triples. The method consists of two steps. First, we compute a latent factor model for nouns from standard co-occurrence data. Next, the latent factors are used to induce a latent model of three-way subject verb object interactions. Our model has been evaluated on a similarity task for transitive phrases, in which it exceeds the state of the art.},
	language = {en},
	booktitle = {Proceedings of {NAACL} 2013},
	author = {de Cruys, Tim Van and Poibeau, Thierry and Korhonen, Anna},
	year = {2013},
	pages = {1142--1151},
	file = {de Cruys et al. - A Tensor-based Factorization Model of Semantic Com.pdf:C\:\\Users\\u0118974\\Zotero\\storage\\G4KFAZBM\\de Cruys et al. - A Tensor-based Factorization Model of Semantic Com.pdf:application/pdf},
}

@inproceedings{decruys.etal_2011,
	address = {Edinburgh, Scotland, UK},
	title = {Latent {Vector} {Weighting} for {Word} {Meaning} in {Context}},
	abstract = {This paper presents a novel method for the computation of word meaning in context. We make use of a factorization model in which words, together with their window-based context words and their dependency relations, are linked to latent dimensions. The factorization model allows us to determine which dimensions are important for a particular context, and adapt the dependency-based feature vector of the word accordingly. The evaluation on a lexical substitution task – carried out for both English and French – indicates that our approach is able to reach better results than state-of-the-art methods in lexical substitution, while at the same time providing more accurate meaning representations.},
	language = {en},
	booktitle = {Proceedings of the 2011 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {de Cruys, Tim Van and Poibeau, Thierry and Korhonen, Anna},
	month = jul,
	year = {2011},
	pages = {1012--1022},
	file = {de Cruys et al. - Latent Vector Weighting for Word Meaning in Contex.pdf:C\:\\Users\\u0118974\\Zotero\\storage\\8R296ST5\\de Cruys et al. - Latent Vector Weighting for Word Meaning in Contex.pdf:application/pdf},
}

@inproceedings{decruys.apidianaki_2011,
	address = {Portland, Oregon},
	title = {Latent {Semantic} {Word} {Sense} {Induction} and {Disambiguation}},
	abstract = {In this paper, we present a uniﬁed model for the automatic induction of word senses from text, and the subsequent disambiguation of particular word instances using the automatically extracted sense inventory. The induction step and the disambiguation step are based on the same principle: words and contexts are mapped to a limited number of topical dimensions in a latent semantic word space. The intuition is that a particular sense is associated with a particular topic, so that different senses can be discriminated through their association with particular topical dimensions; in a similar vein, a particular instance of a word can be disambiguated by determining its most important topical dimensions. The model is evaluated on the SEMEVAL-2010 word sense induction and disambiguation task, on which it reaches stateof-the-art results.},
	language = {en},
	booktitle = {Proceedings of the 49th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {de Cruys, Tim Van and Apidianaki, Marianna},
	month = jun,
	year = {2011},
	pages = {1476--1485},
	file = {de Cruys and Apidianaki - Latent Semantic Word Sense Induction and Disambigu.pdf:C\:\\Users\\u0118974\\Zotero\\storage\\ABWQANAD\\de Cruys and Apidianaki - Latent Semantic Word Sense Induction and Disambigu.pdf:application/pdf},
}

@inproceedings{apidianaki.vandecruys_2011,
	address = {Tokyo, Japan},
	title = {A {Quantitative} {Evaluation} of {Global} {Word} {Sense} {Induction}},
	volume = {6608},
	isbn = {978-3-642-19399-6 978-3-642-19400-9},
	url = {http://link.springer.com/10.1007/978-3-642-19400-9_20},
	abstract = {Word sense induction (WSI) is the task aimed at automatically identifying the senses of words in texts, without the need for handcrafted resources or annotated data. Up till now, most WSI algorithms extract the diﬀerent senses of a word ‘locally’ on a per-word basis, i.e. the diﬀerent senses for each word are determined separately. In this paper, we compare the performance of such algorithms to an algorithm that uses a ‘global’ approach, i.e. the diﬀerent senses of a particular word are determined by comparing them to, and demarcating them from, the senses of other words in a full-blown word space model. We adopt the evaluation framework proposed in the SemEval-2010 Word Sense Induction \& Disambiguation task. All systems that participated in this task use a local scheme for determining the diﬀerent senses of a word. We compare their results to the ones obtained by the global approach, and discuss the advantages and weaknesses of both approaches.},
	language = {en},
	urldate = {2021-05-07},
	booktitle = {Computational {Linguistics} and {Intelligent} {Text} {Processing}},
	author = {Apidianaki, Marianna and Van de Cruys, Tim},
	editor = {Gelbukh, Alexander F.},
	year = {2011},
	doi = {10.1007/978-3-642-19400-9_20},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {253--264},
	file = {Apidianaki and Van de Cruys - 2011 - A Quantitative Evaluation of Global Word Sense Ind.pdf:C\:\\Users\\u0118974\\Zotero\\storage\\YPXMUNDX\\Apidianaki and Van de Cruys - 2011 - A Quantitative Evaluation of Global Word Sense Ind.pdf:application/pdf},
}

@book{jurafsky.martin_2020,
	edition = {3},
	title = {Speech and {Language} {Processing}},
	url = {https://web.stanford.edu/~jurafsky/slp3/ed3book_dec302020.pdf},
	urldate = {2021-05-07},
	author = {Jurafsky, Daniel and Martin, James H.},
	year = {2020},
	file = {Jurafsky_Martin_2020.pdf:C\:\\Users\\u0118974\\Documents\\Bibliography\\Jurafsky_Martin_2020.pdf:application/pdf},
}

@misc{oskolkov_2021,
	title = {How {Exactly} {UMAP} {Works}},
	url = {https://towardsdatascience.com/how-exactly-umap-works-13e3040e1668},
	abstract = {And why exactly it is better than tSNE},
	language = {en},
	urldate = {2021-05-07},
	journal = {Medium},
	author = {Oskolkov, Nikolay},
	month = mar,
	year = {2021},
	note = {Library Catalog: towardsdatascience.com},
	file = {Snapshot:C\:\\Users\\u0118974\\Zotero\\storage\\2RHABLHP\\how-exactly-umap-works-13e3040e1668.html:text/html},
}

@article{mcinnes.etal_2020,
	title = {{UMAP}: {Uniform} {Manifold} {Approximation} and {Projection} for {Dimension} {Reduction}},
	shorttitle = {{UMAP}},
	url = {http://arxiv.org/abs/1802.03426},
	abstract = {UMAP (Uniform Manifold Approximation and Projection) is a novel manifold learning technique for dimension reduction. UMAP is constructed from a theoretical framework based in Riemannian geometry and algebraic topology. The result is a practical scalable algorithm that applies to real world data. The UMAP algorithm is competitive with t-SNE for visualization quality, and arguably preserves more of the global structure with superior run time performance. Furthermore, UMAP has no computational restrictions on embedding dimension, making it viable as a general purpose dimension reduction technique for machine learning.},
	urldate = {2021-05-07},
	journal = {arXiv:1802.03426 [cs, stat]},
	author = {McInnes, Leland and Healy, John and Melville, James},
	month = sep,
	year = {2020},
	note = {arXiv: 1802.03426},
	keywords = {Statistics - Machine Learning, Computer Science - Computational Geometry, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\u0118974\\Zotero\\storage\\BMKPHHQG\\McInnes et al. - 2020 - UMAP Uniform Manifold Approximation and Projectio.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\u0118974\\Zotero\\storage\\636HTZJW\\1802.html:text/html},
}

@inproceedings{vandecruys_2008,
	address = {Hamburg, Germany},
	title = {A {Comparison} of {Bag} of {Words} and {Syntax}-based {Approaches} for {Word} {Categorization}},
	booktitle = {Lexical semantics: bridging the gap between semantic theory and computational simulations},
	author = {Van De Cruys, Tim},
	editor = {Baroni, Marco and Evert, Stefan and Lenci, Alessandro},
	year = {2008},
	pages = {47--54},
}

@article{sahlgren_2008,
	title = {The distributional hypothesis},
	volume = {20},
	issn = {2499-8125},
	url = {http://www.italian-journal-linguistics.com/italian-journal-of-linguistics-2008/},
	language = {en},
	number = {1},
	urldate = {2021-05-11},
	journal = {Italian Journal of Linguistics},
	author = {Sahlgren, Magnus},
	year = {2008},
	pages = {33--53},
	file = {Sahlgren - The distributional hypothesis.pdf:C\:\\Users\\u0118974\\Zotero\\storage\\I267JSGY\\Sahlgren - The distributional hypothesis.pdf:application/pdf},
}

@book{geeraerts_1997,
	address = {Oxford : New York},
	series = {Oxford studies in lexicography and lexicology},
	title = {Diachronic prototype semantics: a contribution to historical lexicology},
	isbn = {978-0-19-823652-8},
	shorttitle = {Diachronic prototype semantics},
	publisher = {Clarendon Press ; Oxford University Press},
	author = {Geeraerts, Dirk},
	year = {1997},
	keywords = {Cognitive grammar, Linguistic change, Semantics, Historical},
}

@incollection{evert_2009,
	address = {Berlin, New York},
	series = {Handbooks of {Linguistics} and {Communication} {Science}},
	title = {58. {Corpora} and collocations},
	volume = {2},
	isbn = {978-3-11-021388-1},
	url = {https://www.degruyter.com/document/doi/10.1515/9783110213881.2.1212/html},
	urldate = {2021-05-11},
	booktitle = {Corpus {Linguistics}. {An} {International} {Handbook}},
	publisher = {Mouton de Gruyter},
	author = {Evert, Stefan},
	editor = {Lüdeling, Anke and Kytö, Merja},
	month = mar,
	year = {2009},
	doi = {10.1515/9783110213881.2.1212},
	pages = {1212--1248},
}

@article{kilgarriff_1997,
	title = {"{I} don't believe in word senses"},
	volume = {31},
	url = {https://www.degruyter.com/document/doi/10.1515/9783110895698.361/html},
	abstract = {Word sense disambiguation assumes word senses. Within the lexicography and lingu literature, they are known to be very slippery entities. The paper looks at problems with ex accounts of 'word sense' and describes the various kinds of ways in which a word's meani deviate from its core meaning. An analysis is presented in which word senses are abstraction clusters of corpus citations, in accordance with current lexicographic practice. The corpus cit not the word senses, are the basic objects in the ontology. The corpus citations will be cluster senses according to the purposes of whoever or whatever does the clustering. In the absence purposes, word senses do not exist. Word sense disambiguation also needs a set of word senses to disambiguate between. In recent work, the set has been taken from a general-purpose lexical resource, with the assu that the lexical resource describes the word senses of English/French/..., between whi applications will need to disambiguate. The implication of the paper is, by contrast, that word exist only relative to a task.},
	language = {en},
	number = {2},
	urldate = {2021-05-11},
	journal = {Computers and the Humanities},
	author = {Kilgarriff, Adam},
	year = {1997},
	doi = {10.1515/9783110895698.361},
	pages = {91--113},
	file = {Kilgarriff - 2003 - I don't believe in word senses.pdf:C\:\\Users\\u0118974\\Zotero\\storage\\K85FHZWU\\Kilgarriff - 2003 - I don't believe in word senses.pdf:application/pdf},
}

@article{geeraerts_2016a,
	title = {The sociosemiotic commitment},
	volume = {27},
	issn = {0936-5907, 1613-3641},
	url = {https://www.degruyter.com/document/doi/10.1515/cog-2016-0058/html},
	doi = {10.1515/cog-2016-0058},
	abstract = {Abstract
            Cognitive Linguistics should complement the Cognitive Commitment with a Sociosemiotic Commitment: a commitment to make one’s account of human language accord with the status of language as a social semiotic, i. e., as an intersubjective, historically and socially variable tool, and to base that account on a methodology that likewise transcends the individual. By looking at defining features of Cognitive Linguistics (its cognitive orientation, and its usage-based character), it is argued that the relevance of the Sociosemiotic Commitment derives from the very essence of Cognitive Linguistics.},
	number = {4},
	urldate = {2021-05-25},
	journal = {Cognitive Linguistics},
	author = {Geeraerts, Dirk},
	month = nov,
	year = {2016},
	pages = {527--542},
}

@incollection{geeraerts_1999,
	address = {Berlin},
	title = {Idealist and empiricist tendencies in {Cognitive} {Linguistics}},
	booktitle = {Cognitive {Linguistics}: {Foundations}, {Scope}, and {Methodology}},
	publisher = {Mouton de Gruyter},
	author = {Geeraerts, Dirk},
	editor = {Janssen, Theo and Redeker, Gisela},
	year = {1999},
	pages = {163--194},
}

@incollection{sinclair_1998,
	address = {Amsterdam},
	series = {Amsterdam studies in the theory and history of linguistic science {Series} 4, {Current} issues in linguistic theory},
	title = {The {Lexical} {Item}},
	isbn = {978-90-272-3676-0 978-1-55619-887-8},
	language = {eng},
	number = {171},
	booktitle = {Contrastive lexical semantics},
	publisher = {Benjamins},
	author = {Sinclair, John},
	editor = {Weigand, Edda},
	year = {1998},
	pages = {1--24},
}

@article{stubbs_2009,
	title = {Memorial {Article}: {John} {Sinclair} (1933–2007)},
	volume = {30},
	issn = {1477-450X, 0142-6001},
	shorttitle = {Memorial {Article}},
	url = {https://academic.oup.com/applij/article-lookup/doi/10.1093/applin/amn052},
	doi = {10.1093/applin/amn052},
	language = {en},
	number = {1},
	urldate = {2021-06-30},
	journal = {Applied Linguistics},
	author = {Stubbs, Michael},
	month = mar,
	year = {2009},
	pages = {115--137},
}
