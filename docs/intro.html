<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 1 Introduction | Cloudspotting</title>
<meta name="author" content="Mariana Montes">
<meta name="description" content="If meaning is found and created in use, and corpora are language in use, can we find meaning in corpora? The field of usage-based semantics is large and rich, so the answer to this question is...">
<meta name="generator" content="bookdown 0.24 with bs4_book()">
<meta property="og:title" content="Chapter 1 Introduction | Cloudspotting">
<meta property="og:type" content="book">
<meta property="og:image" content="/assets/covers/front-cover.png">
<meta property="og:description" content="If meaning is found and created in use, and corpora are language in use, can we find meaning in corpora? The field of usage-based semantics is large and rich, so the answer to this question is...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 1 Introduction | Cloudspotting">
<meta name="twitter:description" content="If meaning is found and created in use, and corpora are language in use, can we find meaning in corpora? The field of usage-based semantics is large and rich, so the answer to this question is...">
<meta name="twitter:image" content="/assets/covers/front-cover.png">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/header-attrs-2.10/header-attrs.js"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.3.0/transition.js"></script><script src="libs/bs3compat-0.3.0/tabs.js"></script><script src="libs/bs3compat-0.3.0/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/kePrint-0.0.1/kePrint.js"></script><link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><link rel="stylesheet" href="assets/style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="Visual analytics for distributional semantics">Cloudspotting</a>:
        <small class="text-muted">Visual analytics for distributional semantics</small>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Preface</a></li>
<li><a class="" href="acknowledgements.html">Acknowledgements</a></li>
<li><a class="active" href="intro.html"><span class="header-section-number">1</span> Introduction</a></li>
<li class="book-part">The cloudspotter’s toolkit</li>
<li><a class="" href="workflow.html"><span class="header-section-number">2</span> From corpora to clouds</a></li>
<li><a class="" href="nephovis.html"><span class="header-section-number">3</span> Visualization tools</a></li>
<li><a class="" href="dataset.html"><span class="header-section-number">4</span> Case studies</a></li>
<li class="book-part">The cloudspotter’s handbook</li>
<li><a class="" href="shapes.html"><span class="header-section-number">5</span> A cloud atlas</a></li>
<li><a class="" href="semantic-interpretation.html"><span class="header-section-number">6</span> The language of clouds</a></li>
<li><a class="" href="no-optimal.html"><span class="header-section-number">7</span> No sky is the best sky</a></li>
<li class="book-part">The cloudspotter’s cheatsheet</li>
<li><a class="" href="conclusions-and-guidelines.html"><span class="header-section-number">8</span> Conclusions and guidelines</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/montesmariana/phdThesis">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="intro" class="section level1" number="1">
<h1>
<span class="header-section-number">1</span> Introduction<a class="anchor" aria-label="anchor" href="#intro"><i class="fas fa-link"></i></a>
</h1>
<p>If meaning is found and created in use, and corpora are language in use, can we find meaning in corpora? The field of usage-based semantics is large and rich, so the answer to this question is clearly positive. Corpora offer an immense amount of usage data on which to carry analyses, even if they barely scratch the surface of the amount of language that is actually produced — it is desirable and tempting to tap into this vast ocean to obtain the most detailed, the most reliable, the most thorough information. But there is a crucial bottleneck when it comes to semantic analysis: annotation is time- and energy-consuming. As long as we cannot instruct an automatic system to disambiguate each word in a corpus — like we do to tokenize and lemmatize, i.e. to identify what counts as a word and what its root is, or even to assign parts of speech or syntactic relations — semantic annotation is performed by humans. Humans are slower than computers; we get tired, we get confused, we need to eat and think of things beyond semantic annotation as weel. We also disagree sometimes — what is a sense? Are these two things <em>really</em> the same?</p>
<p>Automatic disambiguation systems do exist. Word Sense Disambiguation is an important task within Natural Language Processing (<span class="smallcaps">nlp</span>). The notion of <em>task</em> is of crucial importance here: <span class="smallcaps">nlp</span> algorithms are typically concerned with concrete applications and are evaluated in terms of those applications. There exists a correct answer that the algorithm must return. This is not so directly applicable to the situation of lexicological and lexicographical research — the study of the meanings of words and their relationships — especially from a Cognitive Linguistics point of view, where hard, dichotomous answers are rare. But let’s suppose for a moment that we can conciliate both approaches, and what counts as <em>the</em> answer from an <span class="smallcaps">nlp</span> point of view is <em>an</em> answer from the lexicological perspective. Then we could use automatic disambiguation procedures to make the heavy lifting of semantic annotation of our growing body of corpus data and use their results for a partial description of language. As long as we know <em>which</em> answer the <span class="smallcaps">nlp</span> algorithm is returning or, better yet, how to ask what we want to know. Maybe tuning the algorithm for outputs that from an <span class="smallcaps">nlp</span> point of view would be <em>wrong</em> can result in complementary answers for a richer lexicological description. Such a qualitative perspective, trying to interpret not just <em>whether</em> the computational model matches a target but also <em>how</em> or <em>why</em> it does (not), also requires appropriate analytical tools. One such tool represents the internal semantic structure of an item, derived from computational models, as a <span class="smallcaps">2d</span> scatterplot where instances occurring in similar context are shown together, forming clusters or <em>clouds</em>.</p>
<p>This dissertation is concerned with the application of distributional methods to lexicological research and their exploration by means of visual analytics. The methodology will be tested and illustrated with a set of 32 Dutch lemmas, of which concordance lines will be extracted from a corpus of newspapers.
Distributional models, developed within the field of Computational Linguistics, will be introduced in Section <a href="intro.html#comp">1.1</a>. In Section <a href="intro.html#cog">1.2</a> we will discuss their relevance in Cognitive Semantics and Section <a href="intro.html#viz">1.3</a> will offer an overview of the visual analytics dimension.
The study described here is part of a larger research project within the Quantitative Lexicology and Variational Linguistics research group (<span class="smallcaps">qlvl</span>) at KU Leuven. A brief history of the project and how this dissertation fits in it will be offered in Section <a href="intro.html#nephosem">1.4</a>. Finally, Section <a href="intro.html#str">1.5</a> will present the structure of the dissertation.</p>
<div id="comp" class="section level2" number="1.1">
<h2>
<span class="header-section-number">1.1</span> Distributional Semantics and Computational Linguistics<a class="anchor" aria-label="anchor" href="#comp"><i class="fas fa-link"></i></a>
</h2>
<p>Distributional semantics is a usage-based model of meaning that underlies various computational methods for semantic representation
<span class="citation">(<a href="references.html#ref-sahlgren_2008" role="doc-biblioref">Sahlgren 2008</a>; <a href="references.html#ref-lenci_2018" role="doc-biblioref">Lenci 2018</a>)</span>: it is an educational program for computers that lets them pretend they understand human languages. It relies on what is called the Distributional Hypothesis, according to which lexemes with similar meanings will have similar distributions, i.e. will occur in similar contexts. The core idea is typically attributed to <span class="citation"><a href="references.html#ref-harris_1954" role="doc-biblioref">Harris</a> (<a href="references.html#ref-harris_1954" role="doc-biblioref">1954</a>)</span> and <span class="citation"><a href="references.html#ref-firth_1957a" role="doc-biblioref">Firth</a> (<a href="references.html#ref-firth_1957a" role="doc-biblioref">1957</a>)</span>, but exactly how enthusiastic they would be at the sight of the current implementations is disputed: <span class="citation"><a href="references.html#ref-tognini-bonelli_2001" role="doc-biblioref">Tognini-Bonelli</a> (<a href="references.html#ref-tognini-bonelli_2001" role="doc-biblioref">2001: 157</a>)</span> remarks that Firth would not be in favour of electronic corpora, and <span class="citation"><a href="references.html#ref-geeraerts_2017" role="doc-biblioref">Geeraerts</a> (<a href="references.html#ref-geeraerts_2017" role="doc-biblioref">2017</a>)</span> offers a comprehensive comparison between Harris’ position and current distributional semantics. The attribution issue notwithstanding, the idea that meaning can be modelled by means of distributional information is pervasive in <span class="smallcaps">nlp</span> and at the core of every form of Distributional Semantics. A more important question is what we mean by <em>meaning</em> or <em>semantics</em> to begin with <span class="citation">(<a href="references.html#ref-sahlgren_2006" role="doc-biblioref">Sahlgren 2006</a>; <a href="references.html#ref-lenci_2008" role="doc-biblioref">Lenci 2008</a>)</span>, which in this research is informed by the Cognitive Linguistics framework. Beyond the particular attention to the semantic side of distributional semantics, this dissertation sets itself apart from most mainstream computational approaches in three core aspects: its motivation, the definition of units and its reliance on context-counting models.</p>
<div id="motivation" class="section level3" number="1.1.1">
<h3>
<span class="header-section-number">1.1.1</span> Motivation<a class="anchor" aria-label="anchor" href="#motivation"><i class="fas fa-link"></i></a>
</h3>
<p>Computational Linguistics is typically task-oriented: it aims to solve concrete challenges such as information retrieval, question answering, sentiment analysis, machine translation, etc. For that purpose, benchmarks or gold standards are developed and the models are tested against them. For example, <span class="citation"><a href="references.html#ref-baroni.etal_2014" role="doc-biblioref">Baroni, Dinu &amp; Kruszewski</a> (<a href="references.html#ref-baroni.etal_2014" role="doc-biblioref">2014</a>)</span> test different kinds of models against datasets tailored to evaluate semantic relatedness, synonym detection, concept categorization, selectional preferences and analogy; see <span class="citation"><a href="references.html#ref-agirre.edmonds_2007a" role="doc-biblioref">Agirre &amp; Edmonds</a> (<a href="references.html#ref-agirre.edmonds_2007a" role="doc-biblioref">2007</a>)</span> and <span class="citation"><a href="references.html#ref-raganato.etal_2017" role="doc-biblioref">Raganato, Camacho-Collados &amp; Navigli</a> (<a href="references.html#ref-raganato.etal_2017" role="doc-biblioref">2017</a>)</span> for evaluation systems for sense disambiguation. This is understandable and appropriate in a task-oriented workflow: when it comes to output, it does not really matter <em>how</em> the model reached the answer, as long as it is the answer that we seek. In contrast, investigating the structure of semantic representations, i.e. the <em>how</em> of this process, calls for a different approach <span class="citation">(see for example <a href="references.html#ref-baroni.lenci_2011" role="doc-biblioref">Baroni &amp; Lenci 2011</a>; <a href="references.html#ref-wielfaert.etal_2019" role="doc-biblioref">Wielfaert et al. 2019</a>)</span>. On the one hand, we do not assume that there is one correct answer because we do not assume that there is only one question. Beyond “Are these two words similar?” we are interested in: “Are they synonyms?” “Are they co-hyponyms?” “Are they regionally specific expressions of the same concept?” and so forth. Different models may focus on different dimensions of semantic structure and thus answer different questions. For that reason, the dataset collected for this research covers a wide range of semantic phenomena, in the hope of tuning distributional models to their identification. On the other hand, we are not confident that any of those questions has an unequivocal answer either. As Chapter <a href="dataset.html#dataset">4</a> will show, annotators often agree on the sense of an utterance, but not always. Hence, the manual annotations will serve as a guideline for the interpretation of the models, but not as a law to judge their accuracy.</p>
</div>
<div id="units-of-analysis" class="section level3" number="1.1.2">
<h3>
<span class="header-section-number">1.1.2</span> Units of analysis<a class="anchor" aria-label="anchor" href="#units-of-analysis"><i class="fas fa-link"></i></a>
</h3>
<p>Whereas computational models typically work at type-level and often with word forms, this dissertation focuses on token-level models with lemmas as units.
Type-level modelling represents a lexical unit, such as <em>word</em>, as the aggregated distributional behaviour of all its occurrences, e.g. we could see that <em>word</em> tends to be preceded by <em>the</em>. Patterns can be found by accumulating and classifying contextual information from thousands if not millions of events. The profile of a type can subsequently be compared to the profiles of other types, e.g. we can see that <em>sentence</em> also tends to be preceded by <em>the</em>, while <em>walking</em> does not. Such a representation conflates the variation within the range of application of that item as part of one overall tendency, and is therefore not suited to study polysemy. Even if the context does contain disambiguating cues, such as “Can we have a <em>word</em>?” or “That <em>word</em> is not in the dictionary,” the type-level representation will cover both. In spite of these shortcomings, some computational approaches to modelling polysemy do try to find the patterns in the type-level representations, e.g. <span class="citation"><a href="references.html#ref-koptjevskaja-tamm.sahlgren_2014" role="doc-biblioref">Koptjevskaja-Tamm &amp; Sahlgren</a> (<a href="references.html#ref-koptjevskaja-tamm.sahlgren_2014" role="doc-biblioref">2014</a>)</span>. In contrast, the work presented here relies on token-level modelling, which represents individual instances, e.g. comparing the two occurrences of <em>word</em> in the examples above. This approach does originate in computational linguistics <span class="citation">(<a href="references.html#ref-schutze_1998" role="doc-biblioref">Schütze 1998</a>)</span> but is far less popular than type-level approaches, which are considered the default in most introductory descriptions of distributional models <span class="citation">(<a href="references.html#ref-lenci_2018" role="doc-biblioref">Lenci 2018</a>; <a href="references.html#ref-turney.pantel_2010" role="doc-biblioref">Turney &amp; Pantel 2010</a>; <a href="references.html#ref-bolognesi_2020" role="doc-biblioref">Bolognesi 2020</a>)</span>.</p>
<p>Apart from the distinction between modelling types or tokens, a crucial difference between this approach and many studies in computational linguistics is that the unit of analysis is the lemma instead of the word form. On the one hand, relying on word forms avoids layers of preprocessing that already incorporate a certain interpretation in terms of what counts as a word, which different forms go together and how they are classified grammatically. <span class="citation"><a href="references.html#ref-sinclair_1991" role="doc-biblioref">Sinclair</a> (<a href="references.html#ref-sinclair_1991" role="doc-biblioref">1991</a>)</span> also argues along these lines for the usage of word forms as lexical units in corpus linguistics. And, admittedly, different word forms of a given lemma might exhibit diverging distributional and semantic profiles. However, from a lexicological and lexicographical perspective, centring the lemma — the combination of stems and grammatical category — is the common practice. Moreover, the mismatch between word forms and lemmas — and therefore between either of them and meanings — is highly dependent on the language we describe and the words themselves. Therefore, lemmas will be the unit of analysis in this dissertation. This is not to say that the workflow depends on this decision, in the same way that it does not depend on Dutch being the language of the corpus. The methodology presented in these pages could be applied with word forms at the centre, but the degree to which the conclusions reached here would be applicable is an empirical question.</p>
</div>
<div id="context-counting-and-context-predicting" class="section level3" number="1.1.3">
<h3>
<span class="header-section-number">1.1.3</span> Context-counting and context-predicting<a class="anchor" aria-label="anchor" href="#context-counting-and-context-predicting"><i class="fas fa-link"></i></a>
</h3>
<p>Currently, the most popular approach for distributional semantics relies on neural networks, i.e. context-predicting models. The methodology followed in this project relies instead on count-based or context-counting models: the values of the vectors, i.e. numerical representations of lexical units, are (relatively) directly derived from frequency counts. In contrast, the approach initiated by <span class="citation"><a href="references.html#ref-mikolov.etal_2013" role="doc-biblioref">Mikolov et al.</a> (<a href="references.html#ref-mikolov.etal_2013" role="doc-biblioref">2013</a>)</span> and which has taken over <span class="smallcaps">nlp</span>, i.e. word embeddings, is a context-predicting architecture. Neural networks are trained to predict empty slots in a fragment of text: given a fixed window with a target item in the middle, <span class="smallcaps">cbow</span> models are given the surrounding context in order to predict the target item, whereas skip-gram models try to predict the context based on the item in the middle. The training consists on a long sequence of trial and error: there is a right answer, i.e. the actual corpus, the algorithm starts by guessing and receives feedback, and iteratively it adapts its guessing strategy to minimise the error. The strategy consists of weights in the hidden layer of neural network; these weights are then used to represent the target item. In other words, while a context-counting model would define the distributional profile of a word along the lines of “it tends to co-occur with <em>chocolate</em> and <em>cookies</em> but not with <em>mycorrhyza</em> or <em>algorithm</em>,” context-predicting models say, more or less, “this is how I feel/what my brain does when I see that word.” The latter is, in a sense, more in line with the core of meaning as an introspective experience that defies definitions and restrictions, although computational models are far from actually <em>understanding</em> language. Exploring to what degree these models approximate humans’ assessments lies in the purview of other research programmes involving psycholinguistic experiments. Studies have been carried out to compare the performance of context-counting and context-predicting models — in terms, of course, of their accuracy with regards to popular benchmarks. <span class="citation"><a href="references.html#ref-baroni.etal_2014" role="doc-biblioref">Baroni, Dinu &amp; Kruszewski</a> (<a href="references.html#ref-baroni.etal_2014" role="doc-biblioref">2014</a>)</span> found that the word2vec architecture outperformed context-counting models, much to their disappointment. In contrast, <span class="citation"><a href="references.html#ref-levy.etal_2015" role="doc-biblioref">Levy, Goldberg &amp; Dagan</a> (<a href="references.html#ref-levy.etal_2015" role="doc-biblioref">2015</a>)</span> fine-tuned context-counting models based on the hyperparameters from word embedding and found that performance differences where local or even insignificant.</p>
<p>When our purpose is to understand what of meaning, if anything, can be found in text data, the interpretation of context-counting models is much more transparent. We can trace the composition of the vectors to concrete frequencies and instances. As we will see in the second part of this dissertation, these supposedly more transparent models are already quite opaque, especially with the added transformation from type-level to token-level models. That said, most of the workflow described here can also be combined with context-predicting models.</p>
<p>The years since <span class="citation"><a href="references.html#ref-mikolov.etal_2013" role="doc-biblioref">Mikolov et al.</a> (<a href="references.html#ref-mikolov.etal_2013" role="doc-biblioref">2013</a>)</span> have seen a rapid and enthusiastic growth in the field of word embeddings and <span class="smallcaps">nlp</span>, with new models continually surpassing the previous ones. One of these is <span class="smallcaps">bert</span> <span class="citation">(<a href="references.html#ref-BERT" role="doc-biblioref">Devlin et al. 2019</a>)</span>, which, in spite of its indubitable relevance to the approach proposed here, will not be explored. Bidirectional Encoder Representations from Transformers (<span class="smallcaps">bert</span>) is a machine-learning technique that can represent individual instances and sentences: unlike other context-predicting models, it can be used for token-level representations. But like other context-predicting models, its output is somewhat less interpretable than context-counting models. It has been tested on the typical task-based benchmarks and it is so time- and resources-consuming that <span class="smallcaps">nlp</span> researchers will typically use pre-trained embeddings and fine-tune them for specific tasks rather than generate them from scratch. In principle, combining a model of the <span class="smallcaps">bert</span> family with the workflow described here is not impossible: as long as occurrences are represented with vectors from which we can derive pairwise distances, the rest of the analysis stays the same. However, some crucial differences remain: we do not know which elements of the context informed the models’ decision, they are based on word forms and the word forms are based on a different tokenizer. For instance, a brief test of <span class="smallcaps">bert</span>je <span class="citation">(<a href="references.html#ref-devries.etal_2019" role="doc-biblioref">de Vries et al. 2019</a>)</span>, the Dutch counterpart of <span class="smallcaps">bert</span>, on a section of the dataset used for this project revealed that (i) for some lemmas <span class="smallcaps">bert</span>je’s answer might be closer to the human perspective, (ii) for other lemmas a deeper investigation is in order and (iii) other lemmas cannot be modelled at all because of the discrepancy in the tokenization procedure<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;The comparison was applied to a few lemmas, including &lt;em&gt;hoop&lt;/em&gt; ‘hope/heap,’ &lt;em&gt;dof&lt;/em&gt; ‘dull’ and &lt;em&gt;heilzaam&lt;/em&gt; ‘healthy/beneficial.’ In the first case, which was particularly challenging for the context-counting models, &lt;span class="smallcaps"&gt;bert&lt;/span&gt;je outperformed them; in the second, some context-counting models outperformed &lt;span class="smallcaps"&gt;bert&lt;/span&gt;je; and the third was never identified as one unit by &lt;span class="smallcaps"&gt;bert&lt;/span&gt;je’s tokenizer.&lt;/p&gt;'><sup>2</sup></a>. In other words, even if combining the methodologies is possible, the actual implementation requires some planning, specific decisions and tailoring the procedure to extract as much as we can from the backstage operations in context-predicting models.</p>
</div>
</div>
<div id="cog" class="section level2" number="1.2">
<h2>
<span class="header-section-number">1.2</span> Distributional Semantics and Cognitive Semantics<a class="anchor" aria-label="anchor" href="#cog"><i class="fas fa-link"></i></a>
</h2>
<p>As a computational approach, distributional semantics is not intrinsically linked to any particular linguistic theory. Its usage-based essence makes it a natural fit for approaches that describe the <em>parole</em> along with the <em>langue</em> <span class="citation">(in terms of <a href="references.html#ref-desaussure_1971" role="doc-biblioref">de Saussure 1971</a>)</span>, such as Cognitive Linguistics. In the introduction to <em>The Oxford Handbook of Cognitive Linguistics</em>, it is described as</p>
<blockquote>
<p>an approach to the analysis of natural language that originated in the late seventies and early eighties in the work of George Lakoff, Ron Langacker, and Len Talmy, and that focuses on language as an instrument for organizing, processing, and conveying information. <span class="citation">(<a href="references.html#ref-geeraerts.cuyckens_2007a" role="doc-biblioref">Geeraerts &amp; Cuyckens 2007a: 3</a>)</span></p>
</blockquote>
<p>It stands in contrast to frameworks that uphold a strict separation of semantics and pragmatics, of structure and usage, of lexical knowledge and world knowledge <span class="citation">(<a href="references.html#ref-geeraerts_2010a" role="doc-biblioref">Geeraerts 2010a</a>)</span>. As the introduction and composition of the <em>Handbook</em> shows, as well as other compilations along these lines <span class="citation">(such as <a href="references.html#ref-rudzka-ostyn_1988" role="doc-biblioref">Rudzka-Ostyn 1988</a>; <a href="references.html#ref-kristiansen.etal_2006" role="doc-biblioref">Kristiansen et al. 2006</a>; <a href="references.html#ref-ibarretxe-antunano.valenzuela_2016" role="doc-biblioref">Ibarretxe-Antuñano &amp; Valenzuela 2016</a>)</span>, the diverse field of Cognitive Linguistics is guided by a number of principles derived from this central notion of language as categorization. Among these principles, three in particular constitute the theoretical cornerstones of this study: (i) an emphasis on meaning, (ii) the notion of fuzzy and prototypical categories and (iii) a usage-based approach.</p>
<div id="everything-is-semantics" class="section level3" number="1.2.1">
<h3>
<span class="header-section-number">1.2.1</span> Everything is semantics<a class="anchor" aria-label="anchor" href="#everything-is-semantics"><i class="fas fa-link"></i></a>
</h3>
<p>Understanding language as categorization and its function in the organization and communication of knowledge necessarily places the focus on meaning <span class="citation">(<a href="references.html#ref-geeraerts.cuyckens_2007" role="doc-biblioref">Geeraerts &amp; Cuyckens 2007b</a>; <a href="references.html#ref-geeraerts_2016a" role="doc-biblioref">Geeraerts 2016</a>)</span>. From a Cognitive Linguistics perspective, all linguistic structures — not just lexical items but also syntactic patterns — are considered inherently meaningful <span class="citation">(<a href="references.html#ref-langacker_2008a" role="doc-biblioref">Langacker 2008</a>; <a href="references.html#ref-lemmens_2015" role="doc-biblioref">Lemmens 2015</a>)</span>. Moreover, meaning in Cognitive Linguistics goes beyond traditional semantics — i.e. distinguishing linguistic from nonlinguistic features — and includes encyclopedic knowledge and pragmatics <span class="citation">(<a href="references.html#ref-glynn_2010" role="doc-biblioref">Glynn 2010</a>; <a href="references.html#ref-geeraerts_1997" role="doc-biblioref">Geeraerts 1997</a>)</span>. While it is crucially a cognitive phenomenon involving conceptualization, it takes place in the mind of physical, embodied beings who perceive, understand, and interact with their world: meaning is embodied and neither limited to nor separated from reference <span class="citation">(<a href="references.html#ref-rohrer_2007" role="doc-biblioref">Rohrer 2007</a>)</span>.</p>
<p>The centrality of semantics in Cognitive Linguistics has led to a strong body of work on meaning and on how traditional notions fit in with cognitive principles.
For example, the line of work initiated in the ’80s with <span class="citation"><a href="references.html#ref-lakoff.johnson_2003" role="doc-biblioref">Lakoff &amp; Johnson</a> (<a href="references.html#ref-lakoff.johnson_2003" role="doc-biblioref">2003</a>)</span> and further developed along different lines by Raymond Gibbs Jr., Gerard Steen, Zoltán Kövecses, Elena Semino and many others <span class="citation">(see for example <a href="references.html#ref-gibbs.steen_1999" role="doc-biblioref">Gibbs &amp; Steen 1999</a>; <a href="references.html#ref-gibbs_2008" role="doc-biblioref">Gibbs Jr. 2008</a>; <a href="references.html#ref-semino_2008" role="doc-biblioref">Semino 2008</a>; <a href="references.html#ref-kovecses_2015" role="doc-biblioref">Kövecses 2015</a>)</span> builds on understanding a traditional linguistic concept, i.e. metaphor, with the tools of Cognitive Linguistics. In these terms, metaphor refers to ways of thinking, understanding, conceptualizing, that manifest in linguistic behaviour but also permeate other areas of everyday life.</p>
<p>Along these lines, relationships between senses are understood as cognitive mechanisms that need not be restricted to linguistic behaviour nor to extralinguistic reference. Semantic categories such as metaphor, metonymy, specialization, homonymy and prototypicality are crucial tools to make sense of the variety of relationships between what we understand as senses. They are not unique to Cognitive Linguistics, but a framework that understands meaning as a property of any linguistic structure and as covering linguistic and extralinguistic features allows us to look for meaning in distributional models without expecting them to exhaust semantic description.</p>
<p>Cognitive Linguistics also incorporates the combination of a semasiological and onomasiological perspective, while previous frameworks have defined either one or the other as the only possibility <span class="citation">(<a href="references.html#ref-geeraerts_2010a" role="doc-biblioref">Geeraerts 2010a</a>)</span>. A semasiological perspective, which is predominant in the research described here, starts from a form or expression and investigates its range of meanings or applications, e.g. the study of polysemy. An onomasiological perspective, on the other hand, starts from a concept and describes the forms that are used to express it, e.g. synonymy. This dissertation takes a semasiological perspective, but token-level distributional models can be used from both perspectives, as shown in <span class="citation"><a href="references.html#ref-depascale_2019" role="doc-biblioref">De Pascale</a> (<a href="references.html#ref-depascale_2019" role="doc-biblioref">2019</a>)</span>.</p>
</div>
<div id="prototypicality" class="section level3" number="1.2.2">
<h3>
<span class="header-section-number">1.2.2</span> Prototypicality<a class="anchor" aria-label="anchor" href="#prototypicality"><i class="fas fa-link"></i></a>
</h3>
<p>Among the most important notions in the Cognitive Linguistics understanding of categorization we find prototypicality and salience <span class="citation">(<a href="references.html#ref-rosch_1978" role="doc-biblioref">Rosch 1978</a>)</span>. Categories cannot always be described in terms of necessary and sufficient conditions; instead, they may be characterized by clusters of co-occurring properties that do not apply to all members to the same degree. They may even have fuzzy boundaries, an unclear range of application. As a property of categorization, this is a property of language, which Cognitive Linguistics embraces, incorporating a quantitative dimension to the study of meaning <span class="citation">(<a href="references.html#ref-geeraerts_2010a" role="doc-biblioref">Geeraerts 2010a</a>)</span>. At this point, a quantitative perspective does not immediately require statistical methods, but refers to a shift in the understanding of what counts as meaning description. The notion of prototypicality makes it interesting, if not inevitable, to look at the uneven distribution and importance of the different features or members of a category, as is done, for example, in <span class="citation"><a href="references.html#ref-geeraerts.etal_1994" role="doc-biblioref">Geeraerts, Grondelaers &amp; Bakema</a> (<a href="references.html#ref-geeraerts.etal_1994" role="doc-biblioref">1994</a>)</span> and <span class="citation"><a href="references.html#ref-geeraerts_1997" role="doc-biblioref">Geeraerts</a> (<a href="references.html#ref-geeraerts_1997" role="doc-biblioref">1997</a>)</span>:</p>
<blockquote>
<p>…the essence of prototype theory lies in the fact that it highlights the importance of flexibility (absence of clear demarcational boundaries) and salience (differences of structural weight) in the semantic structure of linguistic categories. <span class="citation">(<a href="references.html#ref-geeraerts_2006e" role="doc-biblioref">Geeraerts 2006: 74</a>)</span></p>
</blockquote>
<p>Given the set of meanings that a form can express, i.e. the intensional level, some of them are more salient than others. For example, given my current lifestyle, ‘device to control the cursor on a screen’ is a more salient meaning of <em>mouse</em> than ‘small rodent’; but, crucially, this might not be the case in other contexts, for other speakers. Given the range of application of a form or a meaning (i.e. the extensional level), some may be more typical members than others. For instance, a black, minimalist computer mouse might be more typical than a wavy, wider gaming mouse with a bright green drawing of a dragon. These situations represent intensional and extensional nonequality, respectively: some senses or members of a category are better representatives of the category than others. Both dimensions may overlap: a typical computer mouse concentrates most of the typical features of the category, regarding its functionality, size, shape and colour; conversely, a typical feature is defined by occurring frequently in the members of the category. These are two of the characteristics of prototypicality, and are complemented by intensional and extensional non discreteness, i.e. the lack of a single set of necessary and sufficient conditions and fuzzy boundaries of the categories. As could be expected, even prototypicality is a prototypical category, as these four features need not co-exist. The relative salience of the two senses of <em>mouse</em> does not mean that we might find an unknown entity and be in doubt whether it is a mouse; meanwhile, discussions such as whether a tomato is a fruit might easily ensue. <span class="citation"><a href="references.html#ref-geeraerts_2006e" role="doc-biblioref">Geeraerts</a> (<a href="references.html#ref-geeraerts_2006e" role="doc-biblioref">2006</a>Ch. 4)</span> offers a typology of salience phenomena as an application of prototype theory beyond the semasiological structure. For example, if from the semasiological perspective we are interested in describing how frequent (or salient) apples are as referents for the word <em>fruit</em>, from the onomasiological perspective we are interested in how frequently the word <em>fruit</em> is used to refer to apples (compared to saying <em>apple</em>).</p>
<p>The notion of (semasiological) prototypicality will be relevant for the interpretation of the modelling in Chapter <a href="semantic-interpretation.html#semantic-interpretation">6</a>. Until then, it also permeates the understanding of meaning that underlies this research. On the one hand, fuzzy boundaries and degrees of membership invite us to rethink the usefulness of reified senses: ambiguous examples and overlapping features are to be expected. Instead, a bottom-up procedure would rather capture configurations of features <span class="citation">(<a href="references.html#ref-glynn_2014c" role="doc-biblioref">Glynn 2014</a>)</span>; assigning discrete senses to corpus data imposes a categorical structure that we know to be inappropriate <span class="citation">(see also <a href="references.html#ref-geeraerts_1993" role="doc-biblioref">Geeraerts 1993</a>)</span>. On the other hand, distributional models, as a quantitative approach that measures similarity between entities, is particularly adequate to such a non-discrete representation.</p>
<p>In this dissertation I will continue to talk about senses and I will extract discrete patterns from the non-discrete representations in terms of clusters, in order to manipulate and talk about these abstract entities, without implying that they have any ontological reality beyond the explanatory purposes. When it comes to senses, they are not considered a gold standard, an unique solution to the semasiological description of a lexical item; instead, they are guides and an operationalization of certain research questions. The clusters, on the other hand, will be generated by an algorithm that is forced to produce discrete groups but does assign its elements different degrees of membership (see Section <a href="workflow.html#hdbscan">2.2.4</a>). Finally, the overall approach describes tendencies, preferences, probabilities: at no level are the categories and typologies offered in this dissertation discrete and uniform. I have tried, but language resists.</p>
</div>
<div id="a-usage-based-approach" class="section level3" number="1.2.3">
<h3>
<span class="header-section-number">1.2.3</span> A usage-based approach<a class="anchor" aria-label="anchor" href="#a-usage-based-approach"><i class="fas fa-link"></i></a>
</h3>
<p>Cognitive Linguistics presents itself as a usage-based approach and, as such, it is entirely compatible with a bottom-up, empirical, quantitative methodology such as distributional semantics. Quantitative cognitive semantics is now an established field, as shown by the contributions gathered in <span class="citation"><a href="references.html#ref-gries.stefanowitsch_2006" role="doc-biblioref">Gries &amp; Stefanowitsch</a> (<a href="references.html#ref-gries.stefanowitsch_2006" role="doc-biblioref">2006</a>)</span>, <span class="citation"><a href="references.html#ref-glynn.fischer_2010" role="doc-biblioref">Glynn &amp; Fischer</a> (<a href="references.html#ref-glynn.fischer_2010" role="doc-biblioref">2010</a>)</span> and <span class="citation"><a href="references.html#ref-glynn.robinson_2014" role="doc-biblioref">Glynn &amp; Robinson</a> (<a href="references.html#ref-glynn.robinson_2014" role="doc-biblioref">2014</a>)</span>, among others. However, not all of Cognitive Linguistics — and especially Cognitive Semantics — relies on empirical methods: introspection was still the main source of information in much of the foundational sources <span class="citation">(see for example the discussion illustrated in <a href="references.html#ref-geeraerts_1999" role="doc-biblioref">Geeraerts 1999</a>)</span>. In practice, both introspection and empirical methods are required in scientific research, albeit applied to different stages or aspects of the investigation <span class="citation">(<a href="references.html#ref-geeraerts_2010" role="doc-biblioref">Geeraerts 2010b</a>)</span>.
Interpretation is needed in order to formulate hypotheses that will guide the data collection and analysis and to interpret the results: the data does not speak for itself. The empirical steps, in contrast, facilitate reproducibility and falsifiability: by describing the concrete corpus, the method of collection and the quantitative methods applied to it, the study can be replicated by different researchers and the results compared.
At the same time, large-scale quantitative methods such as distributional semantics delegate time consuming or computationally expensive tasks, such as reading and comparing thousands of attestations of a word, to an automatic system that can perform it faster and more systematically than humans, leaving the researcher to dedicate their energies in the tasks that humans are best at: interpretation and creativity. That is precisely the long-term goal of this research: to offer an empirical, quantitative workflow that transforms huge amounts of data, finds relevant patterns and provides them to the linguist for interpretation and the formulation of hypotheses.</p>
<p>Empirical research in semantics can take different shapes: corpus-based methods, as is the case in this research, but also experimental and referential methods. As <span class="citation"><a href="references.html#ref-geeraerts_2015b" role="doc-biblioref">Geeraerts</a> (<a href="references.html#ref-geeraerts_2015b" role="doc-biblioref">2015: 242–243</a>)</span> argues, each of these approaches captures a different aspect of meaning, namely textual patterns, on-line processing or referential properties. Meaning, especially from the maximalist perspective taken in Cognitive Linguistics, is too complex to be fully described by any one of these methods in isolation <span class="citation">(see also <a href="references.html#ref-arppe.etal_2010" role="doc-biblioref">Arppe et al. 2010</a>; <a href="references.html#ref-stefanowitsch_2010" role="doc-biblioref">Stefanowitsch 2010</a>)</span>. As such, we do not have such high expectations from distributional semantics — part of the question is: <em>what</em> do these models say?
Concretely, we do not expect distributional models to provide information on how we <em>think</em>, but on how a community speaks and categorises: “‘language as cognition’ encompasses shared and socially distributed knowledge and not just individual ideas and experiences” <span class="citation">(<a href="references.html#ref-geeraerts_2016a" role="doc-biblioref">Geeraerts 2016: 533</a>)</span>. It is the pool of shared practices and knowledge that corpora offers and distributional semantics tries to model.</p>
<p>Moreover, despite the large corpora, the advanced quantitative techniques and the sophisticated visualization tools on which this dissertation is built, this study has its limits. It is restricted to a specific corpus, and as such to specific varieties of a specific language, to a specific genre and period in time, to written text; it is restricted to a limited set of lexical items that were investigated; it is restricted to the precise samples collected, the precise questions asked, the precise techniques used to answer them. Most importantly, I will be as thorough as possible in stating the conditions in which the research was carried out and the choices made along the way. As a result, these limits are not just warnings as to the range of applicability of the results and conclusions, but also and more importantly sources of possibilities, inspiration for similar studies facilitated by the empirical nature of the investigation.</p>
</div>
</div>
<div id="viz" class="section level2" number="1.3">
<h2>
<span class="header-section-number">1.3</span> Visual analytics<a class="anchor" aria-label="anchor" href="#viz"><i class="fas fa-link"></i></a>
</h2>
<p>Distributional models return mathematical representations of lexical items — or, in the case of token-level models, their attestations. These mathematical representations are arrays of numbers that, in the best-case scenario, we can interpret as co-occurrence information, as an unsorted list of collocations. We need an additional step to transform these individual representations into similarities, which operationalize the Distributional Hypothesis mentioned above. However, even then, the output is a matrix with as many rows and columns as items we are comparing; depending on the magnitude of our sample and the subtlety of its structure, scanning it visually can be taxing, if not entirely in vain. So, that is not what we do.</p>
<p>For word sense disambiguation, evaluation would normally involve a clustering algorithm, a benchmark and a measure of accuracy. The clustering algorithm would take the vectors or the similarity matrix and return clusters: groups of similar items that are different from each other. The measure of accuracy would report on the agreement between the clustering solution and the benchmark: the closer they are, the better the model. However, these measures say nothing about the qualitative differences between models, i.e. whether they misclassified the same items or how they differ from the benchmark. Even if we take the gold standard as an actual ground truth and the only correct solution — which is not the case in this study — this is not an ideal situation.</p>
<p>It is responding to these concerns that a visualization tool for the exploration of token-level models was envisaged <span class="citation">(<a href="references.html#ref-wielfaert.etal_2019" role="doc-biblioref">Wielfaert et al. 2019</a>)</span>. The tool developed by Wielfaert in the context of the Nephological Semantics project takes the output from a dimensionality reduction algorithm, i.e. a procedure that tries to map distances based on multiple dimensions on a <span class="smallcaps">2d</span> or <span class="smallcaps">3d</span> space, and surrounds its visual representation with interactive features. These additional features, tailored for the exploration of distributional models, set the tool apart from a static scatterplot, or even from a default interactive plot.</p>
<p>To put it in <span class="citation"><a href="references.html#ref-card.etal_1999" role="doc-biblioref">Card, Mackinlay &amp; Shneiderman</a> (<a href="references.html#ref-card.etal_1999" role="doc-biblioref">1999: 6</a>)</span>’s words: “‘The purpose of visualization is insight, not pictures.’ The main goals of this insight are <em>discovery</em>, <em>decision making</em> and <em>explanation</em>”<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Emphasis from the original.&lt;/p&gt;"><sup>3</sup></a>. Indeed, the kind of qualitative exploration achieved through this tool would have been extremely hard without it, if not impossible.
In the first place, the tool sets up a workflow that goes from the exploration of the similarity <em>between</em> models and the role of parameter settings through the qualitative comparison of selections models to the detailed exploration of individual models. It is built to facilitate a fluid exploration and interconnection between levels of analysis. The tool offers simultaneous, interconnected access to the actual output of a model (as coordinates on a <span class="smallcaps">2d</span> plane), the variation of parameter settings, semantic annotation, metadata of the corpora and frequency data on the context words. The interaction of these different aspects of distributional models in a practical visual interface makes patterns and insights accessible that would not have been found any other way.</p>
<p>Because of this, the visualization tool is a key component of this dissertation. It is in these scatterplots that we find the clouds: clusters of similar tokens that come together in denser areas of the (reduced) semantic space. In an actual case study involving the methodological workflow presented here, a lot of the technicalities go into generating the clouds, but a large part of the analysis involves looking at them and finding shapes: cloudspotting.</p>
</div>
<div id="nephosem" class="section level2" number="1.4">
<h2>
<span class="header-section-number">1.4</span> Nephological Semantics<a class="anchor" aria-label="anchor" href="#nephosem"><i class="fas fa-link"></i></a>
</h2>
<p>The research presented in this dissertation is part of a larger project within the <span class="smallcaps">qlvl</span> research unit, the <span class="smallcaps">bof</span> C1-project (3H150305) “Nephological Semantics: using token clouds for meaning detection in variationist linguistics,” with Dr. Prof. Dirk Geeraerts as Principal Investigator. Both the Python module for the creation of the models, written by Tao Chen, and the visualization tools for their analysis, designed by Thomas Wielfaert and myself, are products of this project. Moreover, this dissertation would not be what it is without the integration of the case studies, questions and insights discussed here with other branches of the project, and without the feedback loop on ideas, tests and thoughts on the different techniques.</p>
<p>The main objective of the project is to develop — and understand — appropriate methods for the retrieval of semantic information from corpus data, addressing concerns that stem from a longer tradition of usage-based lexical research. <span class="citation"><a href="references.html#ref-geeraerts.etal_1994" role="doc-biblioref">Geeraerts, Grondelaers &amp; Bakema</a> (<a href="references.html#ref-geeraerts.etal_1994" role="doc-biblioref">1994</a>)</span> and <span class="citation"><a href="references.html#ref-geeraerts.etal_1999" role="doc-biblioref">Geeraerts, Grondelaers &amp; Speelman</a> (<a href="references.html#ref-geeraerts.etal_1999" role="doc-biblioref">1999</a>)</span> embark in comprehensive, detailed lexicological analyses of the lexical fields of clothing and football terms in Dutch. Their approach is referential: <span class="citation"><a href="references.html#ref-geeraerts.etal_1994" role="doc-biblioref">Geeraerts, Grondelaers &amp; Bakema</a> (<a href="references.html#ref-geeraerts.etal_1994" role="doc-biblioref">1994</a>)</span>, for instance, collect pictures and descriptions of garments from Dutch and Flemish magazines and describe each clothing item in terms of a variety of features, such as the length of the sleeve. Based on the relationship between the (configurations of) features and the items used to name the objects, they developed a model of lexical variation that takes into account prototypicality and salience in terms of semasiological, onomasiological and contextual variation. However, the manual and detailed identification of features at a large enough scale is painstaking and time consuming, if at all feasible. In contrast, machine-readable linguistic material is available, more or less accessible and, given the right resources, processable. It will not provide the same kind of information as a referential approach, but it is more easily scalable to large amounts of data.</p>
<p>In the context of this project, token-level models for semasiological research are introduced by <span class="citation"><a href="references.html#ref-heylen.etal_2012" role="doc-biblioref">Heylen, Speelman &amp; Geeraerts</a> (<a href="references.html#ref-heylen.etal_2012" role="doc-biblioref">2012</a>)</span> and <span class="citation"><a href="references.html#ref-heylen.etal_2015" role="doc-biblioref">Heylen et al.</a> (<a href="references.html#ref-heylen.etal_2015" role="doc-biblioref">2015</a>)</span>. Another work-package, culminating in <span class="citation"><a href="references.html#ref-depascale_2019" role="doc-biblioref">De Pascale</a> (<a href="references.html#ref-depascale_2019" role="doc-biblioref">2019</a>)</span>’s PhD dissertation, applies the technique to lexical lectometric research, i.e. measuring distances between language varieties based on their naming choices for different concepts. The visualization tool, as mentioned before, is first described in <span class="citation"><a href="references.html#ref-wielfaert.etal_2019" role="doc-biblioref">Wielfaert et al.</a> (<a href="references.html#ref-wielfaert.etal_2019" role="doc-biblioref">2019</a>)</span>. Between their work, this dissertation and further case studies taking place in the last year, the project is covering the application of token-level vector space models on semasiological, onomasiological and lectometric studies in varieties of Dutch and Mandarin, at both a synchronic and a diachronic level.</p>
</div>
<div id="str" class="section level2" number="1.5">
<h2>
<span class="header-section-number">1.5</span> Structure of the dissertation<a class="anchor" aria-label="anchor" href="#str"><i class="fas fa-link"></i></a>
</h2>
<p>As a product of the Nephological Semantics project, this dissertation aims to contribute to both the development and understanding of distributional models for lexical semasiological research.
It brings together the theoretical perspective on semantics from Cognitive Linguistics with computational methods and visual analytics in the hope of paving the way for future research along the same lines.
With that in mind, the three chapters of the first part of this dissertation, <em>The cloudspotter’s toolkit</em>, will focus on the technical or methodological side of the project.
Chapter <a href="workflow.html#workflow">2</a> will describe the procedure to create clouds and the parameter settings explored, taking care to be thorough and specific about the technical decisions that resulted in the final models.
Then, Chapter <a href="nephovis.html#nephovis">3</a> will showcase the visualization tool designed by Thomas Wielfaert and myself as well as a ShinyApp extension that provides additional functionalities.
Finally, Chapter <a href="dataset.html#dataset">4</a> will illustrate the dataset on which the models were tested: the selection of lemmas and the questions they try to address, the collection of data and the annotation procedure.</p>
<p>The notion behind token-level models, i.e. that we can represent meaning differences in terms of distributional differences, and in particular the image of a scatterplot that translates these intuitions into an interpretable picture, <em>sounds good</em>.
Alas, the reality is not as bright as we could have wished for, and the skies of distributional semantics have all but a stable weather. Hopefully, this dissertation can offer a guide for researchers who would dare to tread these waters. Therefore, the three chapters in the second part, <em>The cloudspotter’s handbook</em>, will discuss the results of the analyses, with an emphasis on crucial assumptions that clash with the data. First, Chapter <a href="shapes.html#shapes">5</a> debunks the idea of a perfect cloud emerging from the ocean of the corpus. Clouds come in many different shapes, caused by different phenomena of distributional behaviour, and thus this chapter offers a classification of what we might encounter. Chapter <a href="semantic-interpretation.html#semantic-interpretation">6</a> follows with a linguistic perspective on the variation of these shapes and discusses what we can or we cannot find in these models. Finally, Chapter <a href="no-optimal.html#no-optimal">7</a> shows how no set of parameter settings offers the best solution across the board — not even close. Instead, the same parameter settings may result in different shapes for different lemmas, and they have to be tailored <em>to the specific lemma</em> to capture the relevant semantic structure.</p>
<p>An enthusiastic and hopeful aspiring cloudspotter might feel discouraged by the variability — bordering on unpredictability — of these clouds. I wouldn’t blame them. However, in spite of the diversity of shapes, of semantic phenomena and of parameter settings to explore, the methodology can offer interesting insights. They are partial insights, but insights nonetheless, and once we know what to expect from clouds, we can focus on acquiring them. In that perspective, the third and final part of this dissertation, <em>The cloudspotter’s cheatsheet</em>, will close with a general practical guide, a summary of suggestions for further research and an overall conclusion.</p>

</div>
</div>




  <div class="chapter-nav">
<div class="prev"><a href="acknowledgements.html">Acknowledgements</a></div>
<div class="next"><a href="workflow.html"><span class="header-section-number">2</span> From corpora to clouds</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#intro"><span class="header-section-number">1</span> Introduction</a></li>
<li>
<a class="nav-link" href="#comp"><span class="header-section-number">1.1</span> Distributional Semantics and Computational Linguistics</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#motivation"><span class="header-section-number">1.1.1</span> Motivation</a></li>
<li><a class="nav-link" href="#units-of-analysis"><span class="header-section-number">1.1.2</span> Units of analysis</a></li>
<li><a class="nav-link" href="#context-counting-and-context-predicting"><span class="header-section-number">1.1.3</span> Context-counting and context-predicting</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#cog"><span class="header-section-number">1.2</span> Distributional Semantics and Cognitive Semantics</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#everything-is-semantics"><span class="header-section-number">1.2.1</span> Everything is semantics</a></li>
<li><a class="nav-link" href="#prototypicality"><span class="header-section-number">1.2.2</span> Prototypicality</a></li>
<li><a class="nav-link" href="#a-usage-based-approach"><span class="header-section-number">1.2.3</span> A usage-based approach</a></li>
</ul>
</li>
<li><a class="nav-link" href="#viz"><span class="header-section-number">1.3</span> Visual analytics</a></li>
<li><a class="nav-link" href="#nephosem"><span class="header-section-number">1.4</span> Nephological Semantics</a></li>
<li><a class="nav-link" href="#str"><span class="header-section-number">1.5</span> Structure of the dissertation</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/montesmariana/phdThesis/blob/master/01-introduction.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/montesmariana/phdThesis/edit/master/01-introduction.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Cloudspotting</strong>: Visual analytics for distributional semantics" was written by Mariana Montes. </p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
