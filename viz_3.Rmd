# Case studies {#dataset}

Every empirical study needs a dataset. The methodological orientation of this project means that it does not aim for a linguistic description of a phenomenon or a language itself, but for the development of a tool that could aid such a description. Therefore, in order to test the workflow described in Chapter \@ref(workflow) and the visualisation tools described in Chapter \@ref(nephovis), we need to apply the methodology to some dataset. For that purpose, we selected 32 Dutch nouns, adjectives and verbs exemplifying a range of semasiological phenomena: homonymy in the case of nouns, interaction between semantic variation and argument structure in the case of verbs and, for across all parts of speech, metaphor, metonymy and generalisation/specialisation. 

Dutch and Flemish newspapers: the mode is written and the genre, journalistic. It comprises articles published between 1999 and 2004, belonging to popular and quality sources for both regions in equal proportion and amounting to a total of 520 million tokens, including punctuation [@depascale_2019 30].

**Homonymy** occurs when the same lemma has two or more (sets of) senses that are not semantically or etymologically related. The rest of the semantic distinctions can be broadly classified as generalisation/specialisation, metaphor, or metonymy. **Generalisation** and **specialisation** are two sides of the same coin: one of the senses involved is applied to a particular context or situation, and the other has a much broader application. Crucially, this process involves some additional semantic feature. For example, *herstructureren* 'to restructure' can be applied to a range of situations, but when it applies to companies or parts of companies in particular it does not only mean 'to change the structure of something' but also 'to reduce the personnel', which is missing in the general application. The direction of the relationship, i.e. whether the first sense is a generalisation of the second or the other way around, is not relevant for the purposes of this study.
Within Cognitive Linguistics, **metaphor** and **metonymy** are understood as cognitive principles that influence semantic structure, rather than mere expressive tools. They are found to interact and, at the same time, the distinction between them is not always unambiguous
[@lakoff.johnson_2003; @barcelona_2015; @lemmens_2015; @geeraerts_2003].
While metaphor is described in terms of comparison, similarity and mapping between different domains, metonymy is described in terms of reference, contiguity and mappings within a domain
[@lemmens_2015].
For example, when *grijs* 'gray' is applied to a weather-related term, e.g. *grijze avond* 'gray evening', the colour of the overcast sky stands for the weather in a metonymical mapping; when it is applied to an abstract entity like a *buurt* 'neighbourhood' a metaphorical sense 'boring, sad' is activated. However, the definition of what counts as a domain is not without problems, leaving the boundaries between metaphor and metonymy challenging to define as well. For the purposes of these case studies, the distinction is relevant to the extent that metonymical senses are more likely than metaphorical senses to occur in the same contexts as their literal counterparts.

In practice, the situation is even more complicated. In the case of structural metaphors
[@lakoff.johnson_2003],
metaphorical extensions might be elaborated by means of longer expressions. For example, in *we richten de spots op de zoektocht naar kandidaten* 'we aim the spotlights towards the search for candidates', *richten* 'to direct' and *op* 'on' can co-occur with either the literal or metaphorical senses of *spots* 'spotlight', and *zoektocht* 'search' is the cue that makes the literal sense less appropriate. This leads us to a situation already discussed by
@geeraerts_2003
regarding the interaction of metaphor and metonymy in idiomatic and composite expressions. In a case like *hete aardappel* 'hot potato', which in the sample always refers to delicate situations that nobody wants to deal with, is the adjective 'hot' literal or metaphorical? Given a prismatic model of composite expressions
[@geeraerts_2003],
it could be explained as a combination of literal *heet* 'hot to the touch' with literal *aardappel* 'potato' that together is metaphorically understood as a delicate situation; a reinterpretation could then complete the mapping between the potato and the situation, and between the property of being hot to the touch and that of being delicate and to be avoided. The degree to which these reinterpreted mappings match systematic metaphorical or metonymical mappings of the individual elements is a separate issue: it could be argued for *heet*, which has a 'conflictive' meaning in non idiomatic constructions, but not for *aardappel* 'potato'. As a rule, these cases have been annotated as literal, understanding that it is the situation as a whole that is metaphorical.

The goal was to explore which phenomena were revealed by distributional models and whether they were related to certain parameter settings. About 300 tokens were collected for each of the lemmas and all attestations were manually annotated by at least three different people.

In this chapter we will introduce this dataset: Section \@ref(selection) will describe which lemmas were chosen and, while Section \@ref(extraction) will focus on how the tokens were collected and the manual annotation procedure. We will not discuss each of the chosen lemmas in detail; their definitions will be provided in a long table and only specifically addressed for concrete examples in Part II. Finally, we close the chapter in Section \@ref(ann-summary).

## The lemmas {#selection}

The selection of lemmas aimed to cover a wide range of phenomena: metaphor, metonymy, generalisation/specialisation, and more. The nouns were chosen because they exhibit both homonymy and polysemy: they have unrelated (groups of) meanings and at least one of them presents finer distinctions. The selection of adjectives also includes different kinds of semantic extension which are mostly related to the kind of noun that is modified by it. Finally, the verbs combine syntactic and semantic dimensions. The definitions provided to the annotators with their respective examples and their translations to English will be listed in tables, but no other examples will be shown in this chapter. Instead, relevant tokens and their contexts will be reproduced in the second part of the dissertation to illustrate the results from the analyses.

The selection of phenomena was attached to certain expectations. We expected specific senses to be easier to identify than general senses, i.e. to have a more identifiable context. With regard to nouns, homonyms were expected to be discriminated more easily that their internal distinctions. For verbs, instead, the expectation was to find more confusion between senses that either shared the semantic or the syntactic dimension than between senses that did not.
We also expected metonymical senses to be harder to disambiguate than synaesthetic or metaphorical senses, since they are more likely to have an overlapping context with the more concrete, literal senses.

### The nouns {#nouns}

A set of 7 nouns was selected that exhibit both homonymy and polysemy in at least one of the homonyms^[Originally we selected 8 nouns, but *spoor* was discarded because of the high disagreement between the annotators and their difficulty in understanding the definitions. The data is, however, available for future analyses.], as shown in Table \@ref(tab:noundefs). The purpose of this selection was to examine how models dealt with granularity: homonyms should be easier to disambiguate than their senses, since they will apply to very different contexts, but maybe it would be possible to tune the parameter settings for different levels of granularity.

(ref:noundefs) Definitions and examples for the senses of each of the 7 analysed nouns. In each sense, the first number indicates the homonym and, if there is a second number, the sense within the homonym.

```{r, noundefs}
showDefs(nouns, "(ref:noundefs)")
```

Three nouns have one frequent, monosemous homonym and a less frequent, polysemous one: *hoop* 'hope/heap', *spot* 'ridicule/show or spotlight' and *horde* 'horde/hurdle'. The polysemy phenomena are varied. First, *horde* 'hurdle' can refer to literal hurdles, e.g. in races, while the other sense is metaphorical: abstract difficulties are talked about as obstacles to be surpassed. In addition, after the annotation a new sense tag derived from 'horde' was included, for the cases in which the members of the horde were not human beings, but insects, cars or other entities.
Second, one of the *hoop* 'heap' senses refers to literal heaps of things that can form a pile, while the other one is a generalisation to large quantities, e.g. *een hoop werk* 'a lot of work'. Finally, the polysemous homonym of *spot* has two main senses linked by metonymy, namely 'short video', e.g. and advertisement spot, or 'spotlight'. The 'spotlight' sense can also be used either literally or metaphorically ('to be in the spotlight'); this distinction was not included in the original definitions, but the annotators pointed it out and it was added afterwards.

The other four nouns have two polysemous homonyms: *schaal* 'scale/dish', *blik* 'gaze/tin', *stof* 'substance/dust...', and *staal* 'steel/sample'.
First, the frequent homonym of *blik* ('gaze') has a concrete sense with two metaphoric extensions: 'intellectual look', which was not attested in the sample, and 'perspective', which is quite infrequent. The infrequent homonym, 'tin', can either refer to the material itself, to an object made of that material ('tin can') or its content ('food from a tin can'); due to their low frequency and the difficulty on part of the annotators to distinguish between the senses, the two last senses were later combined into one.
Second, the frequent homonym of *stof* has two concrete, referentially distinct senses ('substance' and 'fabric') and an abstract one ('topic'). In contrast, for the less frequent homonym we distinguished two senses presenting a subtle, context-specific difference: between 'dust (in the air)' and the 'dust' in 'reducing something to dust, to pulverize'. The last sense was so infrequent that it was excluded, but another distinction emerged from the annotation, namely between literal 'dust' and 'dust' in idiomatic expressions, such as *stof doen opwaaien* 'to be controversial, lit. to lift up dust'. The new sense was added because, even though within the idiomatic expression the meaning of *stof* is still 'dust', the annotators kept confusing it with the 'topic' sense, which actually refers to expressions such as *stof voor een roman* 'material for a novel'.
Third, *schaal* exhibits subtle perspective shifts in one homonym ('scale') and refers to different concrete objects with the second 'shell/dish'.
Finally, *staal* 'steel' could refer, like *blik* 'tin', to either the material or the part of an object that is made from it --- the latter is very infrequent among our sample, but instead another sense could be identified, namely 'steel industry'. The 'sample' homonym, on the other hand, originally presented a metaphorical distinction between material samples and 'evidence' of abstract characteristics, but was modified after annotation to a specialisation distinction between general samples, e.g. a urine sample, and (statistically) representative samples.

As we can see, the nouns present a variety of semantic phenomena at a finer granularity than homonymy: metaphor in the case of *blik* 'gaze', *horde* 'hurdle' and *spot* 'spotlight', metonymy in the case of *horde* 'horde', *blik* 'tin', *staal* 'steel' and *spot* 'videoclip/spotlight', generalisation/specialisation in the case of *staal* 'sample', *schaal* 'dish' and *hoop* 'heap', perspective shifts for *schaal* 'scale' and other relationships in the frequent *stof* homonym.

### The adjectives {#adjs}

The selection of adjectives includes 13 lemmas presenting different kinds of polysemy phenomena (Table \@ref(tab:adjdefs)). The purpose of this selection was to examine how models dealt with their semantic relationships and whether they could extract them from the different nouns modified by the target adjective.

(ref:adjdefs) Definitions and examples for the senses of each of the 13 analysed adjectives.

```{r, adjdefs}
showDefs(adjs, "(ref:adjdefs)")
```

Three adjectives have a metonymic reading: *hoopvol* 'hopeful', *geestig* 'witty' and *hachelijk* 'dangerous/critical'.
For *geestig* and *hoopvol*, one of the senses is anthropocentric, i.e. it's mainly or exclusively applied to people: witty people against the witty things they say or do, and people who express hope against things that inspire it.
In *hachelijk*'s case, the difference is a matter of temporal or telic perspective: between things that might go wrong and situations that are already problematic.

Four adjectives have metaphoric readings: *hoekig* 'angular', *dof* 'dull', *heilzaam* 'healthy/beneficial' and *gekleurd* 'colourful/person of colour/tainted'.
*Heilzaam* has two senses, distinguishing between things that are literally healing, or beneficial for the health, and things that are metaphorically healing, or beneficial in general.
*Hoekig* and *gekleurd* present three sense distinctions, one of which is particularly concrete and the most frequent, 'of angular form' and 'colourful' respectively, and another one explicitly anthropocentric: 'clumsy' and 'non white'. The third sense distinction has a different quality: synaesthetic for *hoekig*, applied to rhythms, and metaphoric for *gekleurd*, meaning 'tainted, corrupted'.
Finally, *dof* has a concrete sense applied to the visual domain, a synaesthetic extension applied to sounds, and an abstract meaning applied to feelings and emotions; the fourth meaning listed in the table was not attested.

Three adjectives present some other form of similarity between the readings: *geldig* 'valid', *hemels* 'heavenly' and *gemeen* 'shared/mean...'.
*Geldig* 'valid' and *hemels* 'heavenly' offer two options: one restricted to a specific context (laws and reglaments for *geldig* and Heaven for *hemels*) and one much broader.
The case of *gemeen* is quite complex, involving a number of rather subtle distinctions that often co-exist in the same attestation: i.e. 'common' and 'shared', or 'average' and 'ordinary'.

Finally, the remaining three adjectives present a more complex picture: *heet* 'hot' for different entities and metaphorically, *grijs* 'gray', with metaphorical and metonymical extensions and *goedkoop* 'cheap', with different entities and metaphorically.
*Heet* 'hot' presents, first, three very concrete senses that differ in perspective: temperatures of objects, of weather and as it is felt in the body; the other three senses are metaphorical, i.e. the objects to which *heet* is applied cannot be physically hot. Crucially, there is no exclusive sense tag for idiomatic expressions in which the combination of *heet* 'hot' and its concrete object (e.g. *hangijzer* 'iron', *aardappel* 'potato') is used metaphorically.
*Grijs* presents a very frequent, concrete sense, three specific metonymic extensions --- to weather and to hair, and from there to old people --- and two metaphorical ones --- 'boring' and 'half legal'.
*Goedkoop*, on the other hand, presents a modest set of 4 sense distinctions: a concrete, prototypical and frequent sense, two perspectival shifts and a clear metaphor.

In sum, the adjectives include more simple semasiological structures with only one kind of semantic extension involved as well as more complex interactions between the phenomena.

### The verbs {#verbs}

The criterion to select the 12 verbs analysed here was to cover a range of combinations of syntactic and semantic variation, with the goal of exploring how different parameter settings dealt with their interaction or whether certain types of models would focus on one or the other aspect.^[The original set of verbs also included *herkennen*, but it was excluded because of the extreme subtlety of its sense distinctions, which made the annotation particularly challenging.] Their senses and translations are shown in Table \@ref(tab:verbdefs).

(ref:verbdefs) Definitions and examples for the senses of each of the 13 analysed verbs.

```{r, verbdefs}
showDefs(verbs, "(ref:verbdefs)")
```

Four verbs are always transitive and their senses can be distinguished by the objects they can take: people or objects for *haten* 'to hate', people or opinions for *huldigen* 'to pay homage/to hold (an opinion)', concrete objects or taxes for *heffen* 'to levy/to lift', and decisions or statements for *herroepen* 'to retract/to annul'.

Two of the verbs can be transitive, with a distinction based on the direct object, or intransitive: *helpen* 'to help' and *herstructureren* 'to restructure'. In both cases the intransitive sense is semantically similar to one of the transitive senses. For example, the intransitive sense and one of the transitive senses of *herstructureren* only apply to companies, with the connotation that the personnel is being reduced, while the other transitive sense has a much more general application.

Three verbs can be transitive, with a distinction based on the direct object, or reflexive: *diskwalificeren* 'to disqualify', *herhalen* 'to repeat' and *herinneren* 'to remember/to remind'. In the case of *diskwalificeren* 'to disqualify' and, to a lesser degree, *herhalen* 'to repeat', this opposition can be interpreted as a specific situation where the object and the subject coincide.
In contrast, *herinneren* means 'to remember' in the reflexive construction and 'to remind' in the transitive construction with the preposition *aan*; the transitive construction without the preposition can also be attested (e.g. *ik word herinnered als*, 'I am remembered as') but very infrequently.

Two more verbs can be transitive, intransitive or reflexive, with semantic distinctions within the transitive structure: *harden* 'to make or become hard/ to tolerate' and *herstellen* 'to repair/ to heal...'. The senses of *harden* can be split in two main groups. One is more closely related to the property of 'hardness': to turn something or someone hard or to become hard, in literal or figurative sense, with different constructions. The second group, however, includes one transitive construction in a very specific pattern but is more frequent in the sample than all the others combined: *(niet) te harden* ('to (not) tolerate', always negative).

Finally, *haken* 'to hook' presents semantic distinctions within both the transitive and the intransitive structures. It can refer literally or metaphorically to hooking something or remaining hooked, but there are also two very specific senses: one characteristic of the football context, meaning 'to make someone trip (by placing a foot in front of them)', and 'to crochet'.

In sum, the set of verbs includes cases where only the kind of direct object plays a role in the disambiguation and cases where it interacts with syntactic patterns. Moreover, the specific ways in which these kinds of direct objects are defined differ across verbs: from animacy or agency in the case of *haten* to concreteness in the case of *heffen*. The semantic distinctions can also rely on a broader context: *diskwalificeren* will typically have people as direct object, but the sports-related context defines a specific sense, characterised by distinct motivations and consequences.

## The dataset {#extraction}

The selection procedure involved some introspection as well as consultation of lexical resources and corpus data: thinking of potential candidates, checking the senses reported in dictionaries (mostly *Van Dale*) and estimating their relative frequencies in small concordances. We tried to avoid extremely skewed distributions approximating a monosemous structure or numerous infrequent senses that would be unlikely to stand out in a model.^[In a number of cases, the corpus survey (reading a random concordance of 40-50 lines) invalidated options that intuitively or according to the dictionary definitions would have conformed to our requirements. When judging such a discrepancy, it is important to take into account the composition of the corpus. The topics addressed in newspapers and the terms used to talk about them are certainly not representative of everyday life or the entirety of language.]
The exploration of these samples of concordances also served for the calculation of the number of tokens to annotate. Regardless of the actual frequency of the items in the corpus, the minimum sample contained 240 tokens; it was raised to 280 if any of the senses had a relative frequency below 20% in the sample, to 320 if it was below 10%, and to 360 if there were many senses and therefore some had a low frequency (e.g. *heet*). Table \@ref(tab:lemmafreq) shows the absolute frequency (in a `r sc("520MW")` corpus) of each selected lemma, the size of the sample and the distribution of the senses: the more the boxplot in the rightmost column goes to the right, the more frequent one of the senses. For example, the long boxplots for *blik* and *hoop* indicate a very skewed distribution, i.e. a sense with very high frequency and senses with very low frequencies, while the narrow, centred boxplots for *hachelijk* and *hemels* indicate that their senses are equally frequent.
The sample extraction was almost completely random, with the only restriction that no two instances of the same lemma would be extracted from the same file. There were, however, a few duplicates, due to repetition of the same fragment on different dates.

(ref:lemmafreq) Absolute frequency of the lemmas in the corpus, number of batches and distribution of their senses. The number next to the boxplots indicate the number of different senses.

```{r, lemmafreq}
lemmas %>%
  select(-pos, -senses) %>%
  rename(sample = batches, senses = n_senses) %>% 
  mutate(sample = sample*40) %>%
  kbl(longtable = T,booktabs = T, escape = F, caption = "(ref:lemmafreq)") %>%
  kable_paper(latex_options = c("repeat_header")) %>%
  column_spec(4, image = spec_boxplot(lemmas$senses, xaxt = T)) %>% 
  pack_rows(index = table(lemmas$pos))
```


For each of the tokens a concordance line was extracted with 15 words to either side. Bachelor students of Linguistics at KU Leuven were recruited and hired to manually annotate the samples of the selected lemmas. Each of them was tasked with annotating 40 tokens of each of 12 types (at least three nouns, four adjectives and four verbs, plus one of either of the categories): a total of 480 tokens^[A few of them doubled their load and annotated two sets of 480 tokens.], to annotate in 6 weeks. In total, each of the 9600 tokens was annotated by at least three annotators; 10% of them were annotated by four. Each lemma was split in 6-9 batches of 40 tokens, each of them annotated by a different group of annotators. The annotators were offered an introductory meeting, a video tutorial and written guidelines, but the procedure itself was performed individually.

Both the lemmas and the batches were assigned randomly, while keeping in mind the part-of-speech distribution. It was the intention to shuffle the samples of each lemma before splitting them into batches, but something went wrong with the code and they were ordered by source.
The annotation involved three compulsory tasks:

1. assign a sense from a predefined set of definitions, namely the Dutch column in Tables \@ref(tab:noundefs) through \@ref(tab:verbdefs). If none of the tags apply, select "None of the above" and explain why;
2. express the confidence of the decision in a Likert scale of 6 values;
3. identify the words of the context that helped in the disambiguation.

Since entering textual information in a spreadsheet can easily lead to typos and inconsistencies and, furthermore, annotating the helpful context words is particularly challenging in such a tool, a user-friendly visual interface was designed that received input from buttons and returned the output in `r sc("json")` format.
The interface, which is not available in its original form any more, had a menu with the list of lemmas and two tabs: an overview of the concordance lines of the selected type and an annotation workspace (Figure \@ref(fig:semann)). The annotation workspace focused on one concordance line^[The web-based interface interpreted `r sc("html")`, of course. As a consequence, the sentence separator `<sentence></sentence>` was simply ignored; it should have been replaced with `<p></p>` to properly render the division, especially after headlines, which lack final stops. Interestingly, this rendering also could have read strings such as `&quot;` as `"`, but at some earlier stage of pre-processing of the corpus all the `&` have been transformed into `and`, resulting in a number of confusing appearances of `andquot;` in the concordance lines.] (or token) at a time, offering first the text, then a series of long radio buttons with the definitions and examples, a star rating option for the confidence evaluation followed by a clickable reproduction of the text, and a text input field for comments. The long radio buttons meant that the annotators had the full definitions and examples at their disposal every time they had to assign a sense for a given lemma, while the final output transformed their decisions into more manageable codes, such as `sense_1`, `sense_2`, etc. The clickable concordance lines let them select the context words they deemed most useful to the annotation procedure by simply clicking on them; the program then translated this as an array of positions relative to the target, e.g. `["R1", "L2"]` if the first word to the right and the second to the left are selected.^[For a few weeks into the annotation, the code had a bug that meant that if a word form was repeated in the concordance line and one of its instances was selected its first occurrence was recorded even if the chosen one was a later co-occurrence. The bug was fixed as soon as it was reported and the rest of the annotators were warned, but not all the resulting errors were corrected.]

(ref:semann) Screenshot of the options in the annotation tool.

```{r, semann, fig.cap = '(ref:semann)', out.width="100%"}
cloud_foto("semann-annotation")
great_agreement <- final_agreement %>% mutate(n = n/sum(n)) %>% filter(final_action == "Same", majority_agreement != "No agreement") %>% pull(n) %>% sum
```

The dataset obtained from this procedure is very rich and interesting for a variety of purposes. For each token we have sense assignment, confidence evaluation and selection of informative cues by at least three different independent annotators, as well as comments at least on all the cases which did not receive a sense. Agreement between the annotators can be measured with coefficients such as Fleiss' $\kappa$ [@fleiss_1971], illustrated in Figure \@ref(fig:agreement), but the resulting picture may be unnecessarily complex. First, disagreement is susceptible to granularity: annotators might disagree between senses of a noun but not between the homonyms, except for their confusion between idiomatic senses of *stof* 'dust' and its 'topic' sense. Second, annotators were not very sensitive to grammatical distinctions (e.g. between transitive and intransitive senses), which was a strong reason for disagreement in *herstructureren*, *helpen*, *haken* and *herstellen*. Third, disagreements were sometimes concentrated on one annotator, who showed a strong preference for a certain sense; as such, they were not an indicator of the ambiguity of the token but of misunderstandings on the part of the annotator. Some annotators exhibited an almost excessive attention to nuances, while others were much less thorough.
More importantly, for the great majority of the tokens
(`r round(great_agreement*100, 1)`%)
the majority of the annotators agreed on one tag that remained as the official sense for that token.
After gathering and exploring the data, the tokens were reread by me and a final decision was made for their sense tags. Figure \@ref(fig:final-agreement) shows the number of tokens with full agreement, a majority agreement (i.e. only one annotator disagreed) or no agreement and whether the same chosen sense was kept in the final annotation, another tag was applied or the token was removed (e.g. tokens of *heet* that corresponded to the verb *heten*). The `Other` category includes new senses suggested by the annotators themselves as well as corrections from misunderstandings, such as the second original sense of *blik*, which annotators interpreted in different ways and was actually not attested in the dataset. The very few cases of `Same` with no agreement were tokens annotated by four annotators where two of them selected the senses that remained, while the other two disagreed.

(ref:agreement) Agreement between annotators per batch per lemma, computed with `irr::kappam.fleiss()` [@R-irr].

```{r, agreement, fig.cap = "(ref:agreement)"}
kappas %>%
  filter(p.value < 0.05) %>%
  ggplot(aes(x = kappa, y = reorder(lemma, kappa, hmean))) +
  geom_point(size = 3, alpha = 0.7) +
  theme_pubr(base_family = "Modern No. 20") + 
  labs(
    x = latex2exp::TeX(r"(Fleiss' $\kappa$)"),
    y = "Lemma")
```


(ref:final-agreement) Number of tokens per lemma with full, partial (majority) or no agreement, split by whether the majority sense was kept or changed. Removed tokens are not included.

```{r, final-agreement, fig.cap = "(ref:final-agreement)"}
final_agreement %>% 
  filter(final_action != "Remove") %>% 
  ggplot() +
  geom_col(aes(y = reorder(type, n, max), x = n, fill = majority_agreement)) +
  theme_pubr(base_family = "Modern No. 20") +
  facet_wrap(~final_action) +
  scale_fill_grey(start = 0.7, end = 0.2) +
  theme(axis.text.y = element_text(size = 9)) +
  labs(x = "Tokens", y = "Lemma", fill = "Agreement")
```

In addition, the final sense distribution is not significantly different from that in the much smaller pilot samples. Distribution across batches, instead, was affected by regional variation. For example, Belgian sources include more sports-related articles than the Netherlandic sources, leading to variation in the sense distribution of lemmas with such a sense (*diskwalificeren* 'to disqualify', *haken* 'to make someone trip' and *horde* 'hurdle') across regions. This discrepancy in distribution across batches could have been avoided if the tokens had been properly shuffled. 

Confidence values were explored but not used, because they tend to be similar across batches, lemmas and senses, with a tendency to the highest values and variation across annotators instead: what is low confidence for some of them is high confidence for others. Figure \@ref(fig:confidence) breaks this down in terms of the degree of agreement and whether the assigned tag matched one of the senses offered or not. Note that the top facet, "None of the above", has much lower counts than the lower facet. We would expect confidence ratings to be lower for annotations that do not agree with the other votes for the same token and, in relative terms, that is the case. Confidence assignment to a "None of the above" tag is ambiguous: some annotators tend to give them the minimum confidence because they are not confident about the meaning of the concordance line, while others assign a high value because they are confident that none of the other options applies.

(ref:confidence) Distribution of confidence values across annotations, by whether the annotators agreed with another in the same token and by whether they selected a sense or "None of the above".

```{r, confidence, fig.cap = "(ref:confidence)"}
ggplot(confidence) +
  geom_bar(aes(x = confidence, fill = majority), position = "dodge") +
  facet_grid(NOTA ~ ., scale = "free_y") + theme_pubr(base_family = "Modern No. 20") +
  scale_fill_grey(start = 0.3, end = 0.7) +
  theme_pubr(base_family = "Modern No. 20") +
  labs(x = "Confidence rating", y = "Annotations", fill = "Agreement in token")
```

The selection of cues was consulted when defining parameter settings (Section \@ref(params)): if two annotators agreed on both the sense tag and a context word for a given token, that context word was considered an official **cue** for that sense. From the relative position representing the cue in the output of the annotation tool, other information available in the corpus could be extracted and counted, such as the lemma of the context word, its dependency relation (or distance) to the target and its `r sc("bow")` distance to the target. For example, Tables \@ref(tab:heilzaamdep) and \@ref(tab:heilzaamcues) list the most frequent dependency paths, lemmas and window sizes across the official cues of *heilzaam* 'healthy/beneficial' for each of its senses. As we will see again in Section \@ref(heilzaam), this lemma is characterised by frequent nouns modified by the target, namely *werking* 'effect', *effect* and *invloed* 'influence', which are ambiguous in terms of the senses of *heilzaam*: in a sentence such as *de heilzame werking van look* 'the healing power of garlic', *garlic* is a better cue in the 'health/beneficial' distinction than *werking* 'effect, power'. Nonetheless, annotators did select these context words as cues for both senses, not realising that they were not distinctive of one or the other sense. The pattern fulfilled by *garlic* in this example was indeed captured by some cues, as shown in the third line of Table \@ref(tab:heilzaamdep), but it is much less frequent.

(ref:heilzaamdep) Four most frequent dependency paths among the cues of *heilzaam*, with counts per sense. `NA` indicates that the cue is not in the sentence of the target. In the path, `CW` stands for the cue and `T` stands for the target: the head is at the left of $\rightarrow$ and its dependents are to the right, preceded by the name of the dependency relation.

```{r, heilzaamdep}
countCues(path, 4) %>%
  mutate(examples = c(
    r"($\textbf{heilzame}$ $\textit{werking}$ 'healing $\textit{power}$')",
    "Different sentence",
    r"(de $\textbf{heilzame}$ werking van $\textit{look}$ 'the healing power of $\textit{garlic}$')",
    r"(look is $\textbf{heilzaam}$ voor de $\textit{gezondheid}$ 'garlic is beneficial for the $\textit{health}$')"
  )) %>% 
  select(path, examples, everything()) %>% 
  kable(caption = "(ref:heilzaamdep)", booktabs = T, escape = F, linesep = "\\addlinespace") %>% 
  kable_paper(full_width = T) %>% 
  column_spec(1:2, width = "11em") %>% 
  column_spec(3:4, width = "3em")
```


(ref:heilzaamcues) Six most frequent lemmas and window spans among the cues of *heilzaam*, with counts per sense.

```{r, heilzaamcues}
bind_cols(countCues(CW, 6), countCues(BOW, 6)) %>% 
  kable(col.names = c("CW", "healthy", "beneficial", "BOW", "healthy", "beneficial"),
        caption = "(ref:heilzaamcues)", booktabs = T) %>% 
  kable_paper() %>% 
  column_spec(3, border_right = T)
```


## Summary {#ann-summary}

In order to apply and test the workflow and visualisation tools to a concrete dataset, we collected occurrences of 32 Dutch lemmas, covering three frequent parts of speech and a wide range of semantic phenomena. The dataset was small enough to allow detailed manual examination: the annotation relied on a number of hired participants, but it was also fully revised and discussed. For each of the lemmas, 200-212 models were generated following the workflow described in Chapter \@ref(workflow). The cues selected by the annotators informed some of the decisions involved in the parameter settings. The visualisation was first applied to samples of the models defined by strong parameters, e.g. keeping second-order- and the part-of-speech parameter fixed to look at the interaction between window size or dependency models and weighting; then, medoids were computed and analysed more thoroughly. The sense annotation was applied to assess how well the models performed at disambiguation: initially, we did not try to match senses to clustering solutions, but looked for a spatial configuration that might hide more subtle relationships.

The range of semantic phenomena was meant to provide different possible aspects of meaning that distributional models might be able to capture. From a lexicological point of view, "similarity of distribution correlates with similarity of meaning" is not enough. What is similarity of meaning? Does this mean that more granular distinctions, such as senses within homonyms, will be more difficult to capture than coarser distinctions, i.e. the homonyms themselves? Are metonymy, metaphor and specialisation modelled by the same parameter settings? Can be discriminated, can we fine-tune models to capture one or the other? And what is the role of constructions: does argument structure interfere in the modelling of senses?
These were the questions that the case studies presented here tried to address, and the following part of this dissertation will present the answers.
