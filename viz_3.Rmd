# From corpora to clouds

The main goal of the distributional models discussed in this text is to explore semasiological structure
from textual data. The starting point is a corpus, and one of the most tangible outputs is the visual representation
as a cloud.

In this chapter we will describe the path needed to take to generate clouds from the raw,
seemingly indomitable ocean that is a corpus.

First, we will describe token-level vector space models are created.
Section \@ref(vector-creation) will explain count-based models, but this is by no means the only viable path.
Other techniques, such as BERT [@BERT], that can generate vectors for individual instances of a word,
can be used for the first stage of this workflow.
<!-- TODO explain why we don't use them -->

Once we have token-level vectors, we need to process them. For visualization purposes,
we need to reduce the numerous dimensions of the vectors to a manageable number, such as 2.
Section \@ref(dim-reduction) will explore and compare a few alternatives.

The same output that is put through dimensionality reduction for the visualization can also
be submitted to other forms of analysis such as clustering algorithms, whose results may
even be combined with the visualization. We will look into HDBSCAN in
<!-- cite appropriate chapter and section. -->

## A cloud machine {#vector-creation}

At the core of vector space models, *aka* distributional models, we find the Distributional Hypothesis, which is most often linked to Harris's observation that "difference of meaning correlates with difference of distribution" [-@harris_1954 156]. In other words, items that occur in similar contexts in a given corpus will be semantically similar, while those that occur in different contexts will be semantically different. Crucially, this does not imply that we can describe an individual item with their distributional properties, but that comparing the distribution of two items can tell us something about their semantic relationship [@sahlgren_2006 19].

Distributional models operationalize this idea by representing words as vectors (i.e. arrays of numbers) coding frequency information. Typically, the raw frequency is transformed to some association strength measure, such as pointwise mutual information [PMI, see @church.hanks_1989], which compares the frequency with which two words occur close to each other and the expected frequency if the words were independent. For example, Table \@ref(tab:vec1) shows small vectors representing the English nouns *linguistic*, *lexicography*, *research* and *chocolate*, as well as the adjective *computational*, as series of association strengths with a set of lemmas. Empty cells indicate that the word in the row and the word in the column never co-occur in the corpus (given a certain window span).

```{r, vec1, echo=FALSE, tab.cap="Example of type-level vectors."}
vex <- read_csv("assets/vector-examples/vectorexample.csv", col_types = cols()) %>% 
  mutate_if(is.numeric, round, 2)
vex %>% 
  select(-`eat/v`) %>% 
  flextable() %>% 
  add_footer_lines("PMI values based on symmetric window of 10; frequency data from GloWbE.") %>% 
  fit_to_width(max_width = 4.7)
```

Each row is a vector coding the distributional information of the lemma it represents. As we can see in this example, words with similar vectors (e.g. *linguistics* and *lexicography*) are semantically similar, while words with different vectors (e.g. *linguistics* and *chocolate*) are semantically different.

The vectors in Table \@ref(tab:vec1) are type-level vectors: each of them aggregates over all the instances of a given word, e.g. *linguistics*, to build an overall profile. As a result, it collapses the internal variation of the lemma, i.e. its semasiological structure. In order to uncover such information, we need to build vectors for the individual instances or tokens, relying on the same principle: items occurring in similar contexts will be semantically similar. For instance, we might want to model the three (artificial) occurrences of *study* in (@ex1) through (@ex3), where the target item is in italics.

(@ex1) Would you like to *study* lexicography?
(@ex2) They *study* this in computational linguistics as well.
(@ex3) I eat chocolate while I *study*.

Given that, at the aggregate level, a word can co-occur with thousands of different words, type-level vectors can include thousands of values. In contrast, token-level vectors can only have as many values as the individual window size comprises, which drastically reduces the chances of overlap between vectors. In fact, the three examples don't share any item other than the target. As a solution, inspired by @schutze_1998, we replace the context words around the token with their respective type-level vectors [@heylen.etal_2015; @depascale_2019].

For example, we could represent example (@ex1) with the vector for its context word *lexicography*, that is, the second row in Table \@ref(tab:vec1); example (@ex2) with the sum of the vectors for *linguistics* (row 1) and *computational* (row 3); and example (@ex3) with the vector for *chocolate* (row 5). This not only solves the sparsity issue, ensuring overlap between the vectors, but also allows us to find similarity between (@ex1) and (@ex2) based on the similarity between the vectors for *lexicography* and *linguistics*.

From applying this method we obtain numerical representations of occurrences of a word. We can compare them to each other by calculating pairwise distances, which is at the base of clustering analyses and visualization techniques based on dimensionality reduction. However, in order to obtain this result we need to make a number of decisions involved, mostly, in defining the context that will be used to represent an item [Cf. @bolognesi_2020 83].

## Dimensionality reduction {#dim-reduction}

<!-- NOTE Dimensionality reduction can also refer to SVD and stuff like that, so we should mention it and dismiss, cite Stefano -->

Dimensionality reduction refers to algorithms that try to locate different items on a low-dimensional space (e.g. 2D) preserving their distances in the high-dimensional space (e.g. 5000D) as well as possible.
The literature up to today tends to go for either multidimensional scaling (MDS) or t-stochastic neighbor embeddings (t-SNE),
which may be run on R with the function `metaMDS()` of the {vegan} package [@R-vegan]
and `Rtsne()` of the homonymous package [@R-Rtsne], respectively.

MDS is an ordination technique, like principal components analysis (PCA). It tries out different low-dimensional configurations and tries to maximize the correlation between the pairwise distances in the high-dimensional space and those in the low-dimensional space: items that are close together in one space should stay close together in the other, and items that are far apart in one space should stay far apart in the other.
It can be evaluated via the stress level, the complement of the correlation coefficient: if the correlation between the pairwise distances is 0.85, the stress level is 0.15.
Unlike PCA, however, the dimensions are not meaningful *per se*; two different runs of MDS may result in plots that mirror each other while representing the same thing. Nonethelesss, the R implementation rotates the plot so that the horizontal axis represents the maximum variation.
In cognitive linguistics literature both metric [@hilpert.correiasaavedra_2017; @hilpert.flach_2020; @koptjevskaja-tamm.sahlgren_2014]
and nonmetric MDS [@heylen.etal_2015; @heylen.etal_2012; @depascale_2019; @perek_2016] have been used.

The second technique, t-SNE [@Rtsne2008; @Rtsne2014], has also been incorporated in cognitive distributional semantics [@depascale_2019; @perek_2018].
The algorithm is quite different from MDS, but for our purposes the crucial point is that it prioritizes preserving local similarity structure instead of the global structure: items that are close together in the high-dimensional space should stay close together in the low-dimensional space, but those that are far apart in the high-dimensional space may be even farther apart in low-dimensional space. This leads to nice, tight clusters but the distance between them is less interpretable than in an MDS plot.
T-SNE was the state-of-the-art visualization technique for word vectors in computational linguistics [@smilkov.etal_2016] but is now generally being replaced by UMAP.

<!-- TODO add stuff about UMAP -->

In both cases we need to state the desired number of dimensions before running the algorithm --for visualization purposes, the most useful choice is 2. Three dimensions are difficult to interpret if projected on a 2D space, such as a screen [@card.etal_1999 18; @wielfaert.etal_2019 222]. In addition, t-SNE requires setting a parameter called perplexity, which basically sets how many neighbors the preserved local structure should cover.
